### **1.5 目前語言模型的主流架構**

現今語言模型的發展大多基於深度學習技術，尤其是基於Transformer架構的模型。以下是目前語言模型的主流架構及其特點。

#### **1.5.1 RNN（循環神經網絡）及其變體**

**RNN**是最早用於處理序列數據的深度學習模型之一。它利用循環結構將輸入數據的時間序列特性納入考量，適用於處理時間序列和自然語言數據。

- **優點**：能夠處理可變長度的序列數據，對於短期依賴關係的建模效果較好。
- **缺點**：難以捕捉長期依賴，容易出現梯度消失問題。

**LSTM（長短期記憶網絡）**和**GRU（門控循環單元）**是RNN的改進變體，通過引入門機制來控制信息流，減少梯度消失問題，能更好地捕捉長期依賴。

#### **1.5.2 Transformer架構**

**Transformer**架構是目前最為主流的語言模型架構，主要依賴於自注意力機制（Self-Attention）和全連接層。

- **自注意力機制**：允許模型在計算某個詞的表徵時，能夠同時考慮序列中其他所有詞的影響，無論它們的位置距離有多遠。
- **並行計算**：與RNN不同，Transformer不需要逐步處理序列數據，這使得訓練和推理更高效。

典型的Transformer架構包括以下部分：

- **編碼器（Encoder）**：負責將輸入序列轉換為內部表徵。
- **解碼器（Decoder）**：利用內部表徵生成輸出序列。

#### **1.5.3 BERT（Bidirectional Encoder Representations from Transformers）**

**BERT**是一種基於雙向Transformer編碼器的模型，能夠同時考慮序列中每個詞的前後文信息，對於多種自然語言處理任務，如問答系統、文本分類等，表現出色。

- **預訓練任務**：BERT通過遮蔽語言模型（Masked Language Model, MLM）和下一句預測（Next Sentence Prediction, NSP）進行預訓練，捕捉語言的深層語義。
- **應用**：在各種NLP任務中，BERT及其變體成為強大的基線模型。

#### **1.5.4 GPT（Generative Pre-trained Transformer）**

**GPT**是一種基於自回歸Transformer的生成模型，主要用於自然語言生成任務。

- **自回歸生成**：GPT通過依次預測序列中的每個詞來生成文本，每次生成一個詞，基於之前生成的詞來預測下一個詞。
- **應用**：GPT在生成式任務中，如文本生成、對話系統和內容創作，表現卓越。

#### **1.5.5 T5（Text-To-Text Transfer Transformer）**

**T5**是一種將所有自然語言處理任務轉換為文本到文本格式的模型，使用單一架構來處理不同的NLP任務。

- **統一框架**：T5將所有任務表示為“給定輸入文本，生成輸出文本”，如翻譯、摘要、問答等，簡化了模型的設計和訓練過程。
- **靈活性**：該架構允許在一個模型中進行多任務學習，提高了模型的通用性和性能。

#### **1.5.6 其他架構**

除了上述架構，還有一些創新模型，如XLNet、RoBERTa、ALBERT等，這些模型在Transformer基礎上進行了不同程度的改進，進一步提升了語言模型的性能。

這些主流架構共同推動了語言模型的迅速發展，為各種自然語言處理應用提供了強大的技術支撐。