### **2.3 轉換器（Transformer）架構**

**轉換器（Transformer）**是一種基於注意力機制的神經網絡架構，專為解決序列到序列的任務而設計，如機器翻譯和文本生成。與傳統的RNN和LSTM不同，Transformer完全依賴於注意力機制，能夠更高效地處理長距離依賴的序列數據。

#### **2.3.1 Transformer的基本結構**

Transformer的架構由編碼器（Encoder）和解碼器（Decoder）組成。編碼器將輸入序列轉換為一組特徵表示，解碼器則根據這些特徵生成輸出序列。

- **編碼器**：由多層相同結構的編碼器層組成，每層包括多頭自注意力機制和前饋神經網絡。
- **解碼器**：結構與編碼器類似，但在自注意力機制之外，還包括對編碼器輸出進行注意的機制。

每個編碼器和解碼器層都包含：
- **多頭自注意力機制（Multi-Head Self-Attention Mechanism）**：使模型能夠在不同的子空間中並行地注意輸入序列的不同部分。
- **前饋神經網絡（Feed-Forward Neural Network, FFN）**：對每個位置的特徵進行非線性變換，增強模型的表達能力。

#### **2.3.2 注意力機制**

注意力機制是Transformer的核心，能夠根據查詢（Query）、鍵（Key）和值（Value）之間的關係來計算輸出。

- **自注意力機制**：
  
```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
```

  其中， $`Q`$ 、 $`K`$ 、 $`V`$  分別是查詢、鍵和值的矩陣， $`d_k`$  是鍵向量的維度， $`\text{softmax}`$  函數用於歸一化。

- **多頭注意力**：多頭注意力機制將輸入投影到不同的子空間中，並行計算注意力，然後將結果拼接起來：
  
```math
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
```

  其中，每個 $`\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`$ ， $`W_i^Q`$ 、 $`W_i^K`$ 、 $`W_i^V`$ 是可學習的權重矩陣。

#### **2.3.3 位置編碼**

由於Transformer不包含任何循環或卷積結構，缺乏處理序列數據的內在順序信息。為此，引入了**位置編碼（Positional Encoding）**，將位置信息顯式地添加到輸入的嵌入向量中。位置編碼常用正弦和餘弦函數來計算：

```math
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
```


```math
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
```

其中， $`pos`$ 是位置， $`i`$ 是維度索引， $`d_{model}`$ 是嵌入向量的維度。

#### **2.3.4 Transformer的應用**

Transformer的設計讓其在多種自然語言處理任務中得到了廣泛應用：

- **機器翻譯**：將一種語言的句子翻譯為另一種語言，Transformer的高效架構極大提升了翻譯質量。
- **文本生成**：用於生成與給定輸入相關的連續文本，如生成對話回應或文章續寫。
- **摘要生成**：將長文本總結為簡短的摘要，保持關鍵信息。
- **文本分類**：通過Transformer學習文本的特徵，進行情感分析或主題分類。

#### **2.3.5 Transformer的優勢和挑戰**

**優勢**：
- **長距依賴建模能力**：Transformer可以有效地捕捉序列中遠距離元素之間的依賴關係。
- **並行計算**：由於自注意力機制的設計，Transformer能夠更高效地進行並行計算，提升了訓練速度。

**挑戰**：
- **計算資源需求**：多頭注意力和大規模參數需要大量的計算資源和存儲空間。
- **長序列處理**：當處理非常長的序列時，計算開銷和存儲需求會顯著增加。

隨著Transformer的成功，許多變體和改進版本，如BERT、GPT、T5等應運而生，進一步推動了自然語言處理的發展，使其在各種應用中表現出色。