### **n-gram 模型詳解**

n-gram 模型是統計語言模型中最基本的模型之一，它假設語言中的每個詞語僅依賴於前  $`n-1`$  個詞語，即上下文長度是固定的。這種簡單的假設使得模型可以在有限的計算資源下進行訓練和推理，並且對於許多語言處理任務仍然有效。n-gram 模型在許多語言生成和語音識別任務中曾經有著廣泛的應用，儘管現在許多更先進的模型（如基於神經網絡的模型）已經取代了它。

#### **1. 基本概念**

n-gram 模型的基本概念是：對於一個詞序列  $`w_1, w_2, \dots, w_n`$ ，模型估計每個詞的條件概率，即在給定前  $`n-1`$  個詞語的條件下，預測下一個詞語出現的概率。這可以用以下公式表示：


```math
P(w_n | w_1, w_2, ..., w_{n-1}) \approx P(w_n | w_{n-1}, w_{n-2}, ..., w_{n-(n-1)})
```


這樣的假設將語言建模問題簡化為估算每個詞依賴於前  $`n-1`$  個詞的條件概率。具體而言，對於一個詞序列  $`w_1, w_2, \dots, w_N`$ ，n-gram 模型的目標是計算該序列的聯合概率：


```math
P(w_1, w_2, ..., w_N) = \prod_{i=1}^{N} P(w_i | w_{i-1}, w_{i-2}, ..., w_{i-(n-1)})
```


#### **2. 不同類型的 n-gram 模型**

- **Unigram 模型**  
  在 unigram 模型中，每個詞語被視為獨立的，並且假設每個詞的出現不依賴於任何上下文。換句話說，每個詞的條件概率只與它自身有關：

  
```math
P(w_i) = P(w_i | w_{i-1}, w_{i-2}, \dots) \quad \text{(不依賴上下文)}
```


  這個模型的最大優點是簡單且計算快速，但它無法捕捉到語言中的上下文依賴性，因此通常不夠精確。

- **Bigram 模型**  
  在 bigram 模型中，每個詞語只依賴於前一個詞語，即假設  $`w_i`$  的條件概率僅由  $`w_{i-1}`$  決定：

  
```math
P(w_i | w_{i-1})
```


  這樣的模型能夠捕捉到相鄰詞語之間的關聯，因此比 unigram 模型能夠生成更合理的語言序列。

- **Trigram 模型**  
  在 trigram 模型中，每個詞語的出現依賴於前兩個詞語，即條件概率由  $`w_{i-1}`$  和  $`w_{i-2}`$  決定：

  
```math
P(w_i | w_{i-1}, w_{i-2})
```


  這樣的模型可以捕捉更多的上下文信息，比 bigram 模型能夠更好地理解語言中的語法和語義結構。

- **Higher-order n-gram 模型**  
  更高階的 n-gram 模型則會依賴於更多的上下文，例如，四元模型（quadrigram）會依賴於前三個詞，五元模型則依賴於前四個詞。這些模型能夠捕捉更長範圍的語言依賴關係，但隨著 n 值的增大，計算和儲存的需求也會顯著增加。

#### **3. 訓練與參數估計**

n-gram 模型的訓練主要涉及估計各種詞對的條件概率。對於大多數 n-gram 模型，使用最大似然估計（MLE）來估算這些條件概率。具體地說，對於一個大語料庫，假設某個詞對  $`(w_{i-1}, w_i)`$  出現了  $`c(w_{i-1}, w_i)`$  次，並且  $`w_{i-1}`$  總共出現了  $`c(w_{i-1})`$  次，那麼  $`P(w_i | w_{i-1})`$  可以用以下公式計算：


```math
P(w_i | w_{i-1}) = \frac{c(w_{i-1}, w_i)}{c(w_{i-1})}
```


這樣的計算方法假設每個詞語對的出現次數反映了它們在語料庫中的概率分佈。

然而，實際語料庫中往往會存在很多未見過的詞對，這會導致某些詞的概率被估算為零。為了處理這個問題，通常會採用**平滑技術**。

#### **4. 平滑技術**

平滑技術的目的是調整估算的概率分佈，以避免零概率的情況。常見的平滑方法有：

- **加一平滑（Additive Smoothing）**  
  加一平滑是最簡單的一種平滑方法，它通過將每個詞對的計數加一來確保每個詞對的概率都不會為零。對於 bigram 模型，平滑後的概率估算公式為：

  
```math
P(w_i | w_{i-1}) = \frac{c(w_{i-1}, w_i) + 1}{c(w_{i-1}) + V}
```


  其中， $`V`$  是語料庫中不同詞的總數。

- **Katz平滑（Katz Smoothing）**  
  Katz平滑方法根據詞對的出現頻率進行調整，對於頻繁出現的詞對，它使用最大似然估計，而對於低頻詞對，則使用更少的權重來調整概率。

- **Good-Turing平滑（Good-Turing Smoothing）**  
  Good-Turing平滑根據未出現過的n-gram進行估算，並將未觀察到的n-gram視為某些詞對的縮減。

#### **5. n-gram 模型的挑戰與限制**

雖然 n-gram 模型在很多語言處理任務中都取得了不錯的效果，但它也存在若干挑戰和限制：

- **長程依賴問題**  
  n-gram 模型的最大問題之一是它只能捕捉固定長度的上下文關係，無法有效處理長程依賴（如語法結構或語篇層次的依賴）。當上下文越來越長，模型的表現就越差。

- **維度爆炸與計算開銷**  
  隨著n值的增大，n-gram模型的計算和儲存需求也會快速增加。特別是在較大規模語料庫上，訓練一個高階n-gram模型需要大量的記憶體和計算資源。

- **稀疏性問題**  
  由於語料庫中每個詞對的出現頻率分佈是不均勻的，某些較少見的詞對可能會遭遇稀疏問題，這對模型的性能產生不利影響。

#### **6. 從 n-gram 模型到神經網絡模型的過渡**

儘管 n-gram 模型曾經在語言處理領域佔據主導地位，但隨著神經網絡的發展，特別是長短期記憶（LSTM）網絡和 Transformer 網絡的興起，基於神經網絡的語言模型開始取代了傳統的統計語言模型。神經網絡模型可以捕捉更長範圍的語言依賴，並且能夠在不需要人工設計特徵的情況下進行學習，從而克服了 n-gram 模型的許多限制。

然而，n-gram 模型依然在許多簡單的應用中保持一定的價值，尤其是當語料庫相對較小或者資源受限時，n-gram 模型仍然是一個有效的選擇。

### 總結

n-gram 模型是一種簡單而直觀的語言建模方法，它依賴於上下文中固定長度的詞序列來預測下一個詞的出現概率。雖然它存在著長程依賴和計算開銷等問題，但在資源有限或語料庫規模較小的情況下，n-gram 模型仍然具有實用價值。隨著深度學習方法的發展，基於神經網絡的語言模型逐漸成為主流，但 n-gram 模型的基本思想仍然對當前的語言建模技術有著深遠的影響。