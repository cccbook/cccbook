### **深度學習基礎回顧**

深度學習（Deep Learning）是機器學習的一個子領域，它利用多層神經網絡（即深層神經網絡，Deep Neural Networks, DNN）進行模式識別和特徵學習。深度學習的興起，特別是在自然語言處理（Natural Language Processing, NLP）領域的應用，徹底改變了語言理解與生成的方式。本章節將回顧深度學習的基礎概念，介紹其在 NLP 中的應用，並對神經網絡的基本構建塊進行說明。

#### **1. 深度學習的核心概念**

深度學習主要依賴於神經網絡的多層結構。神經網絡模擬人腦神經元的結構，其中每一層的輸出都是基於前一層的計算結果。這些網絡模型包含了大量的參數（權重），這些參數通過學習數據中的模式來調整和優化。深度學習與傳統機器學習方法的區別在於，它能夠自動從數據中學習特徵表示，無需手動設計特徵。

##### **1.1 神經網絡基礎結構**

神經網絡由多個層次構成，最常見的結構包括以下幾個層次：

- **輸入層**：輸入層接收外部數據（例如圖像、文本或語音）。每一個節點代表輸入的一個特徵，這些特徵可以是圖像像素、詞嵌入或語音頻譜等。
  
- **隱藏層**：隱藏層是神經網絡的核心部分，負責學習數據中的複雜模式。每個隱藏層由許多神經元組成，每個神經元都對輸入數據進行加權求和後通過非線性激活函數進行處理。深度學習模型的“深度”指的就是隱藏層的層數。
  
- **輸出層**：輸出層根據神經網絡的任務進行設計。如果是分類任務，輸出層會將結果映射到不同的類別。如果是回歸任務，則輸出一個連續值。

##### **1.2 激活函數**

神經網絡中的每個神經元通過一個激活函數將加權和的輸入轉換為輸出。常見的激活函數有：

- **Sigmoid**：將輸出壓縮在 0 到 1 之間，常用於二分類問題的輸出層。
  
- **Tanh**：類似於 Sigmoid，但其範圍是 -1 到 1，常用於隱藏層。
  
- **ReLU（Rectified Linear Unit）**：將所有負值映射為零，並保持正值不變，這使得 ReLU 在計算上更高效，並且能夠減少梯度消失問題，是目前最常用的激活函數。

- **Softmax**：在多分類問題中使用，將輸出值轉換為概率分佈，通常用於輸出層。

#### **2. 優化與訓練**

神經網絡的訓練過程包括計算誤差並根據誤差調整網絡權重。這一過程通常使用反向傳播（Backpropagation）算法和梯度下降（Gradient Descent）算法來實現。

##### **2.1 反向傳播**

反向傳播是神經網絡訓練的關鍵算法，它通過計算損失函數對網絡權重的梯度，將誤差從輸出層逐步傳遞回隱藏層，從而更新權重。這樣的更新可以使得神經網絡在多次訓練後能夠更好地擬合數據。

##### **2.2 梯度下降**

梯度下降是一種優化算法，用於最小化損失函數。在每一次迭代中，梯度下降算法根據損失函數的梯度（即對權重的偏導數）更新權重。常見的梯度下降變種有：

- **批量梯度下降（Batch Gradient Descent）**：每次迭代都使用整個訓練集來計算梯度。
  
- **隨機梯度下降（Stochastic Gradient Descent, SGD）**：每次迭代僅使用一個樣本來計算梯度，這使得算法更快但不穩定。

- **小批量梯度下降（Mini-batch Gradient Descent）**：在每次迭代中使用一小批數據進行更新，這是當前最常用的方式，兼具效率和穩定性。

##### **2.3 優化算法**

除了基本的梯度下降方法外，還有許多優化算法能加速訓練過程，如：

- **Momentum**：通過引入慣性，使得更新過程更加平滑，並加速收斂。
  
- **Adam（Adaptive Moment Estimation）**：結合了 Momentum 和自適應學習率調整，並且被廣泛應用於深度學習任務。

#### **3. 深度學習在自然語言處理中的應用**

深度學習技術在 NLP 中的應用已經產生了深遠的影響，許多過去被認為難以解決的語言處理任務，如語音識別、機器翻譯、情感分析等，都得到了顯著改善。

##### **3.1 詞嵌入（Word Embedding）**

詞嵌入是深度學習在 NLP 中最重要的貢獻之一。通過將詞語映射到低維的連續向量空間，詞嵌入能夠捕捉詞語之間的語義關係。例如，詞語“國王”和“王后”的嵌入向量會比較相似，這些向量能夠幫助模型理解語義。

- **Word2Vec**：由 Google 提出的詞嵌入方法，通過 Skip-gram 或 CBOW 模型來學習詞語的嵌入表示。
  
- **GloVe**：由斯坦福大學提出的一種基於詞語共現矩陣的詞嵌入方法，它能夠捕捉全局語言統計信息。

##### **3.2 循環神經網絡（RNN）**

RNN 是一種特殊的神經網絡，能夠處理序列數據。在 NLP 中，RNN 可以用來處理語言的順序性，捕捉語句中的長程依賴。RNN 擅長於處理時間序列數據，能夠將先前的狀態作為當前輸入的一部分。

- **長短期記憶（LSTM）** 和 **門控循環單元（GRU）** 是 RNN 的兩種改進版本，它們解決了原始 RNN 中的梯度消失和梯度爆炸問題，並更有效地捕捉長程依賴。

##### **3.3 注意力機制與 Transformer**

在 NLP 中，最具影響力的發展之一是注意力機制的引入。注意力機制允許模型在處理當前單詞時，關注輸入序列中的其他單詞，這樣可以捕捉到詞語之間的複雜依賴關係。Transformer 架構正是基於注意力機制，並且完全拋棄了傳統的 RNN 結構，使得模型能夠進行並行計算，大大提高了訓練效率和性能。

#### **4. 小結**

深度學習的發展極大地促進了自然語言處理技術的進步，並且許多基於深度學習的模型在各種 NLP 任務中取得了顯著的突破。從基本的神經網絡結構，到 RNN、LSTM、GRU，再到注意力機制和 Transformer 模型，深度學習技術為語言建模、機器翻譯、情感分析等提供了強大的工具。隨著技術的發展，未來的深度學習模型將能夠處理更複雜的語言問題，並在各種應用中展現出更強的能力。