### **第 7 章：注意力機制的數學**

#### **7.1 自注意力數學表示**

自注意力（Self-Attention）是一種在序列建模中廣泛應用的機制，尤其是在自然語言處理（NLP）和計算機視覺等領域。自注意力機制允許模型在處理每個元素時，根據其他元素的上下文信息進行加權，使得模型能夠捕捉到序列中長距離依賴關係。這種機制是Transformer模型的核心之一，並且它的計算效率較高，尤其適用於長序列數據。

自注意力的數學表示可以分為以下幾個步驟：

#### **7.1.1 輸入表示**

假設輸入序列為 \( \mathbf{X} = [x_1, x_2, \dots, x_n] \)，其中每個 \( x_i \) 是輸入序列中的一個向量。這些向量通常來自於嵌入層或其他先前的網絡層。

為了計算自注意力，我們需要將每個輸入向量映射為三個向量：查詢向量（Query）、鍵向量（Key）和值向量（Value）。這些向量通常是通過與學習的權重矩陣相乘來生成的。

- 查詢向量 \( Q \)
- 鍵向量 \( K \)
- 值向量 \( V \)

對每個輸入 \( x_i \)，這些向量可以通過線性變換（即權重矩陣）來獲得：

\[
Q_i = W_Q x_i, \quad K_i = W_K x_i, \quad V_i = W_V x_i
\]

其中，\( W_Q, W_K, W_V \) 分別是查詢、鍵和值的學習權重矩陣。

#### **7.1.2 注意力權重計算**

注意力的核心思想是根據查詢和鍵的相似性來計算權重。具體而言，我們對每一個查詢向量 \( Q_i \) 計算它與所有鍵向量 \( K_j \) 之間的相似度，這可以通過內積來實現：

\[
\text{score}(Q_i, K_j) = Q_i \cdot K_j^T
\]

接著，這些分數經過一個 softmax 函數進行歸一化，從而得到每個鍵對應的注意力權重：

\[
\alpha_{ij} = \text{softmax}(Q_i \cdot K_j^T) = \frac{\exp(Q_i \cdot K_j^T)}{\sum_{k=1}^n \exp(Q_i \cdot K_k^T)}
\]

這裡，\( \alpha_{ij} \) 表示查詢 \( Q_i \) 對鍵 \( K_j \) 的注意力權重。

#### **7.1.3 輸出計算**

自注意力的輸出是每個值向量 \( V_j \) 的加權和。具體而言，對於查詢 \( Q_i \)，其對應的輸出 \( \mathbf{z}_i \) 是所有值向量的加權和，權重為注意力權重 \( \alpha_{ij} \)：

\[
\mathbf{z}_i = \sum_{j=1}^n \alpha_{ij} V_j
\]

這表示每個查詢向量 \( Q_i \) 都會根據其他所有位置的值向量加權計算出一個新的表示，從而將序列中的長距離依賴關係反映到每個輸出中。

#### **7.1.4 自注意力的矩陣表示**

將以上過程轉化為矩陣表示，可以更高效地進行計算。首先，我們將所有查詢、鍵和值向量組成矩陣：

\[
Q = \begin{bmatrix} Q_1 & Q_2 & \dots & Q_n \end{bmatrix}, \quad K = \begin{bmatrix} K_1 & K_2 & \dots & K_n \end{bmatrix}, \quad V = \begin{bmatrix} V_1 & V_2 & \dots & V_n \end{bmatrix}
\]

接下來，計算查詢與鍵的相似度矩陣：

\[
\text{score}(Q, K) = QK^T
\]

對此結果應用 softmax 函數來獲得注意力權重矩陣：

\[
\alpha = \text{softmax}(QK^T)
\]

最後，計算輸出矩陣 \( Z \)：

\[
Z = \alpha V
\]

這樣就可以同時計算出所有位置的加權和，得到最終的自注意力輸出。

#### **7.1.5 多頭注意力**

在實際應用中，為了提升模型的表達能力，通常會使用多頭注意力（Multi-Head Attention）機制。多頭注意力的基本思想是將查詢、鍵和值分別映射到多個子空間，進行多次注意力計算，然後將結果進行拼接或加權求和。

具體而言，我們將查詢、鍵和值分別分成 \( h \) 頭，每一頭進行獨立的注意力計算，然後將各頭的結果合併並再進行一次線性變換。多頭注意力的數學過程如下：

1. 將查詢、鍵和值分成多個子空間：
   
   \[
   Q = [Q_1, Q_2, \dots, Q_h], \quad K = [K_1, K_2, \dots, K_h], \quad V = [V_1, V_2, \dots, V_h]
   \]

2. 分別計算每一頭的注意力輸出：
   
   \[
   Z_i = \text{Attention}(Q_i, K_i, V_i)
   \]

3. 將所有頭的輸出拼接起來，並進行線性變換：

   \[
   Z_{\text{final}} = W_O \cdot \text{Concat}(Z_1, Z_2, \dots, Z_h)
   \]

其中，\( W_O \) 是一個學習的權重矩陣。

#### **7.1.6 小結**

自注意力機制的數學表示涉及以下幾個步驟：
- 將輸入向量映射為查詢、鍵和值向量。
- 計算查詢和鍵之間的相似度，並通過 softmax 函數獲得注意力權重。
- 根據注意力權重加權求和獲得輸出。
- 多頭注意力機制進一步提升模型的表達能力。

自注意力不僅能夠捕捉到序列中各位置之間的長距離依賴，還具有計算上的高效性，因此在許多深度學習任務中取得了顯著的成功。