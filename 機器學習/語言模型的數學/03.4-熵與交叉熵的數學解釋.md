### **3.4 熵與交叉熵的數學解釋**

熵和交叉熵是信息論中非常重要的概念，它們在機器學習和深度學習中有著廣泛的應用，特別是在分類問題和模型訓練過程中。熵衡量的是一個隨機變數的不確定性，而交叉熵則衡量兩個概率分佈之間的差異。這兩個概念在模型訓練和性能評估中起著至關重要的作用。

#### **3.4.1 熵（Entropy）**

熵（\( H \)）是信息論中的一個基本概念，用來度量隨機變數的不確定性或信息量。對於一個離散隨機變數 \( X \)，其熵定義為：

\[
H(X) = - \sum_{i=1}^{n} p(x_i) \log p(x_i)
\]

其中：
- \( p(x_i) \) 是隨機變數 \( X \) 取值 \( x_i \) 的概率。
- \( \log \) 是對數，通常使用以 2 為底的對數（以比特為單位）或以自然對數為底（以納特為單位）。

熵的物理意涵是：當隨機變數的分佈越為均勻（即所有事件的概率相近），不確定性越高，熵也越大；反之，若某一事件的概率接近 1，而其他事件的概率接近 0，則不確定性較低，熵較小。熵的最大值發生在所有可能結果的概率均勻分佈時。

##### **熵的性質**：
1. **非負性**：熵的值永遠不小於零，且當所有事件的概率都為 1 時，熵為零。
2. **最大熵**：當每個事件的概率都相等時，熵達到最大值。對於一個有 \( n \) 個可能結果的隨機變數，當每個事件的概率均為 \( \frac{1}{n} \) 時，熵為：
   \[
   H(X) = \log n
   \]
   這是熵的最大值。

#### **3.4.2 交叉熵（Cross Entropy）**

交叉熵（Cross Entropy）度量的是兩個概率分佈之間的差異，特別是在預測分佈和真實分佈之間的差異。對於兩個分佈 \( p \) 和 \( q \)，其中 \( p \) 是真實分佈，\( q \) 是預測分佈，交叉熵定義為：

\[
H(p, q) = - \sum_{i=1}^{n} p(x_i) \log q(x_i)
\]

這裡：
- \( p(x_i) \) 是真實分佈的概率；
- \( q(x_i) \) 是預測分佈的概率。

交叉熵可以看作是熵和 Kullback-Leibler 散度的和。具體地說，交叉熵包含了真實分佈的熵和兩個分佈之間的差異（即 KL 散度）。如果 \( p \) 和 \( q \) 完全相同，那麼交叉熵就等於熵，即 \( H(p, p) = H(p) \)。如果 \( p \) 和 \( q \) 相差較大，交叉熵則會大於真實分佈的熵。

##### **交叉熵的應用**：
- **在分類問題中的應用**：在機器學習，特別是深度學習中的分類問題中，交叉熵用作損失函數來衡量模型預測分佈與真實分佈之間的差異。對於二分類問題，交叉熵損失函數是：
  \[
  \mathcal{L}_{CE}(y, \hat{y}) = - \left( y \log \hat{y} + (1 - y) \log (1 - \hat{y}) \right)
  \]
  其中 \( y \) 是真實標籤，\( \hat{y} \) 是模型的預測概率。對於多分類問題，交叉熵損失函數則為：
  \[
  \mathcal{L}_{CE}(y, \hat{y}) = - \sum_{i=1}^{K} y_i \log \hat{y}_i
  \]
  其中 \( K \) 是類別數，\( y_i \) 是真實標籤的 one-hot 編碼，\( \hat{y}_i \) 是對應類別的預測概率。

- **在優化過程中的應用**：交叉熵損失在訓練模型時，會鼓勵模型對真實標籤的預測概率更高，因此可以有效地用於優化過程，最小化預測分佈與真實分佈之間的差異。

#### **3.4.3 熵與交叉熵之間的關係**

熵和交叉熵之間有著密切的關係。對於同一個真實分佈 \( p \)，交叉熵和熵的關係可以寫為：

\[
H(p, q) = H(p) + D_{KL}(p \| q)
\]

其中：
- \( H(p, q) \) 是真實分佈 \( p \) 和預測分佈 \( q \) 之間的交叉熵；
- \( H(p) \) 是真實分佈的熵；
- \( D_{KL}(p \| q) \) 是 Kullback-Leibler 散度，衡量兩個分佈之間的差異。

這個公式顯示，交叉熵等於真實分佈的熵加上預測分佈和真實分佈之間的散度。當預測分佈 \( q \) 越接近真實分佈 \( p \) 時，交叉熵的值就越小。反之，當兩者相差較大時，交叉熵會增加。

#### **3.4.4 小結**

1. **熵**衡量的是隨機變數的內在不確定性，描述了事件的平均信息量。當所有事件的概率均勻時，熵最大；當某個事件的概率為 1 時，熵最小。
   
2. **交叉熵**度量的是兩個概率分佈之間的差異，常用於機器學習中的損失函數，尤其是在分類問題中。交叉熵包含了真實分佈的熵和兩個分佈之間的KL散度。

3. 交叉熵最小化的過程相當於在優化模型時最小化預測分佈與真實分佈之間的差異，這是訓練分類模型（如邏輯回歸或神經網絡）時常見的做法。

通過這些數學推導與應用，我們可以清楚地理解熵與交叉熵在不同情境下的作用，並利用它們來提升機器學習模型的效果。