### **6.6 正則化與批量正規化的數學原理**

在深度學習中，正則化和批量正規化（Batch Normalization）是兩種常用的技術，用於提升模型的泛化能力和訓練效率。正則化主要是通過對模型的複雜度進行約束，避免過擬合，而批量正規化則是通過對每層的輸入進行標準化處理，改善訓練過程中的收斂速度和穩定性。

#### **6.6.1 正則化**

正則化（Regularization）是用來防止模型過擬合的技術，它通過在損失函數中加入額外的懲罰項來限制模型的複雜度。常見的正則化方法包括L1正則化、L2正則化和彈性網絡（Elastic Net）正則化。

**6.6.1.1 L1 正則化（Lasso 正則化）**

L1 正則化的基本思想是通過將模型的權重向量的L1范數加入損失函數，從而達到約束模型權重的效果。L1正則化有助於產生稀疏的模型，即使部分權重變為零，這對特徵選擇有很好的作用。

**數學表達式：**

假設模型的損失函數為 \( J(\theta) \)，則加入L1正則化後的損失函數為：

\[
J_{\text{L1}}(\theta) = J(\theta) + \lambda \sum_{i=1}^n |\theta_i|
\]

其中，\( \lambda \) 是正則化超參數，控制正則化的強度，\( \theta_i \) 是第 \( i \) 個權重。

**6.6.1.2 L2 正則化（Ridge 正則化）**

L2 正則化是通過將權重向量的L2范數（即權重平方和）加入損失函數來約束模型的複雜度。與L1正則化相比，L2正則化會將權重推向0，但不會使它們完全為0，從而使得模型更加平滑。

**數學表達式：**

L2正則化的損失函數為：

\[
J_{\text{L2}}(\theta) = J(\theta) + \lambda \sum_{i=1}^n \theta_i^2
\]

**6.6.1.3 彈性網絡（Elastic Net）**

彈性網絡是L1正則化和L2正則化的結合，能夠同時實現特徵選擇和權重平滑。它使用了一個超參數來平衡L1和L2正則化的貢獻。

**數學表達式：**

彈性網絡正則化的損失函數為：

\[
J_{\text{Elastic Net}}(\theta) = J(\theta) + \lambda_1 \sum_{i=1}^n |\theta_i| + \lambda_2 \sum_{i=1}^n \theta_i^2
\]

其中，\( \lambda_1 \) 和 \( \lambda_2 \) 是L1和L2正則化的超參數。

**正則化的作用：**
- **防止過擬合**：通過限制模型複雜度，減少過度擬合訓練數據的風險。
- **提高泛化能力**：通過正則化，模型能夠學習到更為簡單、穩定的模式，從而提升對未見數據的預測能力。

#### **6.6.2 批量正規化（Batch Normalization）**

批量正規化（Batch Normalization, BN）是一種深度學習中常用的技術，旨在加速神經網絡的訓練過程，並改善其性能。它通過對每層的輸入進行標準化處理，將每層的激活值移至具有零均值和單位方差的分佈，從而消除內部協變量轉移問題。

**數學原理：**

假設對於某一層的輸入，該層的激活值為 \( x_1, x_2, \dots, x_m \)，其中 \( m \) 是批量的大小。對這些激活值進行批量標準化的過程如下：

1. **計算均值和方差：**

   對於每一個維度（即每一個特徵），計算批量數據的均值和方差：

   \[
   \mu_B = \frac{1}{m} \sum_{i=1}^m x_i, \quad \sigma_B^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_B)^2
   \]

2. **標準化：**

   將每個輸入 \( x_i \) 通過以下公式進行標準化：

   \[
   \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
   \]

   其中，\( \epsilon \) 是防止分母為零的小常數，通常取 \( 10^{-8} \)。

3. **重新縮放和平移：**

   標準化後的激活值會進一步進行縮放和位移，使得網絡可以學習更靈活的表達。這是通過引入可學習的參數 \( \gamma \)（縮放因子）和 \( \beta \)（平移因子）來實現的：

   \[
   y_i = \gamma \hat{x}_i + \beta
   \]

   其中，\( \gamma \) 和 \( \beta \) 是可學習的參數。

**數學表達式總結：**

整個批量正規化的過程可以表示為：

\[
y_i = \gamma \left( \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \right) + \beta
\]

**批量正規化的作用：**
- **加速訓練**：通過標準化每層的輸入，避免了不同層之間的梯度消失或爆炸問題，從而加速了訓練過程。
- **減少內部協變量轉移**：在訓練過程中，模型的輸入分佈會發生變化，這會使得每層的參數更新變得更加困難。批量正規化有助於減少這種內部協變量轉移。
- **一定程度上防止過擬合**：儘管批量正規化本身並不是一種正則化技術，但它引入了隨機性（批量的不同樣本），在某些情況下可起到類似於正則化的作用。

#### **6.6.3 小結**

- **正則化**：通過對損失函數加入懲罰項（如L1、L2正則化），正則化技術有助於防止模型過擬合，提升泛化能力。
- **批量正規化**：通過對每層的輸入進行標準化處理，批量正規化技術能加速訓練，減少內部協變量轉移，並改善模型的訓練穩定性。

這些技術的使用可以顯著提高深度學習模型的效果和訓練效率，是現代深度學習中不可或缺的部分。