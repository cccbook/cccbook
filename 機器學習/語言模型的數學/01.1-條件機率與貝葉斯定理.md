### **第 1 章：基礎機率理論**

#### **1.1 條件機率與貝葉斯定理**

**條件機率** 是指在已知某些事件已經發生的情況下，另一事件發生的機率。具體來說，給定事件 \( B \) 已經發生，我們計算事件 \( A \) 發生的條件機率。條件機率的數學定義為：

\[
P(A|B) = \frac{P(A \cap B)}{P(B)} \quad \text{(當 \( P(B) > 0 \))}
\]

這裡：
- \( P(A|B) \) 是事件 \( A \) 在事件 \( B \) 發生的條件下發生的機率。
- \( P(A \cap B) \) 是事件 \( A \) 和 \( B \) 同時發生的機率。
- \( P(B) \) 是事件 \( B \) 發生的機率。

條件機率反映了事件之間的依賴關係。例如，在語言模型中，計算某個詞語出現的機率可能依賴於它前面的詞語，因此需要使用條件機率來表示這種依賴性。

**貝葉斯定理** 是機率論中的一個重要定理，它用於計算某事件的後驗機率，即在觀察到某些證據（事件 \( B \)）後，對事件 \( A \) 發生的機率的更新。貝葉斯定理的數學表達式為：

\[
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
\]

這裡：
- \( P(A|B) \) 是事件 \( A \) 在事件 \( B \) 發生的條件下發生的機率，這是我們需要計算的後驗機率。
- \( P(B|A) \) 是事件 \( B \) 在事件 \( A \) 發生的條件下發生的機率。
- \( P(A) \) 是事件 \( A \) 發生的先驗機率。
- \( P(B) \) 是事件 \( B \) 發生的邊際機率。

**貝葉斯定理的應用：**
貝葉斯定理在語言模型中廣泛應用，尤其是在生成式模型中，例如 GPT 類模型。通過貝葉斯定理，模型可以根據已知的語言模式來更新其對語言的理解，這有助於生成更自然的語句或預測下文。

**貝葉斯推論：**
在語言模型中，貝葉斯推論通常用於從大量文本數據中學習概率分佈，並通過更新模型的參數來優化預測。這個過程的核心是依賴於貝葉斯定理，根據新的數據（例如語料庫中的單詞或句子）來調整語言模型。

#### **例子：語言模型中的貝葉斯定理**
在語言模型中，計算單詞序列的概率，通常是根據前面的單詞來估計下個單詞的機率。假設我們有一個單詞序列 \( w_1, w_2, \dots, w_n \)，並且我們想要估計 \( P(w_n | w_1, w_2, \dots, w_{n-1}) \)，即給定前 \( n-1 \) 個單詞，計算第 \( n \) 個單詞的機率。這樣的計算可以利用貝葉斯定理進行更新和推導，從而在不斷獲取新數據時調整模型的預測。

總結來說，條件機率和貝葉斯定理是語言模型中不可或缺的數學工具。它們能夠有效地處理語言中的不確定性，並幫助模型在多變的語境中作出合理的預測和推斷。