### **9.2 掩碼語言模型（例如 BERT）**

掩碼語言模型（Masked Language Model，簡稱 MLM）是目前自然語言處理領域的一種重要技術，特別是在預訓練模型（如BERT）中得到了廣泛的應用。掩碼語言模型與生成式語言模型（如GPT）有所不同，主要的區別在於訓練目標和生成策略。BERT是基於掩碼語言模型的代表性模型，主要用於理解和推斷語言中的語義。

#### **9.2.1 掩碼語言模型的基本概念**

掩碼語言模型的目標是學習一個語言模型，使得模型能夠根據上下文來預測被隨機掩碼的詞語。與生成式語言模型不同，掩碼語言模型並不是基於自回歸的方式逐步生成詞語，而是進行詞語的填空任務，即在給定一部分上下文的情況下，預測被掩碼的詞語。

假設給定一個句子 \( S = (w_1, w_2, \dots, w_n) \)，其中某些詞語 \( w_t \) 被掩碼，模型的目標是根據上下文來預測這些被掩碼的詞語。具體來說，掩碼語言模型的任務是學習對以下條件概率的預測：

\[
P(w_t | w_1, w_2, \dots, w_{t-1}, w_{t+1}, \dots, w_n)
\]

其中，\( w_t \) 是被掩碼的詞語，模型的目標是根據上下文來預測它。

#### **9.2.2 BERT模型的結構與工作原理**

BERT（Bidirectional Encoder Representations from Transformers）是基於掩碼語言模型的一種預訓練模型。與GPT模型不同，BERT使用了Transformer的編碼器部分，並且在預訓練過程中採用了雙向上下文（bidirectional context）來理解句子。這一特點使得BERT能夠捕捉到詞語之間的雙向依賴，而不像GPT那樣僅依賴於前文的上下文。

BERT的結構主要由以下部分組成：

1. **編碼器架構（Encoder Architecture）**：BERT基於Transformer的編碼器結構。編碼器能夠從序列的每一個詞語中提取出上下文相關的信息，並對每個詞語進行雙向建模。

2. **掩碼語言模型（Masked Language Model，MLM）**：在訓練過程中，BERT會將輸入序列中的一部分詞語隨機掩碼，並要求模型根據剩餘的上下文來預測這些掩碼的詞語。這一過程使得模型能夠學習到更豐富的語義信息。

3. **Next Sentence Prediction（NSP）**：BERT的訓練目標除了掩碼語言模型外，還包括一個名為Next Sentence Prediction（NSP）的任務。該任務的目的是讓模型學習句子之間的關係，即判斷給定的一對句子 \( A \) 和 \( B \) 是否相鄰於原文中。

#### **9.2.3 BERT的數學模型**

BERT模型的輸入是由詞嵌入（word embeddings）、位置編碼（positional encoding）和段落編碼（segment encoding）組成的。這些信息被輸入到Transformer的編碼器中。對於一個包含掩碼的句子 \( S \)，我們有以下的表示：

1. **詞嵌入（Word Embeddings）**：每個詞 \( w_t \) 被映射到一個向量 \( \mathbf{w_t} \)，這是詞嵌入向量。
   
2. **位置編碼（Positional Encoding）**：由於Transformer結構不具有處理序列順序的能力，BERT將每個詞的位置信息加到詞嵌入中。位置編碼通常使用正弦和餘弦函數來編碼詞語的位置信息。

3. **段落編碼（Segment Encoding）**：BERT會將句子對的第一個句子和第二個句子分別標記為A段和B段，這有助於模型學習句子之間的關聯。

假設輸入序列是 \( x = (x_1, x_2, \dots, x_n) \)，每個詞 \( x_i \) 經過詞嵌入、位置編碼和段落編碼後的表示為 \( E(x_i) \)。這些輸入被傳遞進Transformer編碼器，並且對每個位置 \( t \)，BERT的輸出是：

\[
h_t = \text{TransformerEncoder}(E(x_1), E(x_2), \dots, E(x_n))
\]

隨後，對於被掩碼的位置 \( t \)，BERT會預測其對應的詞語 \( w_t \)，即預測該位置的詞的概率分佈：

\[
P(w_t | w_1, w_2, \dots, w_{t-1}, w_{t+1}, \dots, w_n) = \text{softmax}(W h_t + b)
\]

其中，\( W \) 是詞彙表的權重矩陣，\( h_t \) 是第 \( t \) 個位置的隱藏狀態，\( b \) 是偏置項。

#### **9.2.4 訓練BERT模型**

BERT模型的訓練目標是最大化對掩碼的詞語和句子關聯的預測準確性。具體來說，BERT的損失函數包含兩個部分：

1. **掩碼語言模型（MLM）損失**：對於每個被掩碼的詞，BERT通過最大化預測其正確詞語的概率來最小化這部分損失。

\[
\mathcal{L}_{MLM} = - \sum_{t \in M} \log P(w_t | w_1, w_2, \dots, w_{t-1}, w_{t+1}, \dots, w_n)
\]

其中，\( M \) 是掩碼的位置集。

2. **下一句預測（NSP）損失**：BERT還通過最大化預測兩句子是否相鄰的準確性來最小化NSP損失。

\[
\mathcal{L}_{NSP} = - \log P(\text{IsNext}) \text{ or } - \log P(\text{NotNext})
\]

最終，BERT的總損失是這兩個部分損失的加權和：

\[
\mathcal{L}_{BERT} = \mathcal{L}_{MLM} + \alpha \mathcal{L}_{NSP}
\]

其中，\( \alpha \) 是權重超參數。

#### **9.2.5 訓練與推理過程**

在預訓練階段，BERT通過上述兩個任務進行訓練。訓練完成後，BERT可以用於下游任務，例如文本分類、命名實體識別（NER）、問答系統等。這些任務的特點是，BERT在預訓練過程中學到的語言知識可以被用來提高在具體任務上的表現。BERT使用微調（fine-tuning）技術進行任務特定的調整，這使得BERT可以廣泛應用於各種自然語言處理任務。

#### **9.2.6 結論**

BERT模型通過引入掩碼語言模型和雙向上下文建模，在語言理解方面取得了顯著的成效。它的預訓練和微調過程使得BERT成為一個靈活且強大的語言模型，能夠適應多種自然語言處理任務。掩碼語言模型的設計理念和BERT的成功激發了大量後續基於掩碼語言模型的改進和變體，如RoBERTa、ALBERT等。