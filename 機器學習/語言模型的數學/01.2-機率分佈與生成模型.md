#### **1.2 機率分佈與生成模型**

**機率分佈** 是描述隨機變數取值的可能性分佈情況的數學對象。它告訴我們在某一試驗中，各種可能結果的機率如何分布。在語言模型中，機率分佈用來描述單詞或短語的出現機率，從而有助於進行語言生成和理解。

常見的機率分佈有以下幾種類型：

- **離散機率分佈**：用於描述離散隨機變數的分佈，例如骰子擲出的結果。對應的分佈有二項分佈、泊松分佈等。
- **連續機率分佈**：用於描述連續隨機變數的分佈，如正態分佈、指數分佈等。
- **條件機率分佈**：描述在某一條件下事件發生的機率，例如在語言模型中，給定前一個單詞的情況下，計算當前單詞出現的機率。

在語言模型中，我們通常需要處理的是條件機率分佈。例如，假設 \( w_1, w_2, \dots, w_n \) 是一個語言序列，語言模型的目標是計算給定前 \( n-1 \) 個單詞的情況下，第 \( n \) 個單詞的機率：

\[
P(w_n | w_1, w_2, \dots, w_{n-1})
\]

這個條件機率分佈描述了語言序列中的依賴關係，是語言模型的核心。

### **生成模型（Generative Models）**

生成模型是一種統計模型，它能夠學習數據的生成過程，從而能夠生成新的數據樣本。在語言模型中，生成模型的目標是學習給定某些條件下生成文本的過程。例如，在自回歸語言模型中，生成過程是逐詞生成的，模型會根據前面生成的單詞來預測下個單詞。

生成模型的核心思想是學習數據的概率分佈，並通過這個分佈來生成新的樣本。在語言建模中，生成模型試圖描述如何根據某些給定的條件（如上下文或先前的詞語）生成一個合理的語言序列。

**生成模型的類型：**
1. **自回歸模型**：這類模型在生成序列時，每一步都基於先前的輸出進行預測。例如，n-gram模型就是一種自回歸模型，給定前幾個單詞，它預測下個單詞。
   - 在神經網絡中，像是 GPT（Generative Pre-trained Transformer）就是一種典型的自回歸生成模型。它會逐步生成文本，根據當前的上下文來預測下一個單詞。

2. **變分自編碼器（VAE）**：VAE 是一種基於變分推斷的生成模型，通過最大化變分下界來學習數據的隱含分佈。在語言建模中，VAE 可以用於生成具有潛在結構的語句。

3. **生成對抗網絡（GAN）**：GAN 通過對抗訓練生成樣本，並在生成模型和判別模型之間進行博弈。雖然 GAN 主要應用於圖像生成，但它也可以應用於語言生成，特別是在強化學習等領域中。

### **機率生成模型的數學基礎**

生成模型的數學基礎是基於機率論的。我們可以將生成過程建模為一個隱含的機率過程，其中每個詞語的生成都是基於先前詞語的條件機率分佈。例如，在自回歸模型中，生成序列的機率可以表示為：

\[
P(w_1, w_2, \dots, w_n) = \prod_{i=1}^{n} P(w_i | w_1, w_2, \dots, w_{i-1})
\]

這裡，生成序列的機率是各個條件機率的連乘積。這樣的模型假設詞語之間的依賴關係是線性的，每個詞語只依賴於前面的詞語。

生成模型的另一個重要概念是**最大似然估計（MLE）**，它用來學習生成模型的參數。給定一組訓練數據，最大似然估計通過最大化數據的觀察機率來估計模型參數，具體地說，對於語言模型來說，MLE 會根據訓練語料庫來學習最有可能生成語料庫的參數。

\[
\hat{\theta} = \arg \max_{\theta} \prod_{i=1}^{N} P(w_i | \theta)
\]

這裡 \( \theta \) 是生成模型的參數，\( N \) 是語料庫的大小。最大似然估計會調整參數 \( \theta \)，使得觀察到的語料庫的機率最大化。

### **語言模型中的生成式方法**

在語言模型中，生成模型可以用來生成自然語言文本，這些生成模型通常依賴於強大的深度學習架構，如 RNN、LSTM、Transformer 等。通過對大量文本的學習，這些生成模型能夠捕捉語言的語法、語意及其內在結構，從而生成流暢且符合語言規則的文本。

總結來說，機率分佈與生成模型是語言模型的基礎，它們使得模型能夠從數據中學習語言的結構，並根據這些學到的結構生成新的文本或進行語言理解。