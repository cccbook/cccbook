### **第 5 章：詞嵌入與向量空間模型**

#### **5.1 線性代數基礎：向量運算與矩陣分解**

在自然語言處理（NLP）中，詞嵌入（Word Embeddings）和向量空間模型（Vector Space Models）是處理語言數據的核心工具。這些模型的基礎依賴於線性代數中的向量運算和矩陣分解技術。本節將介紹一些與詞嵌入和向量空間模型密切相關的線性代數基礎概念，並展示如何利用這些概念來有效地表示和操作語言數據。

#### **5.1.1 向量運算**

在向量空間模型中，單詞和其他語言單位（如句子、段落等）被表示為高維向量。這些向量通常位於一個多維空間中，其中每個維度代表了某種語言特徵。向量運算是操作這些語言向量的核心工具，常見的運算包括：

- **向量加法：** 若 \( \mathbf{v}_1 \) 和 \( \mathbf{v}_2 \) 是兩個向量，則它們的加法 \( \mathbf{v}_1 + \mathbf{v}_2 \) 結果是將它們的對應分量相加。這種運算可以用來表示語言中不同詞語之間的語義關係，例如「國王」與「女王」的關係。

- **向量減法：** 向量減法 \( \mathbf{v}_1 - \mathbf{v}_2 \) 表示從 \( \mathbf{v}_1 \) 中去掉 \( \mathbf{v}_2 \) 所代表的語義。這在某些語言模型中，用來表達語義的變化，如「男人」和「女人」的差異。

- **點積（內積）：** 點積是兩個向量之間的標量乘積，對於向量 \( \mathbf{v}_1 \) 和 \( \mathbf{v}_2 \)，其點積定義為：
  
  \[
  \mathbf{v}_1 \cdot \mathbf{v}_2 = \sum_{i=1}^{n} v_{1i} \cdot v_{2i}
  \]

  點積的結果反映了兩個向量之間的相似度。若兩個向量方向相同，點積的值將較大，反之則較小。點積常用於測量詞嵌入向量之間的相似度。

- **向量長度（範數）：** 向量的長度（或範數）表示了向量的大小，通常使用 \( L_2 \) 範數來計算，即：
  
  \[
  \|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2}
  \]

  向量的長度常常被用來測量一個詞的“強度”或其在語言空間中的位置。

#### **5.1.2 矩陣分解**

在向量空間模型中，矩陣分解技術是一種強大的工具，可以用來降維並提取數據中隱含的結構信息。矩陣分解通過將大矩陣分解為幾個小矩陣來捕捉數據中的潛在特徵，這對於詞嵌入模型的訓練和表示尤為重要。常見的矩陣分解方法包括：

- **奇異值分解（SVD）：** 奇異值分解是最常見的矩陣分解方法之一。給定一個矩陣 \( A \)（例如詞-詞共現矩陣），其奇異值分解可以寫成：
  
  \[
  A = U \Sigma V^T
  \]

  其中，\( U \) 和 \( V \) 分別是左奇異向量和右奇異向量矩陣，\( \Sigma \) 是對角矩陣，包含了奇異值。奇異值分解能夠將高維矩陣降維，保留數據中最重要的結構信息，這對於詞嵌入的降維非常有用。

- **非負矩陣分解（NMF）：** 非負矩陣分解是一種將非負矩陣分解為兩個非負矩陣的方法，常用於處理文本數據。NMF 的優勢在於它能夠生成具有解釋性的詞嵌入，這是因為它保證了矩陣分解中的所有元素都為非負數，這與語言的語義結構相符。

- **主成分分析（PCA）：** 主成分分析是另一種常見的降維技術，用於從高維數據中提取主要成分。PCA 通過對數據矩陣進行特徵分解，選擇最重要的成分來表示數據，這樣可以在保留關鍵特徵的情況下降低數據的維度。PCA 常用於詞嵌入中的降維處理。

#### **5.1.3 詞嵌入與向量空間模型**

詞嵌入是一種將單詞或語言單位映射到固定維度向量空間中的方法。這些向量捕捉了語言單位之間的語義相似性和結構特徵。在詞嵌入模型中，通過訓練詞語的向量表示來學習語言的語義關聯。

- **詞嵌入的訓練方法：** 常見的詞嵌入模型包括 Word2Vec 和 GloVe。Word2Vec 基於上下文窗口的思想，通過最大化上下文詞語的條件概率來訓練詞向量。GloVe 則是基於詞-詞共現矩陣的分解來學習詞嵌入。

- **詞嵌入的幾何解釋：** 詞嵌入模型生成的詞向量可以在高維空間中進行視覺化。這些向量之間的幾何關係通常反映了語言中的語義結構。例如，通過向量加法和減法，能夠捕捉語言中的語義關係，如「國王」與「女王」的關係可以表示為 \( \mathbf{v}_{\text{國王}} - \mathbf{v}_{\text{男人}} + \mathbf{v}_{\text{女人}} \approx \mathbf{v}_{\text{女王}} \)。

#### **5.1.4 小結**

本節介紹了向量運算和矩陣分解的基本概念，這些概念是理解和操作詞嵌入及向量空間模型的基礎。向量運算，如加法、減法和點積，能夠幫助我們理解語言中不同詞語之間的關係；而矩陣分解技術，如奇異值分解（SVD）和非負矩陣分解（NMF），則為我們提供了強大的降維和結構提取工具，這些工具對於構建高效的詞嵌入模型至關重要。