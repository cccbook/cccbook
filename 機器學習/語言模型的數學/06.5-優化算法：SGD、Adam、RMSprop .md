### **6.5 優化算法：SGD、Adam、RMSprop**

在深度學習中，優化算法是訓練神經網絡的關鍵。這些算法通過調整模型的參數（如權重和偏置）來最小化損失函數。常見的優化算法有隨機梯度下降（SGD）、Adam（自適應矩估計）和RMSprop。這些算法各自有不同的特性和應用場景，能夠有效提升神經網絡的訓練效率和收斂速度。

#### **6.5.1 隨機梯度下降（SGD）**

隨機梯度下降（Stochastic Gradient Descent, SGD）是最簡單且最常用的優化算法之一。與批量梯度下降（Batch Gradient Descent）相比，SGD每次僅使用一個樣本來計算梯度，這使得每一步更新都更加快速，並且能夠跳出局部最小值。

**數學表達式：**

\[
\theta_{t+1} = \theta_t - \eta \nabla_{\theta} J(\theta_t; x^{(i)}, y^{(i)})
\]

其中：
- \( \theta_t \) 表示模型的參數。
- \( \eta \) 是學習率。
- \( \nabla_{\theta} J(\theta_t; x^{(i)}, y^{(i)}) \) 是在樣本 \( (x^{(i)}, y^{(i)}) \) 上的梯度。

**特點：**
- **計算效率高**：由於每次僅使用一個樣本計算梯度，計算速度較快。
- **噪聲大**：由於每次更新僅基於一個樣本，更新過程中包含較大的隨機波動，這使得SGD的收斂過程較為不穩定。
- **跳出局部最小值**：這些隨機波動有助於算法跳出局部最小值，達到全局最小值。

**缺點：**
- 學習率選擇非常關鍵，過大會導致發散，過小會使訓練過慢。

#### **6.5.2 Adam（自適應矩估計）**

Adam（Adaptive Moment Estimation）是目前最流行的優化算法之一，它結合了動量法和RMSprop的優點，對每個參數使用不同的學習率，並且在每次更新時考慮歷史梯度的指數衰減平均值。Adam在大多數情況下能夠快速收斂，並且對學習率的選擇較為不敏感。

**數學表達式：**

Adam的更新規則基於以下幾個步驟：

1. 計算梯度的動量（第一階矩估計）和梯度的平方的指數衰減平均（第二階矩估計）：
   
   \[
   m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
   \]
   \[
   v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
   \]
   其中 \( g_t \) 是當前的梯度，\( m_t \) 是梯度的一階矩，\( v_t \) 是梯度的二階矩，\( \beta_1 \) 和 \( \beta_2 \) 是衰減率。

2. 偏差修正，將初始時刻的偏差修正為1：

   \[
   \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
   \]

3. 更新參數：

   \[
   \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
   \]

其中，\( \eta \) 是學習率，\( \epsilon \) 是防止分母為零的小常數。

**特點：**
- **自適應學習率**：Adam對每個參數有不同的學習率，可以根據參數的歷史梯度自動調整學習率。
- **較快收斂**：由於其自適應性和使用了動量，Adam能夠在大多數情況下比SGD更快收斂。
- **較小的記憶需求**：雖然需要存儲一階和二階矩，但相比於需要存儲整個數據集的批量梯度下降，Adam仍然具有較小的記憶需求。

**缺點：**
- 雖然Adam通常能快速收斂，但在某些情況下可能會導致過擬合，尤其是在小數據集上訓練時。

#### **6.5.3 RMSprop**

RMSprop（Root Mean Square Propagation）是另一種常用的優化算法，它對每個參數使用一個自適應的學習率，這個學習率根據歷史梯度的平方來調整，這樣可以減少梯度消失的問題。RMSprop在處理非平穩目標（如神經網絡訓練中的隨機性）時非常有效。

**數學表達式：**

RMSprop的更新規則如下：

1. 計算梯度的平方的指數衰減平均：

   \[
   v_t = \beta v_{t-1} + (1 - \beta) g_t^2
   \]

2. 更新參數：

   \[
   \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} g_t
   \]

其中，\( \beta \) 是衰減率，\( g_t \) 是當前的梯度，\( v_t \) 是梯度平方的指數衰減平均，\( \eta \) 是學習率，\( \epsilon \) 是防止分母為零的小常數。

**特點：**
- **自適應學習率**：RMSprop根據每個參數的歷史梯度調整學習率，這使得其能在不同參數上選擇不同的學習率。
- **對超參數敏感度較低**：RMSprop比SGD對學習率的選擇更為寬容，能在更多的情況下有效工作。

**缺點：**
- 雖然RMSprop在很多情況下都能良好工作，但其對於學習率的選擇依然需要一些微調。

#### **6.5.4 小結**

- **SGD**：最基本的優化算法，適用於大型數據集，能夠跳出局部最小值，但收斂較慢且噪聲大。
- **Adam**：自適應學習率的優化算法，能夠快速收斂，且對學習率的選擇較不敏感，但有時可能會過擬合。
- **RMSprop**：解決了梯度爆炸和消失的問題，適合於處理非平穩目標，並且能夠有效應對動態數據。

選擇合適的優化算法取決於具體的應用場景和問題，理解這些算法的原理和特點可以幫助在實際訓練中取得更好的效果。