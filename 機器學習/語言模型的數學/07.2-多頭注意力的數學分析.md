### **7.2 多頭注意力的數學分析**

多頭注意力（Multi-Head Attention）是自注意力機制的延伸，旨在捕捉輸入序列中多種不同的注意力模式。這使得模型能夠從不同的子空間學習到更豐富的特徵表示。具體而言，多頭注意力機制將查詢（Query）、鍵（Key）和值（Value）分成多個子空間，並在每個子空間上獨立進行注意力計算，然後將各個頭的結果合併，進行最終的加權計算。

#### **7.2.1 多頭注意力的數學流程**

在傳統的自注意力中，對每個查詢 \(Q_i\) 和鍵 \(K_j\) 的相似度計算以及值向量 \(V_j\) 的加權和計算是單一的。但在多頭注意力中，這一過程被並行化並分配到多個“頭”上。具體的數學過程如下：

1. **將查詢、鍵和值向量分為多個子空間（頭）**：

   假設有 \( h \) 個注意力頭，每個頭對應一個不同的子空間。對每個輸入向量 \(x_i\)，我們通過學習到的權重矩陣 \(W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}\)（其中 \( i \in \{1, 2, \dots, h\} \)）將其映射為不同的查詢、鍵和值向量子空間：

   \[
   Q_i = W_Q^{(i)} x_i, \quad K_i = W_K^{(i)} x_i, \quad V_i = W_V^{(i)} x_i \quad \text{for each head } i
   \]

   這裡，\( Q_i \), \( K_i \), \( V_i \) 分別表示第 \( i \) 個注意力頭中的查詢、鍵和值向量。

2. **每個頭分別計算注意力輸出**：

   每個注意力頭將計算自己的注意力分數 \( \alpha_{ij}^{(i)} \)，並生成對應的加權和輸出 \( Z_i \)。具體過程如下：
   
   - 計算查詢和鍵的相似度：
     \[
     \text{score}(Q_i, K_j) = Q_i \cdot K_j^T
     \]
   
   - 通過 softmax 函數計算每對查詢-鍵的注意力權重：
     \[
     \alpha_{ij}^{(i)} = \frac{\exp(Q_i \cdot K_j^T)}{\sum_{k=1}^n \exp(Q_i \cdot K_k^T)}
     \]

   - 計算加權和輸出：
     \[
     Z_i = \sum_{j=1}^n \alpha_{ij}^{(i)} V_j
     \]
   
   每個頭 \( i \) 都會生成一個加權和輸出 \( Z_i \)。

3. **合併各頭的輸出**：

   完成每個頭的計算後，我們將各個頭的結果拼接起來。這樣可以得到一個更高維度的表示：
   \[
   Z_{\text{concat}} = \text{Concat}(Z_1, Z_2, \dots, Z_h)
   \]
   這裡，\(\text{Concat}\) 表示將所有 \( h \) 個頭的輸出拼接成一個大向量。

4. **線性變換得到最終輸出**：

   最後，我們對拼接后的結果進行一次線性變換，以產生最終的輸出：
   \[
   Z_{\text{final}} = W_O \cdot Z_{\text{concat}}
   \]
   其中 \( W_O \) 是一個學習的權重矩陣。這一過程將拼接的多頭輸出映射到預定的輸出維度。

#### **7.2.2 整體多頭注意力的數學表示**

將上述步驟綜合，可以得到多頭注意力的數學表示：

\[
Z_{\text{final}} = W_O \cdot \text{Concat}\left(\text{Attention}(Q_1, K_1, V_1), \dots, \text{Attention}(Q_h, K_h, V_h)\right)
\]

這裡，\(\text{Attention}(Q_i, K_i, V_i)\) 表示對第 \(i\) 頭的注意力計算，並且通過拼接來合併每個頭的結果。

#### **7.2.3 為何使用多頭注意力**

使用多頭注意力的主要優點是能夠學習不同的表示子空間。每一個注意力頭捕捉到不同的注意力模式，從而使得模型能夠同時關注輸入序列的不同部分，進而捕捉到更多樣的語言結構。這對於處理長文本或複雜結構非常有幫助，因為它使模型能夠捕捉到多個層次的語境信息。

具體來說：
- 每個頭能夠學習輸入序列中不同部分的依賴關係，因此能夠捕捉到更多的語言模式。
- 當使用多頭注意力時，模型能夠平行計算，這有助於提升訓練效率。
- 多頭注意力可以通過不同的子空間學習到更豐富的特徵表示，從而增強模型的表現。

#### **7.2.4 小結**

多頭注意力機制通過並行處理多個注意力頭，能夠在不同的子空間中學習多樣的注意力模式。這一機制不僅提高了模型的表達能力，還能夠提升計算效率，並且對長序列的建模特別有效。其數學基礎是對查詢、鍵和值向量進行線性變換，然後根據注意力權重計算加權和，最後將多個頭的輸出進行拼接和線性變換得到最終結果。