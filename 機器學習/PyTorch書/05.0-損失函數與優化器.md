好的，這是一份根據你提供的綱要，為你撰寫的第五章「損失函數與優化器」的詳細內容，包含文字說明、程式碼範例和圖表輔助：

**第五章：損失函數與優化器**

**5.1 常用的損失函數 (Common Loss Functions)**

*   **5.1.1 損失函數簡介 (Introduction to Loss Functions)**

    在機器學習和深度學習中，損失函數 (loss function) 或成本函數 (cost function) 是一個用於衡量模型預測結果與真實值之間差異的函數。損失函數的值越小，表示模型的預測結果越接近真實值，模型的性能越好。

    選擇合適的損失函數是訓練深度學習模型的關鍵步驟之一，因為不同的任務（例如分類、回歸、生成等）可能需要不同的損失函數。PyTorch 提供了許多內建的損失函數，方便我們使用。

    [插入一張圖表，顯示損失函數的概念，例如一個目標函數和一個實際函數之間的差距]

*   **5.1.1 分類問題損失函數 (Loss Functions for Classification)**
    *   **交叉熵損失函數 (`CrossEntropyLoss`)**
        *   交叉熵損失函數 (cross-entropy loss) 是一種常用的分類問題損失函數，特別適用於多類別分類問題。它衡量兩個機率分佈之間的差異，即真實標籤的機率分佈和模型預測的機率分佈。
        *   數學公式：
            *   對於二元分類：`Loss = -[y * log(p) + (1 - y) * log(1 - p)]`，其中 `y` 是真實標籤 (0 或 1)，`p` 是模型預測為正類的機率。
            *   對於多類別分類：`Loss = -sum(yi * log(pi))`，其中 `yi` 是真實標籤的 one-hot 編碼，`pi` 是模型預測的各類別機率。
        *   在 PyTorch 中，`nn.CrossEntropyLoss` 將 Softmax 和交叉熵損失函數結合在一起，因此，模型的輸出無需再經過 Softmax 處理，可以直接輸出未經過標準化的數值，也稱為 logits。
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn
        import torch.nn.functional as F

        # 創建交叉熵損失函數
        cross_entropy_loss = nn.CrossEntropyLoss()
        
        # 定義類別數量
        num_classes = 3

        # 創建一個隨機預測機率 (batch_size, num_classes)
        predicted_logits = torch.randn(2, num_classes)

        # 創建一個真實標籤 (batch_size)
        target = torch.randint(0, num_classes, (2, ))

        # 計算交叉熵損失
        loss = cross_entropy_loss(predicted_logits, target)
        print("CrossEntropyLoss:", loss)

        # 比較使用 cross_entropy vs. 使用 log_softmax + nll_loss
        # 先使用 log_softmax 轉換為 log probability
        predicted_log_probs = F.log_softmax(predicted_logits, dim=1)
        # 再使用 nll_loss 計算損失
        nll_loss = F.nll_loss(predicted_log_probs, target)
        print("log_softmax + nll_loss:", nll_loss)
        ```
        [插入一張圖表，展示交叉熵損失函數的形狀，並說明它如何衡量兩個機率分佈之間的差異]
        **說明：**
        *   模型的輸入是模型的輸出 logits。
        *   `nn.CrossEntropyLoss()` 會將 `nn.LogSoftmax()` 和 `nn.NLLLoss()` 結合在一起使用。
        *   `F.log_softmax()` 會輸出每個類別的 log probability。
        *   `F.nll_loss()` 計算負對數似然損失，輸入必須是 log probability。

*   **5.1.2 回歸問題損失函數 (Loss Functions for Regression)**
    *   **均方誤差損失函數 (`MSELoss`)**
        *   均方誤差 (mean squared error，MSE) 是一種常用的回歸問題損失函數。它計算預測值與真實值之間差異的平方和的平均值。
        *   數學公式：`MSE = mean((y_pred - y_true)^2)`
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn

        # 創建均方誤差損失函數
        mse_loss = nn.MSELoss()

        # 創建隨機預測值
        predicted_values = torch.randn(2, 1)

        # 創建隨機真實值
        true_values = torch.randn(2, 1)

        # 計算均方誤差
        loss = mse_loss(predicted_values, true_values)
        print("MSELoss:", loss)
        ```
        [插入一張圖表，展示均方誤差損失函數的形狀，並說明它如何衡量預測值和真實值之間的差距]
    *   **L1 損失函數 (`L1Loss`)**
        *   L1 損失函數，也稱為平均絕對誤差 (mean absolute error，MAE)，計算預測值與真實值之間差異的絕對值之和的平均值。與 MSE 不同的是，L1 損失函數對異常值不敏感。
        *   數學公式：`MAE = mean(abs(y_pred - y_true))`
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn

        # 創建 L1 損失函數
        l1_loss = nn.L1Loss()

        # 創建隨機預測值
        predicted_values = torch.randn(2, 1)

        # 創建隨機真實值
        true_values = torch.randn(2, 1)

        # 計算 L1 損失
        loss = l1_loss(predicted_values, true_values)
        print("L1Loss:", loss)
        ```
        [插入一張圖表，展示 L1 損失函數的形狀，並說明它如何衡量預測值和真實值之間的差距]

**5.2 優化器 (Optimizers)**

*   **5.2.1 優化器簡介 (Introduction to Optimizers)**

    優化器 (optimizer) 是一種演算法，用於根據損失函數的梯度，更新模型的參數（例如權重和偏差）。優化器的目標是最小化損失函數，從而使得模型的預測結果更接近真實值。

    PyTorch 提供了許多內建的優化器，例如 SGD, Adam, RMSprop 等。選擇合適的優化器對於模型的訓練非常重要，不同的優化器可能在不同的任務和數據集上表現更好。

    [插入一張圖表，顯示優化器在訓練模型中的作用，例如一個迭代過程，逐步更新模型參數]

*   **5.2.1 常用的優化器 (Common Optimizers)**
    *   **隨機梯度下降 (`SGD`)**
        *   隨機梯度下降 (stochastic gradient descent，SGD) 是一種基本的優化演算法。它每次使用一個批次的數據，計算損失函數的梯度，並根據梯度更新模型的參數。
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn
        import torch.optim as optim

        # 創建一個簡單的線性模型
        model = nn.Linear(10, 1)

        # 創建隨機梯度下降優化器
        optimizer = optim.SGD(model.parameters(), lr=0.01) # 參數為模型權重，學習率為 0.01

        # 創建隨機輸入
        input_data = torch.randn(1, 10)
        
        # 前向傳播
        output = model(input_data)
        
        # 創建隨機真實值
        target = torch.randn(1,1)
        
        # 計算損失函數
        loss_fn = nn.MSELoss()
        loss = loss_fn(output, target)
        
        # 反向傳播計算梯度
        optimizer.zero_grad()
        loss.backward()
        
        # 更新模型參數
        optimizer.step()
        
        print("Updated parameters:", model.weight)
        ```
        [插入一張圖表，展示隨機梯度下降的過程，例如沿著梯度反方向更新參數的迭代]
    *   **Adam 優化器 (`Adam`)**
        *   Adam 是一種常用的優化演算法，它結合了動量法和自適應學習率的優點。Adam 可以自適應地調整每個參數的學習率，通常可以更快地收斂。
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn
        import torch.optim as optim

        # 創建一個簡單的線性模型
        model = nn.Linear(10, 1)

        # 創建 Adam 優化器
        optimizer = optim.Adam(model.parameters(), lr=0.01) # 參數為模型權重，學習率為 0.01

        # 創建隨機輸入
        input_data = torch.randn(1, 10)

        # 前向傳播
        output = model(input_data)
        
        # 創建隨機真實值
        target = torch.randn(1,1)
        
        # 計算損失函數
        loss_fn = nn.MSELoss()
        loss = loss_fn(output, target)
        
        # 反向傳播計算梯度
        optimizer.zero_grad()
        loss.backward()
        
        # 更新模型參數
        optimizer.step()

        print("Updated parameters:", model.weight)
        ```
        [插入一張圖表，展示 Adam 優化器的過程，並說明它的自適應學習率特性]
    *   **RMSprop 優化器 (`RMSprop`)**
        *   RMSprop 是一種自適應學習率的優化演算法，它類似於 Adam，但只使用了梯度平方的指數移動平均值。
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn
        import torch.optim as optim

        # 創建一個簡單的線性模型
        model = nn.Linear(10, 1)

        # 創建 RMSprop 優化器
        optimizer = optim.RMSprop(model.parameters(), lr=0.01) # 參數為模型權重，學習率為 0.01

        # 創建隨機輸入
        input_data = torch.randn(1, 10)
        
        # 前向傳播
        output = model(input_data)
        
        # 創建隨機真實值
        target = torch.randn(1,1)
        
        # 計算損失函數
        loss_fn = nn.MSELoss()
        loss = loss_fn(output, target)
        
        # 反向傳播計算梯度
        optimizer.zero_grad()
        loss.backward()
        
        # 更新模型參數
        optimizer.step()

        print("Updated parameters:", model.weight)
        ```
        [插入一張圖表，展示 RMSprop 優化器的過程，並說明它的自適應學習率特性]

*   **5.2.2 學習率調整 (Learning Rate Scheduling)**
        *  學習率 (learning rate) 決定了模型參數更新的幅度。過大的學習率可能導致模型無法收斂，而過小的學習率則可能導致模型收斂過慢。
        *  學習率調整 (learning rate scheduling) 是一種在訓練過程中動態調整學習率的技術。常見的學習率調整策略包括：
            *   **Step Decay：** 每隔一定的 epoch 數，將學習率減少一定的比例。
            *   **Exponential Decay：** 以指數方式衰減學習率。
            *   **Cosine Annealing：** 使用餘弦函數衰減學習率。
            *   **ReduceLROnPlateau：** 當損失函數不再改善時，減少學習率。
            
        ```python
        import torch
        import torch.optim as optim

        # 創建一個簡單的線性模型
        model = torch.nn.Linear(10, 1)

        # 創建優化器
        optimizer = optim.Adam(model.parameters(), lr=0.1)
        
        # Step Decay
        scheduler1 = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
        
        # Exponential Decay
        scheduler2 = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
        
        # Cosine Annealing
        scheduler3 = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 10, eta_min=0.001)
        
        # ReduceLROnPlateau
        scheduler4 = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience=10)
        
        # 假裝訓練
        for epoch in range(50):
            optimizer.step()
            scheduler1.step()
            scheduler2.step()
            scheduler3.step()
            
            #假設有 validation loss
            val_loss = torch.randn(1).item()
            scheduler4.step(val_loss)

            print(f"Epoch {epoch+1} lr {optimizer.param_groups[0]['lr']:.4f}")
        ```
        [插入圖表，展示不同學習率調整策略的變化，例如 Step Decay, Exponential Decay, Cosine Annealing 的學習率變化曲線]

**5.3 模型訓練流程 (Model Training Process)**

    一個完整的模型訓練流程通常包括以下步驟：
    1.  **準備數據：** 讀取數據集，並對數據進行預處理（例如，歸一化、數據增強等）。
    2.  **建立模型：** 使用 `torch.nn` 模組建立神經網路模型。
    3.  **定義損失函數：** 根據任務選擇合適的損失函數。
    4.  **定義優化器：** 選擇合適的優化器，並設定學習率等超參數。
    5.  **訓練模型：** 在每次訓練迭代中，執行以下步驟：
        *   **前向傳播：** 計算模型的預測值。
        *   **計算損失：** 計算預測值與真實值之間的損失。
        *   **反向傳播：** 計算模型參數的梯度。
        *   **更新參數：** 使用優化器更新模型參數。
    6.  **評估模型：** 在驗證集或測試集上評估模型的性能。
    7.  **調整超參數：** 如果模型的性能不佳，則需要調整超參數，然後重新訓練模型。

    [插入一張流程圖，展示完整的模型訓練流程，例如數據讀取、模型建立、訓練、評估等步驟]

**5.4 實例：使用優化器訓練一個分類模型 (Example: Training a Classification Model with an Optimizer)**

    我們將使用前面章節建立的二元分類模型，並使用 Adam 優化器訓練該模型。
    ```python
        import torch
        import torch.nn as nn
        import torch.optim as optim
        import torch.nn.functional as F
        import matplotlib.pyplot as plt
        
        # 1. 設定超參數
        torch.manual_seed(42)
        batch_size = 10
        num_epochs = 100
        learning_rate = 0.01
        
        # 2. 生成模擬資料
        X = torch.randn(100, 10)
        y = torch.randint(0, 2, (100, 1)).float()
        
        # 3. 定義模型
        class BinaryClassifier(nn.Module):
            def __init__(self, input_size):
                super().__init__()
                self.linear1 = nn.Linear(input_size, 20)
                self.linear2 = nn.Linear(20, 1)
            def forward(self, x):
                x = F.relu(self.linear1(x))
                x = torch.sigmoid(self.linear2(x))
                return x
        
        # 4. 建立模型實例
        model = BinaryClassifier(10)
        
        # 5. 定義優化器
        optimizer = optim.Adam(model.parameters(), lr = learning_rate)
        
        # 6. 開始訓練
        loss_history = []
        for epoch in range(num_epochs):
            
            # 前向傳播
            y_pred = model(X)
            
            # 計算二元交叉熵損失函數 (Binary Cross Entropy Loss)
            loss = F.binary_cross_entropy(y_pred, y)
            
            # 反向傳播
            optimizer.zero_grad()
            loss.backward()
            
            # 更新參數
            optimizer.step()
            
            loss_history.append(loss.item())
            
            # 輸出當前 epoch 的 loss
            if (epoch+1) % 10 == 0:
                print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')
        
        # 7. 繪製 loss
        plt.plot(range(1, len(loss_history) + 1), loss_history)
        plt.xlabel("Epochs")
        plt.ylabel("Loss")
        plt.title("Training Loss")
        plt.show()
    ```
    **說明：**
    *   這個程式碼與第四章的程式碼類似，只是優化器改為 Adam。
    *   我們使用 `optim.Adam()` 創建 Adam 優化器。
    *   訓練流程與前面章節的相同。

**章節結尾**

恭喜你完成了第五章的學習！在本章中，你深入學習了 PyTorch 中常用的損失函數和優化器，這對於訓練深度學習模型至關重要。你學習了以下重要的概念：

*   損失函數的作用和重要性。
*   常用的分類問題損失函數：交叉熵損失函數 (`CrossEntropyLoss`)。
*   常用的回歸問題損失函數：均方誤差損失函數 (`MSELoss`) 和 L1 損失函數 (`L1Loss`)。
*   優化器的作用和重要性。
*   常用的優化器：隨機梯度下降 (`SGD`), Adam, RMSprop 等。
*   學習率調整策略。
*   完整的模型訓練流程。
*   使用優化器訓練一個簡單的分類模型。

在接下來的章節中，我們將學習如何使用 `torch.utils.data` 模組讀取和處理數據，這對於處理真實世界數據非常重要。請繼續努力！

**[下一章預告]**
第六章：資料讀取與處理

希望這份第五章的內容對你有所幫助！記得在實際寫作時，盡可能加入更多圖表和程式碼範例，並詳細解釋每一個步驟，讓讀者更容易理解和掌握。
