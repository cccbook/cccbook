好的，這是一份根據你提供的綱要，為你撰寫的第四章「建立神經網路模型」的詳細內容，包含文字說明、程式碼範例和圖表輔助：

**第四章：建立神經網路模型**

**4.1 `torch.nn` 模組 (The `torch.nn` Module)**

*   **4.1.1 `torch.nn` 簡介 (Introduction to `torch.nn`)**

    `torch.nn` 模組是 PyTorch 中用於構建神經網路的核心模組。它提供了許多預先定義好的類別，稱為「層」(layers)，可以用於建立各種深度學習模型。這些層可以包括線性層、激活層、卷積層、池化層、正規化層和 Dropout 層等。

    `torch.nn` 模組的優點是它將模型的各個組件封裝成易於使用的類別，同時也提供了計算圖追蹤、梯度計算和模型參數管理的功能。這使得我們可以更方便、更有效率地建立、訓練和評估神經網路模型。

    [插入一張示意圖，顯示 `torch.nn` 模組在 PyTorch 中的地位，以及它提供的各種層]

*   **4.1.1 基本層 (Basic Layers)**
    *   **線性層 (`Linear`)**
        *   線性層，也稱為全連接層 (fully connected layer)，是神經網路中最基本的一種層。它執行線性變換，即將輸入資料與權重矩陣相乘，然後加上偏置向量。
        *   數學公式：`output = input @ W^T + b`，其中 `W` 是權重矩陣，`b` 是偏置向量，`@` 是矩陣乘法。
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn

        # 創建一個線性層，輸入大小為 10，輸出大小為 5
        linear_layer = nn.Linear(10, 5)

        # 創建一個隨機輸入向量
        input_data = torch.randn(1, 10) # batch_size, input_features

        # 將輸入資料傳遞給線性層
        output = linear_layer(input_data)
        print("Linear layer output shape:", output.shape) # Output: torch.Size([1, 5])
        ```
        [插入一張圖表，展示線性層的運算，例如一個輸入向量經過權重矩陣乘法和偏置向量加法後得到輸出向量]
    *   **ReLU 激活層 (`ReLU`)**
        *   ReLU (Rectified Linear Unit) 是一種常用的激活函數。它將所有負值設為 0，而正值則保持不變。
        *   數學公式：`ReLU(x) = max(0, x)`
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn

        # 創建一個 ReLU 激活層
        relu_layer = nn.ReLU()

        # 創建一個隨機輸入向量
        input_data = torch.randn(1, 5)

        # 將輸入資料傳遞給 ReLU 層
        output = relu_layer(input_data)
        print("ReLU layer output:\n", output)
        ```
        [插入一張圖表，展示 ReLU 函數的形狀，即當 x < 0 時，輸出為 0，當 x >= 0 時，輸出為 x]
    *   **Sigmoid 激活層 (`Sigmoid`)**
        *   Sigmoid 函數是一種常用的激活函數，它可以將輸入值壓縮到 0 和 1 之間。它通常用於二元分類問題中，將輸出映射到機率值。
        *   數學公式：`Sigmoid(x) = 1 / (1 + exp(-x))`
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn

        # 創建一個 Sigmoid 激活層
        sigmoid_layer = nn.Sigmoid()

        # 創建一個隨機輸入向量
        input_data = torch.randn(1, 5)

        # 將輸入資料傳遞給 Sigmoid 層
        output = sigmoid_layer(input_data)
        print("Sigmoid layer output:\n", output)
        ```
        [插入一張圖表，展示 Sigmoid 函數的形狀，即將輸入值映射到 0 和 1 之間]

*   **4.1.2 卷積層 (Convolutional Layers)**
    *   **二維卷積層 (`Conv2d`)**
        *   卷積層是卷積神經網路 (CNN) 中的核心層。它使用卷積核 (kernel) 在輸入圖像上滑動，提取局部特徵。
        *   `Conv2d` 適用於處理二維數據，例如圖像。
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn

        # 創建一個二維卷積層
        # 輸入通道數為 3，輸出通道數為 16，卷積核大小為 3x3，stride 為 1，padding 為 1
        conv2d_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)

        # 創建一個隨機輸入圖像 (batch_size, channels, height, width)
        input_image = torch.randn(1, 3, 32, 32)

        # 將輸入圖像傳遞給卷積層
        output = conv2d_layer(input_image)
        print("Conv2d output shape:", output.shape) # Output: torch.Size([1, 16, 32, 32])
        ```
        [插入一張圖表，展示卷積層的運算，例如卷積核在輸入圖像上滑動，並計算點積]
    *   **二維反卷積層 (`ConvTranspose2d`)**
        *   反卷積層 (deconvolution layer)，也稱為轉置卷積層 (transpose convolutional layer)，執行的是卷積層的逆操作，可以擴大輸入資料的尺寸。
        *   常用於圖像的生成或上採樣任務中。
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn

        # 創建一個二維反卷積層
        # 輸入通道數為 16，輸出通道數為 3，卷積核大小為 3x3，stride 為 2，padding 為 1
        conv_trans_layer = nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=3, stride=2, padding=1, output_padding = 1)

        # 創建一個隨機輸入
        input_feature_map = torch.randn(1, 16, 16, 16)

        # 將輸入圖像傳遞給反卷積層
        output = conv_trans_layer(input_feature_map)
        print("ConvTranspose2d output shape:", output.shape) # Output: torch.Size([1, 3, 32, 32])
        ```
        [插入一張圖表，展示反卷積層的運算，例如使用卷積核在輸入 feature map 上進行擴大]

*   **4.1.3 池化層 (Pooling Layers)**
    *   **最大池化層 (`MaxPool2d`)**
        *   最大池化層 (max pooling layer) 在輸入資料的指定區域中，選擇最大值作為輸出。它可以降低特徵圖的尺寸，並提取主要的特徵。
        *   常用於卷積神經網路中。
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn

        # 創建一個最大池化層，池化窗口大小為 2x2
        maxpool2d_layer = nn.MaxPool2d(kernel_size=2)

        # 創建一個隨機輸入張量
        input_feature_map = torch.randn(1, 3, 32, 32)

        # 將輸入資料傳遞給最大池化層
        output = maxpool2d_layer(input_feature_map)
        print("MaxPool2d output shape:", output.shape) # Output: torch.Size([1, 3, 16, 16])
        ```
        [插入一張圖表，展示最大池化層的運算，例如在輸入圖像上滑動一個窗口，選擇窗口中的最大值作為輸出]
    *   **平均池化層 (`AvgPool2d`)**
        *   平均池化層 (average pooling layer) 在輸入資料的指定區域中，計算平均值作為輸出。它可以降低特徵圖的尺寸，並提取平均特徵。
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn

        # 創建一個平均池化層，池化窗口大小為 2x2
        avgpool2d_layer = nn.AvgPool2d(kernel_size=2)

        # 創建一個隨機輸入張量
        input_feature_map = torch.randn(1, 3, 32, 32)

        # 將輸入資料傳遞給平均池化層
        output = avgpool2d_layer(input_feature_map)
        print("AvgPool2d output shape:", output.shape) # Output: torch.Size([1, 3, 16, 16])
        ```
        [插入一張圖表，展示平均池化層的運算，例如在輸入圖像上滑動一個窗口，計算窗口中的平均值作為輸出]

*   **4.1.4 正規化層 (Normalization Layers)**
    *   **批量正規化層 (`BatchNorm2d`)**
        *   批量正規化 (batch normalization) 可以加速模型的訓練，並提高模型的泛化能力。它通過對每個批次的輸入數據進行正規化，將其均值設為 0，標準差設為 1。
        *   `BatchNorm2d` 適用於處理二維數據，例如卷積層的輸出。
        *   程式碼範例：
        ```python
        import torch
        import torch.nn as nn

        # 創建一個批量正規化層，輸入通道數為 16
        batchnorm2d_layer = nn.BatchNorm2d(num_features=16)

        # 創建一個隨機輸入張量
        input_feature_map = torch.randn(1, 16, 32, 32)

        # 將輸入資料傳遞給批量正規化層
        output = batchnorm2d_layer(input_feature_map)
        print("BatchNorm2d output shape:", output.shape) # Output: torch.Size([1, 16, 32, 32])
        ```
        [插入一張圖表，展示批量正規化層的運算，例如對每個批次的輸入數據進行均值和標準差的正規化]
    *   **層正規化層 (`LayerNorm`)**
        *   層正規化 (layer normalization) 類似於批量正規化，但它對每個樣本的輸入數據進行正規化，而不是對每個批次。
        *  `LayerNorm` 通常應用於序列數據，例如自然語言處理中的文本序列。
        ```python
            import torch
            import torch.nn as nn
            # 創建一個 LayerNorm 層， 輸入大小為 10
            layer_norm = nn.LayerNorm(10)
            # 創建一個隨機輸入張量
            input_feature_map = torch.randn(1, 5, 10)
            output = layer_norm(input_feature_map)
            print("LayerNorm output shape:", output.shape) # Output: torch.Size([1, 5, 10])
        ```
        [插入一張圖表，展示層正規化層的運算，例如對每個樣本的輸入數據進行均值和標準差的正規化]

*   **4.1.5 Dropout 層 (`Dropout`)**
        *  Dropout 層在訓練過程中，以一定的機率隨機將一些神經元的輸出設為 0，從而防止過度擬合。Dropout 可以在訓練過程中產生不同的網路結構，有助於模型學習更強健的特徵表示。
        *  程式碼範例：
        ```python
            import torch
            import torch.nn as nn
            # 創建一個 Dropout 層，丟棄率為 0.5
            dropout_layer = nn.Dropout(p = 0.5)
            
            # 創建一個隨機輸入張量
            input_feature_map = torch.randn(1, 10)
            output = dropout_layer(input_feature_map)
            print("Dropout output:\n", output)
        ```
        [插入一張圖表，展示 Dropout 層的運算，例如隨機將一些神經元設置為 0]

**4.2 使用 `torch.nn.Sequential` 建立簡單網路 (Building Simple Networks with `torch.nn.Sequential`)**

*   `torch.nn.Sequential` 是一個容器，可以用來按順序組合多個層。它非常適合建立結構簡單的網路模型。
    ```python
        import torch
        import torch.nn as nn

        # 使用 Sequential 建立一個簡單的網路模型
        model = nn.Sequential(
            nn.Linear(10, 20),
            nn.ReLU(),
            nn.Linear(20, 5),
            nn.Sigmoid()
        )
        
        # 創建一個隨機輸入張量
        input_data = torch.randn(1, 10)
        output = model(input_data)
        
        print("Output Shape:", output.shape)
    ```
    **說明：**
    *   `nn.Sequential()` 接收多個層作為參數，按照順序建立一個模型。
    *   你可以將 `nn.Sequential()` 當作一個層使用，將輸入數據傳遞給它即可得到模型的輸出。

**4.3 使用 `torch.nn.Module` 建立客製化網路 (Building Custom Networks with `torch.nn.Module`)**

*   `torch.nn.Module` 是建立更複雜網路模型的基本類別。你需要繼承 `nn.Module` 類別，並在 `__init__` 方法中定義模型中的各個層，然後在 `forward()` 方法中定義資料的傳遞流程。
    ```python
        import torch
        import torch.nn as nn
        import torch.nn.functional as F

        # 建立一個繼承自 nn.Module 的自定義類別
        class MyNetwork(nn.Module):
            def __init__(self, input_size, hidden_size, output_size):
                super().__init__()
                self.linear1 = nn.Linear(input_size, hidden_size)
                self.linear2 = nn.Linear(hidden_size, output_size)

            def forward(self, x):
                x = F.relu(self.linear1(x))
                x = F.sigmoid(self.linear2(x))
                return x
                
        # 建立模型實例
        model = MyNetwork(10, 20, 5)
        
        # 創建一個隨機輸入張量
        input_data = torch.randn(1, 10)
        output = model(input_data)
        print("Output Shape:", output.shape)
    ```
    **說明：**
    *   `super().__init__()` 呼叫父類別 `nn.Module` 的初始化方法。
    *   在 `__init__()` 中，我們使用 `nn.Linear()` 定義兩個線性層。
    *   在 `forward()` 中，我們定義了資料的傳遞流程：
        1.  將輸入資料 `x` 傳遞給第一個線性層，並使用 ReLU 激活函數。
        2.  將第一個線性層的輸出傳遞給第二個線性層，並使用 Sigmoid 激活函數。
        3.  返回模型的輸出。
    * 可以使用 `torch.nn.functional` (慣例使用 F 作為簡稱) 來呼叫一些沒有權重的層。

*   **4.3.1 `__init__` 和 `forward` 方法 (The `__init__` and `forward` Methods)**
    *   `__init__` 方法：
        *   用於初始化模型中的各個層。
        *   需要使用 `super().__init__()` 來呼叫父類別的初始化方法。
    *   `forward` 方法：
        *   定義資料在模型中的傳遞流程。
        *   需要返回模型的輸出。
        *  `forward()` 方法的輸入是一個 Tensor，輸出也必須是一個 Tensor。

**4.4 模型參數的初始化 (Initializing Model Parameters)**

*   在建立神經網路模型後，需要初始化模型的參數，例如權重和偏差。合理的初始化方法可以加速模型的訓練，並提高模型的性能。
*   PyTorch 的 `torch.nn` 模組會自動初始化模型的參數。
*   常見的模型參數初始化方法：
    *   **隨機初始化：** 將權重和偏差初始化為隨機數 (例如使用標準常態分佈、均勻分佈等)。
    *   **Xavier 初始化：** 根據輸入和輸出的尺寸，調整權重的隨機範圍。
    *   **Kaiming 初始化：** 根據輸入的尺寸，調整權重的隨機範圍。
        ```python
            import torch
            import torch.nn as nn
            
            # 創建一個線性層
            linear_layer = nn.Linear(10, 5)
            
            # 查看權重和偏差
            print("Initial weights:", linear_layer.weight)
            print("Initial bias:", linear_layer.bias)
            
            # 使用不同的初始化方式
            nn.init.xavier_normal_(linear_layer.weight)
            nn.init.zeros_(linear_layer.bias)
            
            print("Updated weights:", linear_layer.weight)
            print("Updated bias:", linear_layer.bias)
        ```
        **說明：**
        *   `nn.init.xavier_normal_()` 使用 Xavier 初始化方法。
        *   `nn.init.zeros_()` 將參數初始化為零。

**4.5 實例：建立一個簡單的分類模型 (Example: Building a Simple Classification Model)**

    我們將使用 `torch.nn` 模組來建立一個簡單的二元分類模型，該模型接收一個 10 維的輸入向量，並輸出一個機率值，表示輸入數據屬於正類的機率。
    ```python
        import torch
        import torch.nn as nn
        import torch.optim as optim
        import torch.nn.functional as F
        import matplotlib.pyplot as plt
        
        # 1. 設定超參數
        torch.manual_seed(42)
        batch_size = 10
        num_epochs = 100
        learning_rate = 0.01
        
        # 2. 生成模擬資料
        X = torch.randn(100, 10)
        y = torch.randint(0, 2, (100, 1)).float()
        
        # 3. 定義模型
        class BinaryClassifier(nn.Module):
            def __init__(self, input_size):
                super().__init__()
                self.linear1 = nn.Linear(input_size, 20)
                self.linear2 = nn.Linear(20, 1)
            def forward(self, x):
                x = F.relu(self.linear1(x))
                x = torch.sigmoid(self.linear2(x))
                return x

        # 4. 建立模型實例
        model = BinaryClassifier(10)
        
        # 5. 定義優化器
        optimizer = optim.SGD(model.parameters(), lr = learning_rate)
        
        # 6. 開始訓練
        loss_history = []
        for epoch in range(num_epochs):
           
            # 前向傳播
            y_pred = model(X)
            
            # 計算二元交叉熵損失函數 (Binary Cross Entropy Loss)
            loss = F.binary_cross_entropy(y_pred, y)
            
            # 反向傳播
            optimizer.zero_grad()
            loss.backward()
            
            # 更新參數
            optimizer.step()
            
            loss_history.append(loss.item())

            # 輸出當前 epoch 的 loss
            if (epoch+1) % 10 == 0:
                 print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')

        # 7. 繪製 loss
        plt.plot(range(1, len(loss_history) + 1), loss_history)
        plt.xlabel("Epochs")
        plt.ylabel("Loss")
        plt.title("Training Loss")
        plt.show()
    ```
    **說明：**
    *   我們建立了一個 `BinaryClassifier` 模型，它繼承自 `nn.Module`。
    *   模型包含兩個線性層，中間使用 ReLU 激活函數，最後使用 Sigmoid 激活函數。
    *   我們使用了二元交叉熵 (Binary Cross Entropy) 作為損失函數。
    *   我們使用了隨機梯度下降 (SGD) 作為優化器。
    *   在每次訓練迭代中，我們執行以下步驟：
        1.  **前向傳播：** 計算模型的預測值。
        2.  **計算損失：** 計算預測值與真實值之間的損失。
        3.  **反向傳播：** 計算模型參數的梯度。
        4.  **更新參數：** 使用梯度下降更新模型參數。
        5. 我們最後繪製了 loss 隨著 epoch 變化的曲線。

**章節結尾**

恭喜你完成了第四章的學習！在本章中，你深入學習了 PyTorch 的 `torch.nn` 模組，這是建立神經網路模型的核心模組。你學習了以下重要的概念：

*   `torch.nn` 模組的作用和基本概念。
*   常用的基本層：線性層 (`Linear`)、ReLU 激活層 (`ReLU`)、Sigmoid 激活層 (`Sigmoid`)。
*   卷積層：二維卷積層 (`Conv2d`) 和二維反卷積層 (`ConvTranspose2d`)。
*   池化層：最大池化層 (`MaxPool2d`) 和平均池化層 (`AvgPool2d`)。
*   正規化層：批量正規化層 (`BatchNorm2d`) 和層正規化層 (`LayerNorm`).
*   Dropout層 (`Dropout`)
*   使用 `torch.nn.Sequential` 建立簡單的網路模型。
*   使用 `torch.nn.Module` 建立客製化的網路模型。
*   模型參數的初始化方法。
*   使用 `torch.nn` 模組建立一個簡單的分類模型。

在接下來的章節中，我們將學習如何定義損失函數和優化器，這將是訓練深度學習模型的另一個重要部分。請繼續努力！

**[下一章預告]**
第五章：損失函數與優化器

希望這份第四章的內容對你有幫助！記得在實際寫作時，盡可能加入更多圖表和程式碼範例，並詳細解釋每一個步驟，讓讀者更容易理解和掌握。
