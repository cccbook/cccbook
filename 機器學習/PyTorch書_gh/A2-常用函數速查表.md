好的，這是為你撰寫的附錄 A2「常用函數速查表」的詳細內容，它將列出在 PyTorch 中常用的函數，並提供簡單的說明和程式碼範例，方便讀者快速查閱：

**附錄 A2：常用函數速查表**

本附錄列出了在 PyTorch 中常用的函數，它們涵蓋了張量操作、數學運算、模型建立、優化器、以及其他實用工具。這些函數的說明和範例，將幫助你快速找到所需的函數並應用於你的 PyTorch 專案中。

**A2.1 張量操作 (Tensor Operations)**

| 函數                                 | 描述                                                                                   | 範例                                                                                    |
| :------------------------------------ | :------------------------------------------------------------------------------------ | :-------------------------------------------------------------------------------------- |
| `torch.tensor()`                      | 建立張量，可以從列表或 NumPy 數組建立                                                   | `torch.tensor([1, 2, 3])`                                                               |
| `torch.zeros()`                      | 建立全零張量                                                                           | `torch.zeros(2, 3)`                                                                      |
| `torch.ones()`                       | 建立全一張量                                                                           | `torch.ones(2, 3)`                                                                       |
| `torch.rand()`                       | 建立隨機數張量（均勻分佈）                                                           | `torch.rand(2, 3)`                                                                       |
| `torch.randn()`                      | 建立隨機數張量（標準常態分佈）                                                       | `torch.randn(2, 3)`                                                                      |
| `torch.arange()`                     | 建立一個等差數列的張量                                                                 | `torch.arange(0, 10, 2)` (結果為 `tensor([0, 2, 4, 6, 8])`)                               |
| `torch.linspace()`                  | 建立一個指定範圍內等間隔的張量                                                          | `torch.linspace(0, 1, 5)` (結果為 `tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])`) |
| `tensor.reshape()`                   | 改變張量的形狀 (返回一個新的 Tensor)                                                        | `tensor.reshape(3, 2)`                                                                  |
| `tensor.view()`                    | 改變張量的形狀 (返回一個 view，修改其中一個會影響另一個)                                | `tensor.view(3, 2)`                                                                     |
| `tensor.to()`                        | 將張量移動到指定的裝置（例如 CPU 或 GPU）                                               | `tensor.to(device)`                                                                      |
| `tensor.dtype`                       | 獲取張量的數據類型                                                                    | `tensor.dtype` (例如 `torch.float32`)                                                     |
| `tensor.shape`                       | 獲取張量的形狀                                                                        | `tensor.shape` (例如 `(2, 3)`)                                                             |
| `tensor.ndim`                        | 獲取張量的維度                                                                        | `tensor.ndim`  (例如 2)                                                           |
| `tensor.unsqueeze(dim)`             | 在指定維度增加一個維度                                                               | `tensor.unsqueeze(0)`                                                                 |
| `tensor.squeeze(dim)`               | 在指定維度移除一個維度 (若該維度大小為 1)                                              | `tensor.squeeze(0)`                                                                   |
| `torch.cat(tensors, dim)`             | 在指定維度拼接張量                                                                     | `torch.cat([tensor1, tensor2], dim=0)`                                                 |
| `torch.stack(tensors, dim)`          | 在指定維度堆疊張量，增加一個維度                                                        | `torch.stack([tensor1, tensor2], dim=0)`                                                |
| `torch.transpose(input, dim0, dim1)` | 交換張量的兩個維度                                                                    | `torch.transpose(tensor, 0, 1)`                                                      |
| `torch.matmul(tensor1, tensor2)`     | 矩陣乘法                                                                               | `torch.matmul(tensor1, tensor2)`                                                        |

**A2.2 數學運算 (Mathematical Operations)**

| 函數                    | 描述                                                                              | 範例                                                                          |
| :---------------------- | :-------------------------------------------------------------------------------- | :--------------------------------------------------------------------------- |
| `torch.add(tensor1, tensor2)`或 `tensor1 + tensor2` | 元素級加法                                                                           | `torch.add(tensor1, tensor2)`  或 `tensor1 + tensor2`                              |
|`torch.sub(tensor1, tensor2)` 或 `tensor1 - tensor2`|元素級減法                                                                             |`torch.sub(tensor1, tensor2)` 或 `tensor1 - tensor2`                                  |
|`torch.mul(tensor1, tensor2)` 或 `tensor1 * tensor2` |元素級乘法                                                                            | `torch.mul(tensor1, tensor2)` 或 `tensor1 * tensor2`                                 |
|`torch.div(tensor1, tensor2)` 或 `tensor1 / tensor2` | 元素級除法                                                                             |`torch.div(tensor1, tensor2)` 或 `tensor1 / tensor2`                                 |
| `torch.pow(tensor, exponent)`         | 張量的元素級指數運算                                                               | `torch.pow(tensor, 2)`                                                            |
| `torch.sqrt(tensor)`                | 張量的元素級平方根運算                                                          | `torch.sqrt(tensor)`                                                           |
| `torch.exp(tensor)`                 | 張量的元素級指數運算 (e^x)                                                          | `torch.exp(tensor)`                                                            |
| `torch.log(tensor)`                 | 張量的元素級對數運算 (ln(x))                                                           | `torch.log(tensor)`                                                            |
| `torch.abs(tensor)`                 | 張量的元素級絕對值                                                              | `torch.abs(tensor)`                                                            |
| `torch.sin(tensor)`                 | 張量的元素級正弦函數                                                              | `torch.sin(tensor)`                                                            |
| `torch.cos(tensor)`                 | 張量的元素級餘弦函數                                                               | `torch.cos(tensor)`                                                            |
| `torch.mean(tensor)`                 | 張量的平均值                                                                     | `torch.mean(tensor)`                                                            |
| `torch.sum(tensor)`                  | 張量的元素總和                                                                     | `torch.sum(tensor)`                                                             |
| `torch.max(tensor)`                  | 張量的最大值                                                                      | `torch.max(tensor)`                                                             |
| `torch.min(tensor)`                  | 張量的最小值                                                                      | `torch.min(tensor)`                                                             |
| `torch.argmax(tensor)`             | 張量最大值的索引                                                                   | `torch.argmax(tensor)`                                                          |
| `torch.argmin(tensor)`              | 張量最小值的索引                                                                   | `torch.argmin(tensor)`                                                           |
| `torch.clamp(tensor, min, max)`    | 將張量的值限制在 min 和 max 之間                                                   | `torch.clamp(tensor, 0, 1)`                                                   |
| `torch.round(tensor)`              |  將張量四捨五入到最接近的整數                                                     | `torch.round(tensor)`                                                         |
| `torch.sigmoid(tensor)` |  將張量中的數值轉換為 0 和 1 之間的 sigmoid 值             |`torch.sigmoid(tensor)`                               |

**A2.3 模型建立 (Model Building)**

| 函數                                   | 描述                                                                               | 範例                                                                                  |
| :------------------------------------- | :--------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------- |
| `torch.nn.Linear(in_features, out_features)`  |  全連接層                                                                     | `torch.nn.Linear(10, 2)` (輸入 10 個特徵，輸出 2 個特徵)                            |
| `torch.nn.ReLU()`                     |  ReLU 激活函數                                                                     | `torch.nn.ReLU()`                                                                    |
| `torch.nn.Sigmoid()`                   |  Sigmoid 激活函數                                                                 | `torch.nn.Sigmoid()`                                                                 |
| `torch.nn.Conv2d(in_channels, out_channels, kernel_size)` | 二維卷積層                                                                         | `torch.nn.Conv2d(3, 16, 3)`(輸入 3 個通道，輸出 16 個通道，卷積核大小為 3x3)           |
| `torch.nn.MaxPool2d(kernel_size)`       | 二維最大池化層                                                                    | `torch.nn.MaxPool2d(2)` (池化窗口大小為 2x2)                                       |
| `torch.nn.BatchNorm2d(num_features)`     | 二維批量正規化層                                                                     | `torch.nn.BatchNorm2d(16)` (輸入通道數為 16)                                          |
| `torch.nn.Dropout(p)`                | Dropout 層                                                                       | `torch.nn.Dropout(0.5)` (丟棄機率為 0.5)                                      |
| `torch.nn.Sequential(*layers)`      | 將多個層按順序組合在一起                                                          | `torch.nn.Sequential(nn.Linear(10, 20), nn.ReLU())`                                |
| `torch.nn.Module`                     |  建立自定義模型類別的基底類別，必須定義 `__init__()` 和 `forward()` 方法             |  ```python class MyModel(nn.Module): def __init__(self): ... def forward(self, x): ...```   |
| `torch.nn.Embedding(num_embeddings, embedding_dim)` | 建立一個 embedding layer                                                            | `torch.nn.Embedding(1000, 128)` (詞彙大小為 1000，embedding 維度為 128)          |

**A2.4 優化器 (Optimizers)**

| 函數                                    | 描述                                                                           | 範例                                                                             |
| :-------------------------------------- | :----------------------------------------------------------------------------- | :------------------------------------------------------------------------------- |
| `torch.optim.SGD(params, lr)`        | 隨機梯度下降優化器                                                              | `torch.optim.SGD(model.parameters(), lr=0.01)`                                    |
| `torch.optim.Adam(params, lr)`       | Adam 優化器                                                                    | `torch.optim.Adam(model.parameters(), lr=0.001)`                                 |
| `torch.optim.RMSprop(params, lr)`    | RMSprop 優化器                                                                 | `torch.optim.RMSprop(model.parameters(), lr=0.001)`                              |
| `optimizer.zero_grad()`             | 將優化器的梯度清零                                                                | `optimizer.zero_grad()`                                                          |
| `optimizer.step()`                   | 更新模型參數                                                                    | `optimizer.step()`                                                                |
| `torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma)`       | 學習率隨 epoch 遞減                                                                  | `torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)`      |
| `torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min)`| 學習率使用餘弦函數衰減 | `torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 10, eta_min=0.001)`|
| `torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode, factor, patience)` | 當驗證損失值不再下降時，減少學習率 | `torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience=10)`|

**A2.5 損失函數 (Loss Functions)**

| 函數                     | 描述                                                                | 範例                                                              |
| :----------------------- | :------------------------------------------------------------------ | :---------------------------------------------------------------- |
| `torch.nn.MSELoss()`      | 均方誤差損失函數                                                       | `torch.nn.MSELoss()`                                            |
| `torch.nn.L1Loss()`       | L1 損失函數                                                            | `torch.nn.L1Loss()`                                            |
| `torch.nn.CrossEntropyLoss()`| 交叉熵損失函數 (用於分類問題)                                           | `torch.nn.CrossEntropyLoss()`                                  |
| `torch.nn.BCEWithLogitsLoss()` | 二元交叉熵損失函數 (用於二元分類問題)                             | `torch.nn.BCEWithLogitsLoss()`                               |

**A2.6 自動微分 (Autograd)**

| 函數             | 描述                                                                   | 範例                                                                   |
| :--------------- | :--------------------------------------------------------------------- | :--------------------------------------------------------------------- |
| `tensor.backward()`| 計算梯度                                                                  | `loss.backward()`                                                     |
| `tensor.grad`   |  取得張量的梯度                                                       | `x.grad`                                                             |
|`torch.no_grad():` | 在此區塊內不會計算梯度，可用於評估模型時                                         |   ```python with torch.no_grad():  output = model(input)  ```                                                      |
|`tensor.requires_grad = True` | 指定此張量需要計算梯度                                     |  `x = torch.randn(1, requires_grad = True)`                            |
|`tensor.detach()` | 切斷梯度計算，返回一個與原張量相同，但不需要梯度計算的 Tensor             |   `detached_tensor = tensor.detach()`                                            |
| `tensor.grad.zero_()` | 將梯度清零                                                        | `optimizer.zero_grad()` 或 `x.grad.zero_()`                            |

**A2.7 其他實用工具 (Other Utility Functions)**

| 函數                                  | 描述                                                               | 範例                                                           |
| :------------------------------------ | :----------------------------------------------------------------- | :------------------------------------------------------------- |
| `torch.cuda.is_available()`          | 檢查 CUDA 是否可用                                                  | `torch.cuda.is_available()`                                  |
| `torch.device()`                       | 建立一個 device 物件，表示 CPU 或 GPU 裝置                                     | `torch.device("cuda")`, `torch.device("cpu")`                         |
| `torch.save(obj, path)`             | 將物件儲存到硬碟                                                                | `torch.save(model.state_dict(), 'model.pth')`                        |
| `torch.load(path, map_location)`   | 從硬碟載入物件，並可以指定裝置                                                | `torch.load('model.pth', map_location='cpu')`          |
| `torch.manual_seed(seed)`             | 設定 PyTorch 的隨機種子                                                                 | `torch.manual_seed(42)`                                         |
| `torch.nn.utils.clip_grad_norm_(parameters, max_norm)` | 將梯度限制在 max_norm 之間        |   `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)`        |

**A2.8 PyTorch Lightning 常用函數**

| 函數                                   | 描述                                                                    | 範例                                                                |
| :------------------------------------- | :---------------------------------------------------------------------- | :------------------------------------------------------------------ |
| `pl.LightningModule`                   | 定義模型和訓練步驟的類別                                                  | `class MyModel(pl.LightningModule): ...`                            |
| `pl.LightningDataModule`               | 定義數據讀取方式的類別                                                 | `class MyDataModule(pl.LightningDataModule): ...`             |
| `pl.Trainer(max_epochs, callbacks = [], strategy= , precision = )`     | 設定訓練參數和執行訓練                                                                  | `pl.Trainer(max_epochs = 100, callbacks = [early_stopping], strategy="ddp", precision = 16)`                      |
| `self.log(name, value, prog_bar)`      | 將資料記錄到 Tensorboard (或者其他日誌工具)，`prog_bar` 表示是否顯示在進度條上    | `self.log('train_loss', loss, prog_bar = True)`                          |
|`trainer.fit(model, datamodule)`| 開始訓練模型                                |`trainer.fit(model, datamodule = data_module)`|
|`trainer.test(model, datamodule)`| 開始測試模型 | `trainer.test(model, datamodule = data_module)`|

**A2.9 Transformers 庫常用函數**

| 函數                         | 描述                                             | 範例                                                     |
| :--------------------------- | :----------------------------------------------- | :-------------------------------------------------------- |
| `transformers.AutoTokenizer.from_pretrained(model_name)`        |  載入預訓練的 tokenizer          |   `AutoTokenizer.from_pretrained('bert-base-uncased')`                    |
|`transformers.AutoModel.from_pretrained(model_name)`| 載入預訓練的模型 |`AutoModel.from_pretrained('bert-base-uncased')`|
|`tokenizer(text, return_tensors="pt")`| 將文本轉為數字表示，並輸出 Pytorch Tensor  | `tokenizer("hello world", return_tensors = "pt")`|
|`model(input_ids, attention_mask)` |  模型的預測 (需要輸入 Tensor)               |`model(input_ids, attention_mask)`|
|`model.generate(input_ids, max_length, num_beams, early_stopping, no_repeat_ngram_size)` | 使用模型產生文字             |`model.generate(input_ids, max_length = 50, num_beams = 5, early_stopping=True, no_repeat_ngram_size = 2)`|

**A2.10 TorchAudio 常用函數**
| 函數                         | 描述                                             | 範例                                                     |
| :--------------------------- | :----------------------------------------------- | :-------------------------------------------------------- |
|`torchaudio.load(path)` | 讀取音訊文件 |`torchaudio.load('audio.wav')`|
|`torchaudio.transforms.MFCC(sample_rate, n_mfcc)`| 將音頻轉換為 MFCC 特徵| `torchaudio.transforms.MFCC(sample_rate, n_mfcc = 40)`|
|`torchaudio.transforms.Spectrogram(n_fft)`| 將音頻轉換為 Spectrogram 特徵| `torchaudio.transforms.Spectrogram(n_fft = 512)`|

**A2.11 TorchText 常用函數**
| 函數                         | 描述                                             | 範例                                                     |
| :--------------------------- | :----------------------------------------------- | :-------------------------------------------------------- |
|`torchtext.data.utils.get_tokenizer(tokenizer)` | 建立一個 tokenizer         | `get_tokenizer("basic_english")`|
|`torchtext.vocab.build_vocab_from_iterator(iterator, specials)` | 建立詞彙表        | `build_vocab_from_iterator(map(tokenizer, data), specials=["<unk>"])`|
|`torchtext.transforms.LabelEncoder(label_encoding)` | 將文字轉換為數字 ID| `transforms.LabelEncoder(label_encoding=vocab)` |
|`torchtext.transforms.PadTransform(pad_value)` | 設定 Padding 的值         | `transforms.PadTransform(vocab["<pad>"])`|
| `torch.nn.utils.rnn.pad_sequence(tensors, batch_first, padding_value)`| 將不同長度的 Tensor list 補齊至相同長度 | `pad_sequence(tensors, batch_first=True, padding_value=0)` |

**A2.12 其他常用函數**
| 函數                         | 描述                                             | 範例                                                     |
| :--------------------------- | :----------------------------------------------- | :-------------------------------------------------------- |
|`torch.nn.utils.clip_grad_norm_(parameters, max_norm)` | 使用梯度裁剪限制梯度大小 |`torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)`|
|`torch.nn.functional.binary_cross_entropy(output, target)`| 計算二元交叉熵損失函數         |`torch.nn.functional.binary_cross_entropy(output, target)`|
|`torch.nn.functional.cross_entropy(output, target)` | 計算交叉熵損失函數  |`torch.nn.functional.cross_entropy(output, target)`|

**注意事項:**
* 這只是一份常用的函數速查表，PyTorch 還有很多其他的函數可以使用。
* 建議查閱 PyTorch 的官方文件了解更多函數的細節和使用方法。
* 你可以使用 `help(torch.function_name)` 或 `help(torch.nn.Module_name)` 來查詢函式的詳細說明。

**章節結尾**

本附錄提供了 PyTorch 中常用函數的速查表，希望對你在使用 PyTorch 時有所幫助。你可以將本附錄作為一個快速查詢的工具，以便快速找到你需要的函數，並將其應用到你的專案中。

**[下一節預告]**
附錄 C：常見問題與解答
