好的，這是一份根據你提供的綱要，為你撰寫的第十九章第五節「模型評估與推理」的詳細內容，包含文字說明、程式碼範例和圖表輔助：

**第十九章：PyTorch 語言模型 (LLM) 實作**

**19.5 模型評估與推理**

*   **19.5.1 語言模型評估指標 (Evaluation Metrics for Language Models)**

    *   **困惑度 (Perplexity)**

        困惑度 (perplexity) 是一種常用的語言模型評估指標，它衡量模型預測文本的能力。困惑度越低，表示模型的性能越好，也就是模型預測下一個詞的準確度越高。困惑度可以理解為模型預測下一個詞時，有多少種選擇是合理的。

        困惑度的數學定義是交叉熵損失函數的指數形式：

         ```
         Perplexity = exp(Loss)
         ```

        其中 `Loss` 是模型在測試數據集上的平均交叉熵損失。

        [插入一張圖表，顯示困惑度的概念，例如一個模型在預測下一個詞時，有多種合理的選擇，可以顯示不同困惑度數值，代表模型不同預測能力]

    *   **BLEU (Bilingual Evaluation Understudy)**

        BLEU (Bilingual Evaluation Understudy) 是一種常用的機器翻譯評估指標。它衡量機器翻譯的結果與人工翻譯的參考答案之間的相似程度。BLEU 分數的範圍在 0 到 1 之間，分數越高，表示翻譯質量越好。BLEU 分數的計算方法是將機器翻譯結果中的 n-gram (例如 1-gram, 2-gram, 3-gram, 4-gram) 與人工翻譯結果中的 n-gram 進行比較，並計算匹配程度。

       [插入一張圖表，顯示 BLEU 的概念，例如比較機器翻譯的結果和參考答案之間的相似程度，以呈現不同 BLEU 分數]

    *   **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**

        ROUGE (Recall-Oriented Understudy for Gisting Evaluation) 是一種常用的文本摘要評估指標。它衡量機器摘要的結果與人工摘要的參考答案之間的相似程度。ROUGE 分數通常使用召回率 (recall) 和 F1 分數 (F1-score) 來衡量，這些分數越高，表示摘要質量越好。ROUGE 考慮了參考答案中包含多少訊息被摘要中覆蓋，以及摘要中是否有錯誤的訊息。

        [插入一張圖表，顯示 ROUGE 的概念，例如比較機器摘要的結果和參考答案之間的相似程度，以呈現不同 ROUGE 分數]

*   **19.5.2 如何使用 PyTorch 進行模型評估 (How to Evaluate Language Models with PyTorch)**

    *   **計算困惑度 (Calculating Perplexity)**

        你可以通過以下步驟計算語言模型的困惑度：
        1.  將測試數據傳遞給模型，計算模型的輸出。
        2.  使用交叉熵損失函數計算模型的損失值。
        3.  對損失值取指數，得到困惑度。
        ```python
            import torch
            import torch.nn as nn
            import torch.nn.functional as F

            # 假設有訓練好的模型 model, 以及測試輸入 input_ids (batch_size, seq_len), 標籤 target_ids (batch_size, seq_len)
            # 檢查 CUDA 是否可用
            if torch.cuda.is_available():
                 device = torch.device("cuda")
            else:
                 device = torch.device("cpu")
            
            # 設定為評估模式
            model.eval()
            model.to(device)
            
            # 產生測試數據
            batch_size = 16
            seq_len = 50
            vocab_size = 100
            input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)
            target_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)
            mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)
            
            # 計算輸出
            with torch.no_grad():
               output = model(input_ids, mask)
               output = output.view(-1, vocab_size) # 需要將 output reshape 成 2 維矩陣，方便計算 cross entropy
                
            # 計算損失函數
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(output, target_ids.view(-1))

            # 計算困惑度
            perplexity = torch.exp(loss)
            print(f"Perplexity: {perplexity.item():.4f}")
        ```
         **說明：**
          *   需要將模型切換為 `model.eval()` 的評估模式。
          *   可以使用 `with torch.no_grad():` 關閉梯度計算。
          *  需要將輸出 reshape 成 2 維矩陣，才能計算 Cross-Entropy 的損失。
          *  `torch.exp()` 可以計算指數函數。

    *   **使用第三方套件計算其他評估指標 (Calculating Other Evaluation Metrics with Third-Party Libraries)**

        可以使用以下第三方套件來計算 BLEU 和 ROUGE 分數：
        *  **nltk:** `pip install nltk`
             ```python
                import nltk
                from nltk.translate.bleu_score import sentence_bleu

                # 下載 nltk 工具
                nltk.download('punkt')

                # 模擬數據
                reference = [['the', 'cat', 'sat', 'on', 'the', 'mat']]
                candidate = ['the', 'cat', 'sat', 'on', 'a', 'mat']
                
                # 計算 BLEU
                bleu_score = sentence_bleu(reference, candidate)
                print("BLEU Score:", bleu_score)
            ```
         *   **rouge-score:** `pip install rouge-score`
             ```python
                from rouge_score import rouge_scorer
                
                # 建立 rouge scorer
                scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
                
                # 模擬數據
                reference = "the cat sat on the mat"
                candidate = "the cat sat on a mat"

                # 計算 ROUGE 分數
                scores = scorer.score(reference, candidate)
                print("ROUGE Score:", scores)
            ```
    **說明：**
     *  需要安裝 NLTK 和 Rouge-score 套件。
     *  `nltk.translate.bleu_score.sentence_bleu()` 可以計算 BLEU 分數。
     *  `rouge_score.rouge_scorer.RougeScorer()` 可以建立 rouge scorer。
     *  `scorer.score()` 可以計算 ROUGE 分數。

*   **19.5.3 模型推理 (Model Inference)**

    *   **使用 PyTorch 模型進行文本生成 (Generating Text with PyTorch Models)**

        你可以使用訓練好的語言模型來生成文本。在生成文本時，通常使用以下方法：
        1.  **初始輸入：** 給定一個初始的詞語或句子。
        2.  **模型輸出：** 將當前詞序列傳遞給模型，預測下一個詞的機率分佈。
        3.  **選擇下一個詞：** 根據機率分布，選擇機率最高的詞或使用其他抽樣方式來選擇下一個詞。
        4.  **更新詞序列：** 將選擇的下一個詞加到當前詞序列的末尾。
        5.  **重複步驟 2-4：** 重複執行，直到生成指定長度的文本或遇到結束符號。

          [插入一張圖表，顯示如何使用語言模型生成文本，例如模型根據先前的詞彙，依序預測出下一個詞]

    *   **Beam Search 解碼 (Beam Search Decoding)**

        Beam Search 是一種常用的解碼演算法，用於文本生成任務。它與貪婪解碼 (greedy decoding) 不同，Beam Search 會同時追蹤多個候選序列，並選擇總機率最高的序列作為最終的輸出。Beam Search 使用一個 beam size 的參數，控制每次保留的候選序列的數量。Beam Search 可以生成比貪婪解碼更高品質的文本，但也需要更多的計算資源。

         [插入一張圖表，顯示 Beam Search 的過程，例如每次保留多個候選序列，並擴展到下一個詞]

    *   **貪婪解碼 (Greedy Decoding)**

        貪婪解碼是一種簡單的文本生成算法，它在每個步驟中都選擇機率最高的詞作為下一個詞。貪婪解碼雖然計算速度快，但生成的文本品質通常不如 Beam Search 高，因為它只考慮當前的最佳選擇，而沒有考慮到全局的最佳解。

        以下是如何使用貪婪解碼，從 Transformer 模型中生成文本的範例：
        ```python
            import torch
            import torch.nn as nn
           from transformers import BertModel, BertTokenizer

            def greedy_decode(model, input_ids, tokenizer, max_len = 50, device = "cpu"):
             model.eval()
              
             with torch.no_grad():
               for _ in range(max_len):
                    
                    output = model(input_ids.to(device))
                    next_token_logits = output.logits[:, -1, :] # 取最後一個 token 的輸出
                    next_token_id = torch.argmax(next_token_logits, dim=-1) # 貪婪選擇機率最高的 token

                    # 將新產生的 token 加到 input_ids 中
                    input_ids = torch.cat([input_ids, next_token_id.unsqueeze(1)], dim = 1)

                    if next_token_id == tokenizer.sep_token_id: # 如果遇到結束符號，停止生成
                        break

               generated_text = tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True) # 將數字 id 轉換為文本
               return generated_text
    
          # 1. 載入預訓練的 BERT 模型
          model_name = 'bert-base-uncased'
          tokenizer = BertTokenizer.from_pretrained(model_name)
          model = BertModel.from_pretrained(model_name)

          # 2. 檢查 CUDA 是否可用
          if torch.cuda.is_available():
            device = torch.device("cuda")
          else:
             device = torch.device("cpu")
          
          # 3. 將模型移動到 GPU
          model.to(device)

          # 4. 建立起始文本
          text = "Hello, how are you?"
          input_ids = tokenizer(text, return_tensors = "pt").input_ids
         
          # 5. 使用貪婪解碼生成文本
          generated_text = greedy_decode(model, input_ids, tokenizer, device = device)
          print("Generated text:", generated_text)
        ```
        **說明：**
          *  `greedy_decode()` 函式可以從 Transformer 模型中貪婪地生成文字。
          *   在生成文字時，每次選機率最高的 token，並加到輸入序列的後面。
          *  需要設定最大生成長度，避免無限生成。
          *   遇到 `tokenizer.sep_token_id` 就停止生成，通常 `sep_token_id` 會代表 [SEP] 這個特殊符號，也就是句子的結束。
          *   `tokenizer.decode()` 可以將 token 的數字 id 轉換回文字。

**章節結尾**

本節中，我們學習了語言模型評估與推理的重要概念。你學習了以下內容：

*   語言模型常用的評估指標，包括困惑度、BLEU 和 ROUGE。
*   如何使用 PyTorch 計算困惑度。
*   如何使用第三方套件計算 BLEU 和 ROUGE 分數。
*  如何使用 PyTorch 模型進行文本生成。
*   Beam Search 解碼和貪婪解碼的原理。

在下一節中，我們將學習 LLM 的部署與應用。請繼續努力！

**[下一節預告]**
19.6 LLM 部署與應用

希望這份第十九章第五節的內容對你有所幫助！記得在實際寫作時，盡可能加入更多圖表和程式碼範例，並詳細解釋每一個步驟，讓讀者更容易理解和掌握。
