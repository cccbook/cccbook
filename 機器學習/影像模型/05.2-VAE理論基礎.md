#### 5.2 VAE理論基礎

變分自編碼器（Variational Autoencoder, VAE）是自編碼器（AE）的擴展，它不僅學習數據的隱含表示，還將隱含空間建模為概率分佈。VAE的核心概念是在生成模型中引入隨機性，並使用變分推斷（Variational Inference）來進行訓練。VAE不僅能夠重建輸入數據，還能生成新數據，這使得它成為一種強大的生成模型。

VAE的基本框架包含兩個主要部分：
1. **編碼器**：將輸入數據映射到隱含空間的概率分佈。
2. **解碼器**：從隱含空間的表示中生成數據。

VAE的目標是學習一個將數據轉換為潛在變量空間的映射，並能夠從該潛在變量中生成數據。這個過程的挑戰在於，如何設計一個可以有效學習這些潛在變量的模型。

##### 5.2.1 變分推斷

在VAE中，我們不直接學習隱藏變量 \( z \)，而是學習其概率分佈。對於給定的觀察數據 \( x \)，VAE的目標是學習隱藏變量 \( z \) 的後驗分佈 \( p(z|x) \)，這是基於觀察數據的隱藏變量的條件概率。

然而，直接計算後驗分佈 \( p(z|x) \) 是不現實的，因為它包含了難以解析的積分。為了解決這個問題，VAE引入了變分推斷方法，使用一個簡單的分佈 \( q(z|x) \) 來近似後驗分佈 \( p(z|x) \)。VAE的訓練目標是最大化變分下界（Variational Lower Bound, ELBO），即最大化近似後驗分佈與真實後驗分佈之間的相似度。

變分下界可以表達為：
\[
\mathcal{L}(x) = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{\text{KL}}[q(z|x) \| p(z)]
\]
其中：
- 第一項 \( \mathbb{E}_{q(z|x)}[\log p(x|z)] \) 是重建誤差，衡量從潛在變量 \( z \) 重建數據 \( x \) 的效果。
- 第二項 \( D_{\text{KL}}[q(z|x) \| p(z)] \) 是KL散度，度量近似分佈 \( q(z|x) \) 和先驗分佈 \( p(z) \) 之間的差異。這一項促使模型學會將隱含變量的分佈逼近先驗分佈。

##### 5.2.2 潛在變量空間建模

在VAE中，隱含變量 \( z \) 被建模為一個高斯分佈。這意味著我們假設潛在變量具有一個均值和一個方差，並且從中生成數據。VAE的編碼器學習將數據映射到潛在空間的均值 \( \mu(x) \) 和方差 \( \sigma(x) \)，這兩者都由神經網絡決定。

具體來說，編碼器會將輸入數據 \( x \) 映射到隱含變量的均值和對數方差，從這些參數中我們可以抽取潛在變量 \( z \)。為了進行有效的反向傳播，VAE使用了重參數化技巧（Reparameterization Trick），將隨機性從模型的訓練過程中分離出來。

重參數化技巧的核心思想是：
\[
z = \mu(x) + \sigma(x) \cdot \epsilon
\]
其中 \( \epsilon \sim \mathcal{N}(0, I) \)，即來自標準正態分佈的噪聲。這樣，我們就能夠通過神經網絡計算 \( \mu(x) \) 和 \( \sigma(x) \)，並將這些參數應用於重參數化過程，進而有效地訓練模型。

##### 5.2.3 重建誤差與KL散度

VAE的目標是最小化重建誤差和KL散度的加權和。重建誤差衡量了模型生成的數據與原始數據之間的差異，而KL散度則確保隱藏變量的分佈接近先驗分佈。

VAE的損失函數（即變分下界）如下所示：
\[
\mathcal{L}_{\text{VAE}} = - \mathbb{E}_{q(z|x)}[\log p(x|z)] + D_{\text{KL}}[q(z|x) \| p(z)]
\]
其中，第一項是重建誤差，第二項是KL散度。VAE在訓練過程中會同時最小化這兩項，從而學習到既能有效生成數據又能保持隱藏變量空間結構的模型。

##### 小結

變分自編碼器（VAE）是一種生成模型，通過引入隨機性並將隱藏變量建模為概率分佈來提高生成數據的能力。VAE的訓練目標是最大化變分下界，這包括重建誤差和KL散度兩部分。重參數化技巧使得VAE能夠進行有效的反向傳播，並從數據中學習潛在變量的結構。VAE在許多領域中都取得了顯著的成功，特別是在生成圖像和數據降維等方面。