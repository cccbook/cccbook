### **2.5 自注意力機制與多頭注意力**

自注意力機制（Self-Attention Mechanism）和多頭注意力（Multi-Head Attention）是Transformer架構的核心組成部分，也是語言模型如BERT、GPT等成功的關鍵。這些技術使得模型能夠有效地捕捉序列中各個位置之間的依賴關係，並在處理長序列時展現出優越的表現。

#### **2.5.1 自注意力機制（Self-Attention Mechanism）**

自注意力機制旨在讓每個序列元素（如單詞或字符）在計算其表示時，可以根據其他所有元素的資訊進行加權。這意味著每個詞的表示會受到序列中其他所有詞的影響，從而能夠捕捉到上下文中長距離的依賴關係。

- **工作原理**：
  假設我們有一個輸入序列，記為 \( x_1, x_2, \dots, x_n \)，每個元素 \( x_i \) 都會被映射到三個向量：查詢（Query）、鍵（Key）和值（Value）。這些向量是通過將輸入嵌入向量乘以不同的學習權重矩陣來計算的。

  對於序列中的每個元素 \( x_i \)，自注意力機制會計算該元素的查詢向量與其他元素的鍵向量的相似度，這個相似度反映了元素間的依賴關係。然後，基於這些相似度加權值向量，得到該元素的最終表示。具體步驟如下：

  1. **計算查詢、鍵和值向量**：
     \[
     Q = XW_Q, \quad K = XW_K, \quad V = XW_V
     \]
     其中 \( W_Q \)、\( W_K \)、\( W_V \) 是學習得到的權重矩陣，\( X \) 是輸入嵌入矩陣。

  2. **計算注意力權重**：通過計算查詢向量與鍵向量的點積來衡量它們之間的相似度，然後通過softmax函數將這些相似度轉換為權重：
     \[
     \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
     \]
     其中 \( d_k \) 是鍵向量的維度，用來做縮放操作，避免點積值過大。

  3. **加權和值的加和**：通過將計算得到的權重與值向量相乘，得到每個元素的最終表示。

- **優點**：
  - 可以捕捉到遠距離的依賴關係，相比於傳統的RNN和LSTM，避免了信息在長序列中逐漸消失的問題。
  - 並行計算效率高，可以在多個處理器上並行處理序列的不同部分。

- **缺點**：
  - 計算成本較高，特別是在處理長序列時，因為每個元素都需要與其他所有元素進行比較，計算量為 \( O(n^2) \)（其中 \( n \) 是序列長度）。

#### **2.5.2 多頭注意力（Multi-Head Attention）**

多頭注意力是自注意力的一個擴展，它通過並行計算多個自注意力機制來捕捉不同的語言特徵或依賴關係。每個頭（head）都學習到不同的查詢、鍵和值映射，從而使得模型能夠以多種方式聚焦於序列中的不同部分。

- **工作原理**：
  多頭注意力的基本想法是將自注意力機制擴展為多個並行的「頭」，每個頭對應於一組查詢、鍵和值矩陣。具體步驟如下：

  1. **多個注意力頭**：將輸入序列映射到多組查詢、鍵和值向量：
     \[
     Q_i = XW_Q^i, \quad K_i = XW_K^i, \quad V_i = XW_V^i, \quad \text{for each head } i
     \]
     每個頭使用不同的權重矩陣 \( W_Q^i \)、\( W_K^i \)、\( W_V^i \)。

  2. **並行計算注意力**：對每個頭，計算注意力權重：
     \[
     \text{Attention}_i(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_iK_i^T}{\sqrt{d_k}}\right) V_i
     \]
     每個頭都會產生一個加權值的向量。

  3. **合併頭的輸出**：將每個頭的輸出拼接起來，然後通過一個線性變換得到最終的注意力表示：
     \[
     \text{Multi-Head Attention}(Q, K, V) = \text{concat}(\text{Attention}_1, \dots, \text{Attention}_h)W_O
     \]
     其中 \( h \) 是頭的數量，\( W_O \) 是用來融合所有頭的線性權重矩陣。

- **優點**：
  - 能夠捕捉不同語言特徵和依賴關係，因為每個頭專注於序列中的不同部分。
  - 提高了模型的表達能力，從而增強了模型的學習效果。
  - 並行計算，提高了訓練和推理的效率。

- **缺點**：
  - 增加了計算複雜度，因為需要計算多個注意力頭。

#### **2.5.3 自注意力和多頭注意力在Transformer中的應用**

自注意力機制和多頭注意力是Transformer架構的關鍵，Transformer的Encoder和Decoder都基於這些機制來建構高效的語言模型。

- **Encoder**：Encoder由多層自注意力和前向傳播層（Feed-Forward Layer）組成，每層都包括自注意力機制和多頭注意力。自注意力機制使得Encoder能夠考慮到整個序列的上下文，並根據多頭注意力的結果來聚焦不同的語言特徵。

- **Decoder**：Decoder與Encoder類似，但它還包括一個額外的自回歸機制（Masked Self-Attention），即在計算每個位置的表示時，僅依賴前面已生成的詞語。這種結構確保了生成過程的自回歸性。

#### **2.5.4 實際應用**

自注意力機制和多頭注意力在語言模型中有著廣泛的應用，特別是在下列任務中：

- **機器翻譯**：能夠有效地捕捉源語言和目標語言之間的長距離依賴關係。
- **文本生成**：如GPT等生成式模型可以利用這些技術生成高質量的文本。
- **文本分類**：在BERT等模型中，利用多頭注意力來學習文本的關鍵特徵，並進行分類。

自注意力和多頭注意力使得Transformer架構能夠在許多NLP任務中取得突破性的成果，成為現代語言模型的基礎技術。