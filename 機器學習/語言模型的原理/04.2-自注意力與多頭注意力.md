### **自注意力與多頭注意力**

在 Transformer 模型中，自注意力機制和多頭注意力機制是核心的創新之一，這些機制使得模型能夠捕捉序列中不同位置詞之間的依賴關係，並且提高了模型的並行性和計算效率。以下將詳細介紹自注意力和多頭注意力的工作原理及其作用。

#### **1. 自注意力（Self-Attention）**

自注意力機制的主要目的是讓模型在處理序列數據時，能夠根據當前輸入的詞來「關注」序列中的其他詞。這樣的機制使得每個詞的表示能夠根據整個句子的上下文進行調整，而不僅僅依賴於它自己的詞向量。

##### **自注意力的計算過程**

自注意力機制基於三個向量：**查詢（Query）**、**鍵（Key）**、**值（Value）**。這三個向量是從每個詞的嵌入向量（embedding）通過線性變換得到的。具體而言，對於輸入序列中的每一個詞，我們會生成一組查詢、鍵和值向量，這些向量將用於計算該詞與其他詞之間的關聯程度。

1. **查詢（Query）**：用來「查詢」該詞是否應該關注序列中的其他詞。
2. **鍵（Key）**：用來表示序列中每個詞的「內容」。
3. **值（Value）**：用來實際提供該詞的內容信息。

自注意力的核心是計算每對查詢和鍵之間的相似度，並利用這些相似度來加權值向量，得到該詞的上下文表示。

計算過程如下：

- **計算相似度**：對於每一個查詢向量 \(Q\) 和鍵向量 \(K\)，我們計算它們的內積來衡量相似度：

  \[
  \text{score}(Q, K) = Q \cdot K^T
  \]

- **進行縮放**：為了避免內積過大或過小，我們將得分除以鍵向量的維度的平方根 \( \sqrt{d_k} \)，這樣能夠使得注意力權重的分佈更加穩定：

  \[
  \text{scaled\_score}(Q, K) = \frac{Q \cdot K^T}{\sqrt{d_k}}
  \]

- **計算注意力權重**：將相似度分數通過 softmax 函數進行歸一化，得到注意力權重：

  \[
  \text{Attention Weight} = \text{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right)
  \]

- **加權求和**：最後，根據計算出的注意力權重，對值向量 \(V\) 進行加權求和，得到該詞的上下文表示：

  \[
  \text{Attention Output} = \text{Attention Weight} \cdot V
  \]

這樣，經過自注意力層的處理，每個詞的表示就會融合來自其他詞的信息，從而能夠捕捉到序列中的長期依賴關係。

#### **2. 多頭注意力（Multi-Head Attention）**

多頭注意力是 Transformer 架構中進一步改進自注意力的機制，它允許模型在不同的「子空間」中並行學習不同的注意力模式，從而增加模型的表達能力。多頭注意力通過將自注意力機制擴展為多個獨立的「頭」，每個頭學習一個不同的注意力模式，並最終將所有頭的輸出拼接起來，進行進一步的線性變換。

##### **多頭注意力的運作**

多頭注意力的基本思想是將查詢、鍵和值向量進行切分，並分別對每個子集執行自注意力計算。具體來說：

1. **切分查詢、鍵和值**：首先，將查詢向量 \(Q\)、鍵向量 \(K\) 和值向量 \(V\) 分成 \(h\) 個子空間，每個子空間的維度為 \(d_k/h\)，這樣每個頭就能學到不同的注意力模式。

2. **計算多個注意力頭**：對每個子空間的查詢、鍵和值進行自注意力計算，得到 \(h\) 個不同的注意力輸出。

3. **拼接和線性變換**：將所有 \(h\) 個注意力頭的輸出拼接成一個長向量，並通過一個線性變換矩陣 \(W^O\) 得到最終的注意力輸出：

  \[
  \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
  \]

其中，\(\text{head}_i\) 是第 \(i\) 個注意力頭的輸出，計算過程如前所述。每個頭學到不同的關聯信息，因此多頭注意力能夠捕捉到不同層次和不同範圍的語境信息。

##### **數學公式**

多頭注意力的具體計算過程如下：

1. 每個頭的計算公式：

  \[
  \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
  \]

2. 拼接所有頭的結果：

  \[
  \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
  \]

#### **3. 小結**

自注意力機制使得每個詞能夠根據序列中其他詞的上下文來調整其表示，而多頭注意力進一步提升了模型的表達能力，讓模型能夠學習到多層次、不同視角的語境信息。這種機制在 Transformer 架構中發揮了至關重要的作用，並成為了現代語言模型（如 BERT、GPT）的核心技術。通过這些注意力機制，Transformer 能夠有效地捕捉長期依賴，並且具備了高度的並行計算能力。