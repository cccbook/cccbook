參考 -- [吳冠緯:RLHF vs DPO](https://hackmd.io/@dave0907/rkpFaDROp)

### ccc: DPO 是Direct Preference Optimization: Your Language Model is Secretly a Reward Model 這篇論文提出的慨念

抱歉之前的理解錯誤，您提到的 **DPO** 確實是指 **Direct Preference Optimization**，這是由 Google Research 在 2023 年提出的，並且與傳統強化學習的策略優化方法不同。DPO 是一種用來直接訓練語言模型的方法，特別是在使用人類反饋優化模型時，避免了複雜的強化學習回報信號和策略梯度的計算，並且能夠有效地進行模型優化。

### **DPO（Direct Preference Optimization）概念背景**

DPO 是在強化學習中一個新興的方法，它利用人類偏好的比較信息來直接優化模型，而不需要傳統的強化學習中的回報設計或獎勳信號。這意味著它可以直接用於語言模型的訓練，且不必額外設計一個獨立的強化學習回報模型。

#### **1. 人類反饋的引入**
DPO 是基於人類反饋的生成模型訓練方法。人類反饋通常是根據一組生成文本的偏好進行排序，這些排序反映了模型生成文本的相對質量。與傳統強化學習中需要根據某個具體的獎勳信號來進行學習不同，DPO 直接使用這些偏好排序來進行模型的更新。

#### **2. DPO 訓練過程**

DPO 的核心是通過優化一個 **偏好排序損失（preference loss）** 來更新模型。具體來說，這些偏好是由人類標註者基於生成的候選文本來確定的，並以這些偏好排序來直接優化語言模型的生成策略。

假設模型生成了兩個候選文本 \(y_1\) 和 \(y_2\)，並且根據人類標註者的偏好，\(y_1\) 比 \(y_2\) 更好。那麼，DPO 就會根據這種偏好對模型進行優化，使得模型生成 \(y_1\) 的概率大於 \(y_2\) 的概率。

數學上，DPO 的目標是最大化偏好排序的概率，這可以通過最小化下面的損失來達到：

\[
\mathcal{L}_{\text{DPO}} = -\sum_{(y_1, y_2)} \log \left( \frac{p(y_1|x)}{p(y_1|x) + p(y_2|x)} \right)
\]

其中：
- \( p(y|x) \) 是語言模型對生成文本 \(y\) 在給定上下文 \(x\) 下的概率。
- \( (y_1, y_2) \) 是由人類標註者根據文本偏好進行排序的兩個候選文本。

### **3. 主要優勢**

- **簡化的訓練過程：** 由於不依賴於複雜的強化學習回報模型和策略梯度計算，DPO 可以更簡單地進行訓練。
- **直接基於偏好的學習：** DPO 直接根據人類偏好來進行優化，不需要設計額外的回報模型，從而減少了強化學習中的繁瑣調參過程。
- **高效的樣本利用：** 因為 DPO 使用的是人類反饋（通常基於相對偏好排序），因此能夠在有限的標註數據上進行高效學習。

### **4. DPO 的數學基礎**

在 DPO 中，模型的目標是最大化根據人類偏好排序的機率，這意味著我們希望模型能夠「預測」哪些生成結果更符合人類的期望。這一過程可以看作是最大化生成對的預測概率。對於每一對候選文本 \(y_1\) 和 \(y_2\)，我們希望模型能夠將正確的文本（即人類更喜歡的文本）給出更高的概率。

數學上，這可以表達為最大化偏好排序的對數似然損失：

\[
\mathcal{L}_{\text{DPO}} = - \mathbb{E}_{x, y_1, y_2} \left[ \log \left( \frac{p(y_1 | x)}{p(y_1 | x) + p(y_2 | x)} \right) \right]
\]

這裡的期望是對所有可能的輸入 \(x\) 及其對應的候選文本對 \(y_1\) 和 \(y_2\) 進行積分。換句話說，這是基於人類偏好的排序訓練語言模型，使其能夠對偏好的排序做出正確的預測。

### **5. 與傳統強化學習的區別**

傳統的強化學習中，智能體通常會根據回報信號來更新其策略，這些回報信號可能來自於環境的反饋。相比之下，DPO 不依賴於這樣的獎勳信號，而是基於人類的偏好排序來直接進行優化。這使得 DPO 成為一種更加直觀且高效的方法，尤其是在訓練語言模型時，因為人類偏好本身可以作為直接的指導信號。

### **6. 實際應用與挑戰**

DPO 被認為是一種較為簡單和高效的優化方法，特別是在處理生成模型（如語言生成）的任務時。實際應用中，DPO 能夠用於各種基於人類反饋的模型訓練，像是對話系統、文本生成、摘要生成等。然而，DPO 仍面臨一些挑戰，特別是在如何高效地收集大量的偏好數據以及如何進一步改善模型的泛化能力等問題。

### **總結**

DPO（Direct Preference Optimization）是一種基於人類偏好的直接優化方法，通過簡化的訓練過程和基於偏好的回報更新來優化語言模型。這種方法避免了傳統強化學習中繁瑣的回報信號和策略梯度計算，提供了一種新的優化語言模型的思路。