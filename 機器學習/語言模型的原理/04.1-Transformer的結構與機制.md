### **Transformer 的結構與機制**

Transformer 是一種革命性的神經網絡架構，主要被應用於序列到序列的任務中，特別是在自然語言處理（NLP）領域，對機器翻譯、文本生成、語音識別等任務產生了深遠影響。它首次由 Vaswani 等人在 2017 年的論文《Attention is All You Need》中提出，摒棄了傳統的循環神經網路（RNN）和長短期記憶網路（LSTM），改用完全基於注意力機制的架構，從而顯著提高了並行計算效率和長期依賴建模能力。

#### **1. Transformer 的整體結構**

Transformer 模型的核心思想是使用多層的注意力機制來處理序列數據，將原始序列進行編碼並轉換為上下文相關的表示。Transformer 的結構分為兩大部分：

1. **編碼器（Encoder）**：負責處理輸入序列，並將其轉換為一組上下文相關的隱藏表示。
2. **解碼器（Decoder）**：負責根據編碼器的輸出生成目標序列，並將模型的最終預測產生。

編碼器和解碼器都由多層相同的結構組成，每層包含兩個主要的子結構：注意力層（Attention Layer）和前向傳遞層（Feed Forward Layer）。

#### **2. 編碼器（Encoder）**

編碼器的主要目的是將輸入序列轉換為一組上下文相關的隱藏狀態，這些狀態包含了輸入序列的語言信息和結構特徵。每一層編碼器由兩個子層構成：

1. **自注意力機制層（Self-Attention Layer）**：這一層用於計算輸入序列中各個詞之間的關係，並將其加權，生成每個詞的上下文表示。自注意力機制根據查詢（Query）、鍵（Key）和值（Value）的關係來計算每個詞與其他詞的注意力分數，這些分數用來加權輸入序列中各個位置的詞。

2. **前向傳遞層（Feed Forward Layer）**：這一層由兩層全連接神經網絡組成，負責對注意力層的輸出進行非線性變換，進一步提升模型的表達能力。

每一層的輸出會通過層正則化（Layer Normalization）和殘差連接（Residual Connection）進行處理，這能夠幫助加速訓練並穩定模型。

#### **3. 解碼器（Decoder）**

解碼器的作用是生成目標序列，它的結構與編碼器類似，但有一些關鍵的區別。每一層的解碼器也包括兩個主要部分：

1. **自注意力層（Masked Self-Attention Layer）**：解碼器中的自注意力層與編碼器中的自注意力層相似，但是這裡進行了遮蔽（masking）。這樣的遮蔽機制確保了模型在生成每一個詞的過程中，只能看到已經生成的詞，而無法提前看到未來的詞，這樣保證了生成過程的順序性。

2. **編碼器-解碼器注意力層（Encoder-Decoder Attention Layer）**：這一層的作用是根據編碼器的輸出來幫助解碼器生成對應的目標序列。通過與編碼器的輸出進行交互，解碼器能夠聚焦於與當前解碼詞相關的輸入詞，從而更好地生成翻譯或其他類型的輸出。

3. **前向傳遞層（Feed Forward Layer）**：與編碼器中的前向傳遞層類似，解碼器的前向傳遞層負責進行非線性變換，將其輸出進行處理以提高模型的表達能力。

解碼器的每一層同樣會經過層正則化和殘差連接的處理。

#### **4. 注意力機制的運作**

注意力機制是 Transformer 架構中的關鍵創新之一，它使得模型可以根據當前的上下文選擇性地「關注」輸入序列中的不同部分。自注意力機制的核心運作是計算每個詞與序列中其他詞的相關性，並根據這些關聯計算加權和。具體步驟如下：

- **查詢（Query）、鍵（Key）和值（Value）向量**：自注意力機制的輸入是三個向量，分別是查詢（Q）、鍵（K）和值（V）。這些向量是通過將輸入嵌入向量進行線性變換而得到的。
  
- **注意力權重計算**：模型計算查詢和鍵之間的相似度，然後通過 softmax 函數對相似度進行歸一化，得到注意力權重。
  
- **加權和**：根據注意力權重，將值向量加權求和，得到加權表示。這一過程保證了每個輸入詞對其他詞的依賴關係被考慮在內。

具體數學公式如下：

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\]

其中，\(d_k\) 是鍵向量的維度，用於縮放內積的大小，防止內積過大導致梯度消失問題。

#### **5. Transformer 的多頭注意力**

為了讓模型能夠學習到更多維度的信息，Transformer 採用了 **多頭注意力機制**（Multi-Head Attention）。這一機制將查詢、鍵和值向量分成多個子空間，並行計算每個子空間的注意力。最終，將多個注意力頭的結果進行拼接，並進行線性變換。

數學表達為：

\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
\]

其中，\(\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\) 是每個注意力頭的輸出，並且 \(W_i^Q\), \(W_i^K\), \(W_i^V\) 是學習的權重矩陣。

#### **6. 位置編碼（Positional Encoding）**

由於 Transformer 模型不依賴於序列的順序處理，它無法直接捕捉序列中詞的位置信息。為了解決這一問題，Transformer 引入了 **位置編碼**（Positional Encoding），將每個詞的位置資訊與其嵌入向量結合。

位置編碼是一組基於正弦和餘弦函數的固定向量，其維度與詞嵌入維度相同。這些位置向量會與詞嵌入向量相加，從而使得模型能夠識別每個詞在序列中的位置。

位置編碼的數學公式如下：

\[
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)
\]
\[
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
\]

其中，\(pos\) 是詞的位置，\(i\) 是位置向量中的維度，\(d\) 是嵌入向量的維度。

#### **7. 小結**

Transformer 是一個強大的神經網絡架構，它利用自注意力機制和多頭注意力來捕捉序列中不同位置之間的關聯，從而克服了 RNN 和 LSTM 模型在長序列處理中的局限性。其並行計算能力和高效的長期依賴建模能力使其在許多自然語言處理任務中取得了突破性的成就，並成為現代語言模型（如 BERT、GPT）的基礎。