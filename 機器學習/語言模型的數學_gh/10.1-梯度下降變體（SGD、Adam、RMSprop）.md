### **第 10 章：優化算法與模型訓練**

#### **10.1 梯度下降變體（SGD、Adam、RMSprop）**

在機器學習和深度學習中，優化算法是模型訓練的核心，目的是通過不斷調整模型參數，最小化損失函數。在這一過程中，**梯度下降**（Gradient Descent）是最常用的優化方法，然而，隨著模型和數據集的增大，基本的梯度下降方法會遇到效率低下和收斂緩慢等問題，因此，出現了多種梯度下降的變體來解決這些問題，其中最常用的包括 **SGD**（隨機梯度下降）、**Adam** 和 **RMSprop**。

##### **10.1.1 基本梯度下降法（Gradient Descent）**

梯度下降法的基本思想是，根據損失函數對模型參數的梯度來更新參數。對於每一個模型參數  $`\theta`$ ，其更新規則為：


```math
\theta = \theta - \eta \cdot \nabla_\theta J(\theta)
```


其中， $`\eta`$  是學習率（learning rate）， $`\nabla_\theta J(\theta)`$  是損失函數  $`J(\theta)`$  相對於參數  $`\theta`$  的梯度。

基本梯度下降的缺點是每次更新都依賴於所有訓練樣本，因此每次更新的計算成本較高，尤其是在大規模數據集上訓練時。

##### **10.1.2 隨機梯度下降（SGD）**

為了解決基本梯度下降計算成本高的問題，**隨機梯度下降**（Stochastic Gradient Descent，SGD）將每次更新限制為隨機選擇一個訓練樣本進行參數更新。這樣，對於每個參數的更新規則為：


```math
\theta = \theta - \eta \cdot \nabla_\theta J(\theta; x^{(i)}, y^{(i)})
```


其中， $`x^{(i)}, y^{(i)}`$  是第  $`i`$  個訓練樣本。由於每次更新只使用一個樣本，這樣的更新方式較快，但也有較大的波動，導致收斂不穩定。

**優點**：
- 計算速度較快，尤其適用於大數據集。
- 由於隨機性，每次更新都有機會跳出局部最小值。

**缺點**：
- 更新過程中波動較大，可能無法穩定收斂。

##### **10.1.3 小批量隨機梯度下降（Mini-batch SGD）**

為了解決 SGD 中的波動性問題，**小批量隨機梯度下降**（Mini-batch SGD）被提出來。它的基本思想是每次使用一小批（mini-batch）訓練樣本來更新參數，而不是使用單個樣本。這樣，更新規則為：


```math
\theta = \theta - \eta \cdot \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta; x^{(i)}, y^{(i)})
```


其中， $`m`$  是小批量的大小，這樣可以在一定程度上平衡計算效率和收斂穩定性。

##### **10.1.4 Adam（自適應矩估計）**

**Adam**（Adaptive Moment Estimation）是目前最常用的優化算法之一。它結合了動量（Momentum）和自適應學習率的思想，通過計算一階矩（即梯度的均值）和二階矩（即梯度的平方的均值）來調整每個參數的學習率。Adam 的更新規則如下：

1. 計算一階矩和二階矩的估計：


```math
m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta J(\theta)
```


```math
v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_\theta J(\theta))^2
```


2. 對一階矩和二階矩進行偏差修正：


```math
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
```


3. 更新參數：


```math
\theta = \theta - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
```


其中， $`\beta_1`$  和  $`\beta_2`$  分別是控制一階矩和二階矩衰減的超參數， $`\epsilon`$  是防止除零錯誤的小常數。

**優點**：
- 結合了動量和自適應學習率，能夠加速收斂。
- 適用於大規模數據和參數多的問題，計算高效。
- 在大多數情況下可以自動調整學習率。

**缺點**：
- 計算開銷較大（需要保存額外的一階和二階矩）。
- 可能在某些問題上會陷入過早的收斂，特別是學習率較高時。

##### **10.1.5 RMSprop（Root Mean Square Propagation）**

**RMSprop** 是另一個常用的優化算法，它專注於自適應學習率的調整。RMSprop 會計算每個參數的梯度平方的移動平均，並使用這個值來調整學習率。RMSprop 的更新規則如下：

1. 計算梯度平方的移動平均：


```math
v_t = \beta v_{t-1} + (1 - \beta) (\nabla_\theta J(\theta))^2
```


2. 更新參數：


```math
\theta = \theta - \eta \cdot \frac{\nabla_\theta J(\theta)}{\sqrt{v_t} + \epsilon}
```


其中， $`\beta`$  是衰減率， $`\epsilon`$  是防止除零錯誤的小常數。

**優點**：
- 能夠有效應對梯度爆炸和梯度消失的問題。
- 具有自適應學習率，能夠更好地處理不同維度的參數。

**缺點**：
- 需要調整超參數，尤其是學習率和衰減率。
- 計算過程比基本的 SGD 略為複雜。

#### **10.1.6 結論**

不同的優化算法各有優缺點，根據具體的任務和數據集，可以選擇合適的算法來訓練模型。通常來說，**Adam** 是一個較為通用且高效的選擇，尤其適合於大規模深度學習問題；而 **RMSprop** 則常用於需要穩定自適應學習率的場景；**SGD** 則適用於計算資源有限的情況，並且與小批量更新（Mini-batch）結合使用能提高訓練速度和效果。