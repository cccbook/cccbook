### **8.3 層正規化的統計基礎**

**層正規化**（Layer Normalization）是一種用於神經網絡訓練的技術，旨在改善網絡訓練的穩定性和加速收斂。它主要通過對每一層的激活進行標準化來調整數據的分佈，減少內部協變量偏移（Internal Covariate Shift）問題。

層正規化的核心思想是，對每一層的激活進行歸一化處理，使得每層的輸出均值為零，方差為一。這有助於保持每層的激活值處於適當的範圍，從而提高模型的訓練效率。

#### **8.3.1 層正規化的數學描述**

在層正規化中，我們對每一層的輸出進行標準化，具體公式如下：


```math
\hat{x}_i = \frac{x_i - \mu}{\sigma}
```


其中：
-  $`x_i`$  是第  $`i`$  個樣本在當前層的激活值，
-  $`\mu`$  是當前層激活值的均值，
-  $`\sigma`$  是當前層激活值的標準差。

即，對於一個向量  $`x = [x_1, x_2, \dots, x_n]`$ （這裡的  $`x_i`$  是第  $`i`$  層的激活），首先計算這個向量的均值和標準差：


```math
\mu = \frac{1}{n} \sum_{i=1}^{n} x_i
```



```math
\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2}
```


然後，對每一個激活  $`x_i`$  進行標準化處理，得到每一個  $`\hat{x}_i`$ ：


```math
\hat{x}_i = \frac{x_i - \mu}{\sigma}
```


#### **8.3.2 加權與偏置調整**

在層正規化中，標準化後的輸出常常會被進一步調整，以便適應不同的數據分佈。具體地，標準化後的激活值會進行縮放和偏移操作：


```math
y_i = \gamma \hat{x}_i + \beta
```


其中：
-  $`\gamma`$  是學習到的縮放參數，控制每一層的輸出範圍，
-  $`\beta`$  是學習到的偏置參數，調整輸出的平移。

這樣一來，層正規化不僅保證了每層的激活均值為零，標準差為一，還能根據數據特性調整激活的範圍和分佈。

#### **8.3.3 層正規化的統計意圖**

層正規化的目的是通過標準化處理，使得每層的激活值符合標準正態分佈，即均值為零，方差為一。這樣的操作可以消除每層激活值的分佈差異，從而避免訓練過程中的內部協變量偏移問題。

內部協變量偏移指的是每層輸入的分佈在訓練過程中隨著參數更新而不斷變化，這會使得模型訓練變得不穩定。層正規化通過標準化每層的輸出，減少了這個問題，使得每層的輸入分佈更加穩定，從而提高訓練過程中的穩定性和收斂速度。

#### **8.3.4 層正規化與批量正規化的區別**

層正規化與批量正規化（Batch Normalization）在本質上都試圖減少內部協變量偏移的影響，但它們的實現方式有所不同。

- **批量正規化**：對每一層進行標準化時，批量正規化是基於一個批次（Batch）的均值和標準差進行的。它的計算方式是對一批次中的所有樣本進行均值和方差的估算，然後對每個樣本進行標準化處理。

  
```math
\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i, \quad \sigma_B = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2}
```


  這裡  $`\mu_B`$  和  $`\sigma_B`$  分別是該批次的均值和標準差， $`m`$  是批次中樣本的數量。

- **層正規化**：與批量正規化不同，層正規化對每一層的激活值進行標準化時，並不依賴於批次中的統計量。每一層的均值和標準差都是基於當前層的輸入樣本進行計算，而不是基於批次中的樣本。

#### **8.3.5 層正規化的優勢**

1. **適用於變動的批次大小**：層正規化不像批量正規化那樣依賴批次的大小，它對每一層進行標準化，這使得它更適合於小批量訓練或者在批次大小不固定的情況下使用。

2. **穩定性**：層正規化對每一層進行正規化處理，避免了批量正規化中可能出現的批次內樣本的分佈不均的問題，從而使得訓練過程更加穩定。

3. **無需計算整體批次的統計量**：層正規化的計算過程是基於單一層進行的，因此它不需要像批量正規化那樣計算整體批次的均值和方差，這使得它的計算更加高效。

#### **8.3.6 結論**

層正規化是一種有效的技術，能夠解決深度神經網絡訓練中的內部協變量偏移問題，並且提高訓練過程的穩定性。通過對每層的激活值進行標準化，並引入學習到的縮放和偏置參數，層正規化能夠促進網絡的高效訓練和加速收斂。