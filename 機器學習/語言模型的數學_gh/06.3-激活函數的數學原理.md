### **6.3 激活函數的數學原理**

在深度學習中，激活函數（Activation Function）是一個非線性函數，用於神經網絡中每一層的神經元，以便引入非線性因素，讓神經網絡能夠學習和表示複雜的模式。激活函數的選擇對神經網絡的性能有重要影響，它決定了每層神經元的輸出，以及對網絡學習過程的影響。

#### **6.3.1 激活函數的作用**

激活函數的主要作用是引入非線性，使得神經網絡可以近似任何複雜的函數映射。沒有激活函數，無論有多少層，神經網絡都會退化為線性模型，因此無法有效地處理複雜的問題。通過激活函數，神經網絡可以學習到輸入和輸出之間的非線性關係，從而提高模型的表現。

#### **6.3.2 常見的激活函數及其數學表達**

1. **Sigmoid 函數**

   Sigmoid 函數是最早使用的激活函數之一，其數學表達式為：
   
   
```math
\sigma(x) = \frac{1}{1 + e^{-x}}
```


   其中  $`x`$  是神經元的輸入。Sigmoid 函數的輸出範圍在  $`(0, 1)`$  之間，常用於二元分類問題中，將輸出值映射到概率範圍內。然而，Sigmoid 函數存在梯度消失問題，尤其在處理深層網絡時，可能會使得梯度非常小，從而減緩訓練過程。

2. **Tanh 函數**

   Tanh（雙曲正切）函數是 Sigmoid 函數的變種，其數學表達式為：
   
   
```math
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1
```


   其中  $`x`$  是神經元的輸入。Tanh 函數的輸出範圍為  $`(-1, 1)`$ ，相較於 Sigmoid 函數，它的輸出範圍更廣，有助於減少梯度消失的問題。然而，Tanh 函數在極端值時仍然會遭遇梯度消失問題。

3. **ReLU（Rectified Linear Unit）函數**

   ReLU 是當前深度學習中最常用的激活函數之一，其數學表達式為：
   
   
```math
\text{ReLU}(x) = \max(0, x)
```


   其中  $`x`$  是神經元的輸入。ReLU 函數將輸入小於零的值映射為 0，對於正數值則直接返回該值。ReLU 函數非常簡單且計算高效，並且能夠有效解決梯度消失問題。然而，ReLU 也存在 "死亡神經元" 問題，這是指某些神經元在訓練過程中一直輸出 0，無法參與學習。

4. **Leaky ReLU 函數**

   Leaky ReLU 是對 ReLU 的改進，旨在解決死亡神經元問題。其數學表達式為：
   
   
```math
\text{Leaky ReLU}(x) = \max(\alpha x, x)
```


   其中  $`\alpha`$  是一個小的常數（例如  $`\alpha = 0.01`$ ）。Leaky ReLU 函數對負數輸入有一個小的斜率，從而避免了完全停用神經元的情況。

5. **ELU（Exponential Linear Unit）函數**

   ELU 是另一種改進版本的 ReLU，其數學表達式為：
   
   
```math
\text{ELU}(x) =
   \begin{cases}
   x & \text{if } x > 0 \\
   \alpha (e^x - 1) & \text{if } x \leq 0
   \end{cases}
```


   其中  $`\alpha`$  是一個常數，通常取  $`\alpha = 1`$ 。ELU 在負數區域呈現指數增長，因此可以避免死神經元問題並保持激活函數的非線性。

6. **Swish 函數**

   Swish 函數是由 Google 提出的新型激活函數，其數學表達式為：
   
   
```math
\text{Swish}(x) = x \cdot \sigma(x)
```


   其中  $`\sigma(x)`$  是 Sigmoid 函數。Swish 函數具有平滑的非線性形狀，能夠在訓練過程中提供更好的梯度流，從而提高模型的學習效率。

7. **Softmax 函數**

   Softmax 函數常用於多類別分類問題，其數學表達式為：
   
   
```math
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
```


   其中  $`x_i`$  是第  $`i`$  類的輸入， $`\sum_{j} e^{x_j}`$  是所有類別的指數和。Softmax 函數將每個類別的預測值映射到  $`(0, 1)`$  之間，並且保證所有類別的輸出總和為 1，從而可以解釋為概率分佈。

#### **6.3.3 激活函數的選擇與實踐**

在實際應用中，選擇合適的激活函數對於模型的性能至關重要。一般來說，ReLU 是大多數深度學習模型的首選激活函數，因為它計算簡單且能夠有效減少梯度消失問題。對於更複雜的問題或特殊情況（如對負數輸入有更大需求的情況），可以考慮使用 Leaky ReLU 或 ELU。

在多類別分類任務中，Softmax 函數通常用於輸出層，以生成各類別的預測概率。對於二元分類問題，Sigmoid 函數在輸出層也很常見。

#### **6.3.4 激活函數的數學性質**

1. **連續性**：激活函數需要是連續的，這樣可以確保網絡的輸出隨輸入變化平滑，且不會出現不必要的跳躍或不連續。
   
2. **可微性**：激活函數通常要求是可微的，這樣在訓練過程中可以通過反向傳播計算梯度，進行權重更新。

3. **非線性**：激活函數需要具有非線性特徵，這樣神經網絡才能學習和擬合複雜的非線性關係。

#### **6.3.5 小結**

激活函數在神經網絡中扮演著至關重要的角色，能夠引入非線性，讓模型學習到複雜的數據模式。不同的激活函數有不同的數學特性和應用場景，選擇合適的激活函數對於深度學習模型的訓練和性能提升至關重要。ReLU 和其變體（如 Leaky ReLU 和 ELU）是最常用的選擇，而對於多類別分類，Softmax 是理想的選擇。