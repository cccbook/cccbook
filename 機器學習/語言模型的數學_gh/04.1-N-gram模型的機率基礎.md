### **第 4 章：統計語言模型**

#### **4.1 N-gram模型的機率基礎**

N-gram模型是最早且最簡單的統計語言模型之一，廣泛應用於語言處理領域，尤其在語音識別、機器翻譯、文本生成等任務中具有重要地位。N-gram模型的基本思想是基於馬可夫假設，假設當前詞語的出現只依賴於它前面固定長度的詞語（即N-1個詞語）。這種模型透過對語言中詞語序列的統計來捕捉詞語之間的依賴關係，並通過估算條件概率來進行預測。

#### **4.1.1 N-gram模型的定義**

給定一個詞語序列  $`w_1, w_2, \dots, w_T`$ ，N-gram模型關心的是條件概率  $`P(w_t | w_1, w_2, \dots, w_{t-1})`$ ，即給定前面一組詞語，預測當前詞語  $`w_t`$  的概率。對於N-gram模型來說，這個條件概率的估算依賴於「馬可夫假設」，即假設當前詞語  $`w_t`$  只依賴於前  $`N-1`$  個詞語  $`w_{t-1}, w_{t-2}, \dots, w_{t-N+1}`$ 。因此，N-gram模型的條件概率可以寫作：


```math
P(w_t | w_1, w_2, \dots, w_{t-1}) \approx P(w_t | w_{t-1}, w_{t-2}, \dots, w_{t-N+1})
```


這樣的假設使得語言模型可以根據前  $`N-1`$  個詞語來進行簡單的概率計算，從而避免處理長期依賴關係，將問題簡化為固定大小的窗口。

#### **4.1.2 N-gram的概率估算**

在N-gram模型中，條件概率  $`P(w_t | w_{t-1}, w_{t-2}, \dots, w_{t-N+1})`$  的估算方法通常是基於語料庫中的頻率分佈。具體地，給定一個訓練語料庫  $`C`$ ，我們可以通過計算詞語序列出現的頻次來估算這些概率。對於一個給定的  $`N`$ -gram，條件概率可以表示為：


```math
P(w_t | w_{t-1}, w_{t-2}, \dots, w_{t-N+1}) = \frac{\text{Count}(w_{t-N+1}, \dots, w_t)}{\text{Count}(w_{t-N+1}, \dots, w_{t-1})}
```


其中， $`\text{Count}(w_{t-N+1}, \dots, w_t)`$  表示詞語序列  $`w_{t-N+1}, \dots, w_t`$  在語料庫中的出現頻次，而  $`\text{Count}(w_{t-N+1}, \dots, w_{t-1})`$  是前  $`N-1`$  個詞語出現的頻次。

#### **4.1.3 N-gram模型的類型**

根據N的大小，N-gram模型可以分為以下幾種類型：

- **Unigram模型**：當N=1時，N-gram模型簡化為Unigram模型，它只考慮每個詞語的出現概率，並假設各詞語之間是獨立的。對於Unigram模型，條件概率計算式為：
  
```math
P(w_t) = \frac{\text{Count}(w_t)}{\text{Total Count of words in the corpus}}
```

  
- **Bigram模型**：當N=2時，N-gram模型就是Bigram模型，它考慮前一個詞語對當前詞語的影響。條件概率為：
  
```math
P(w_t | w_{t-1}) = \frac{\text{Count}(w_{t-1}, w_t)}{\text{Count}(w_{t-1})}
```


- **Trigram模型**：當N=3時，模型進一步考慮前兩個詞語對當前詞語的影響。條件概率為：
  
```math
P(w_t | w_{t-1}, w_{t-2}) = \frac{\text{Count}(w_{t-2}, w_{t-1}, w_t)}{\text{Count}(w_{t-2}, w_{t-1})}
```


#### **4.1.4 N-gram模型的優點與挑戰**

**優點**：
1. **簡單直觀**：N-gram模型依賴於統計頻次來計算條件概率，因此易於實現且計算直觀。
2. **高效訓練**：由於N-gram模型的訓練過程只是計算詞語序列的頻次，訓練過程通常相對高效。
3. **計算複雜度低**：與其他更為複雜的語言模型相比，N-gram模型的計算複雜度較低，適用於資源有限的情況。

**挑戰**：
1. **長期依賴無法捕捉**：N-gram模型僅依賴於有限長度的上下文，無法捕捉到語言中長期的依賴關係。
2. **資料稀疏問題**：隨著N值的增大，訓練資料中出現某些N-gram的頻次可能會變得非常低，甚至為零，從而造成概率估算的不準確，這稱為資料稀疏問題。
3. **模型大小與存儲需求**：隨著N值的增大，模型需要記錄更多的N-gram條目，這會增加計算和存儲需求。

#### **4.1.5 資料平滑技術**

為了應對資料稀疏問題，通常會使用**平滑**技術來修正那些出現頻次為零的N-gram。常見的平滑技術包括：

- **Additive Smoothing（加法平滑）**：為所有可能的N-gram賦予一個小的常數，避免零頻次的問題。常見的加法平滑方法是**拉普拉斯平滑**（Laplace Smoothing），其公式為：
  
```math
P(w_t | w_{t-1}, w_{t-2}, \dots, w_{t-N+1}) = \frac{\text{Count}(w_{t-N+1}, \dots, w_t) + \alpha}{\text{Count}(w_{t-N+1}, \dots, w_{t-1}) + \alpha V}
```

  其中  $`\alpha`$  是平滑參數， $`V`$  是詞彙表大小。

- **Kneser-Ney Smoothing**：這是一種更為高效的平滑方法，針對低頻詞語給予更高的概率。

#### **4.1.6 小結**

N-gram模型是語言建模中的基礎方法，它通過簡單地假設當前詞語僅依賴於前  $`N-1`$  個詞語，從而簡化了條件概率的計算。雖然N-gram模型簡單直觀且訓練高效，但由於長期依賴問題、資料稀疏問題及模型大小的增長，這使得它在處理複雜語言模式時面臨挑戰。平滑技術的引入有效地緩解了資料稀疏問題，然而隨著語言建模需求的增加，更高效且複雜的模型（如神經網絡語言模型）逐漸取代了N-gram模型的地位。