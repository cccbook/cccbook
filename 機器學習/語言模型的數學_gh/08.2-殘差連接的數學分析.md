### **8.2 殘差連接的數學分析**

在深度學習模型中，**殘差連接**（Residual Connection）是一種用於緩解梯度消失或梯度爆炸問題的技巧，尤其在深層網絡結構中十分重要。Transformer 架構中的每一層，包括自注意力層和前饋網路層，都採用了殘差連接。這種連接方式允許原始輸入信息在層間直接傳遞，從而促進了網絡的學習過程並加速了收斂。

#### **8.2.1 殘差連接的基本結構**

在每一層的計算過程中，輸入  $`X`$  與經過變換的輸出  $`z`$  進行元素級的加法，得到最終的結果：


```math
Y = X + z
```


這裡：
-  $`X`$  是當前層的輸入向量（來自前一層或自注意力層的輸出），
-  $`z`$  是通過計算後得到的變換向量（可以是自注意力層的輸出或前饋網路的輸出），
-  $`Y`$  是最終的輸出。

這種結構的目的是使信息能夠直接從輸入流向輸出，減少深層網絡中可能出現的梯度問題。

#### **8.2.2 殘差連接的數學原理**

我們可以從數學的角度理解殘差連接，特別是在多層神經網絡中的應用。假設我們有一個深度網絡，每一層的計算過程如下：


```math
h_l = f(h_{l-1}; \theta_l)
```


其中  $`h_l`$  是第  $`l`$  層的輸出， $`h_{l-1}`$  是第  $`l-1`$  層的輸入， $`f(\cdot)`$  是該層的非線性映射， $`\theta_l`$  是該層的權重。

在引入殘差連接後，每一層的計算公式會變為：


```math
h_l = f(h_{l-1}; \theta_l) + h_{l-1}
```


這裡的加法操作代表了殘差連接。這樣做的目的有以下幾個數學解釋：

1. **避免信息丟失**：直接將前一層的輸入  $`h_{l-1}`$  加入到當前層的輸出中，能保證即使在深層網絡中，初始信息依然能夠傳遞並影響最終的結果。
   
2. **緩解梯度消失問題**：在傳遞梯度時，殘差連接使得反向傳播的梯度可以繞過某些層，直接從網絡的深層層級流向較淺層的層級，這有助於緩解梯度消失問題，尤其是在深層神經網絡中。

3. **加速訓練過程**：由於殘差連接使得學習過程中的信息流動更加順暢，這不僅提高了訓練的穩定性，還能加速模型的收斂。

#### **8.2.3 殘差連接與層正規化的結合**

在 Transformer 架構中，殘差連接通常與**層正規化**（Layer Normalization）結合使用。這樣做的目的是避免在加入殘差時引入不穩定性，並且對層的輸出進行標準化，以確保每層的輸出有相似的分佈。

具體來說，在經過殘差連接後，進行層正規化的步驟如下：


```math
\hat{Y} = \text{LayerNorm}(X + z)
```


這裡  $`X`$  是當前層的輸入， $`z`$  是經過自注意力或前饋網路變換後的結果， $`\hat{Y}`$  是經過正規化處理的最終輸出。

層正規化的目的是將每層的輸出進行均值為零、方差為一的標準化，從而使得模型在訓練過程中更加穩定。

#### **8.2.4 數學分析的延伸**

假設我們對一個簡單的多層感知機（MLP）進行數學分析，並比較傳統的層和引入殘差連接後的層。

- **傳統層的計算**：

  傳統的神經網絡層的輸出計算為：

  
```math
Y = f(X W + b)
```


  其中  $`f(\cdot)`$  是激活函數， $`W`$  和  $`b`$  是該層的權重和偏置。

- **引入殘差連接後的層**：

  引入殘差連接後，層的計算公式變為：

  
```math
Y = f(X W + b) + X
```


  在這裡， $`X`$  被直接加到激活函數的結果中，這樣做可以讓原始輸入直接影響最終的輸出，從而減少梯度的損失。

#### **8.2.5 小結**

殘差連接是 Transformer 和深度學習模型中的一個關鍵組件，它能夠幫助緩解梯度消失問題，促進信息的流動，並加速訓練過程。通過數學分析，我們可以看到殘差連接不僅使得每層的計算過程更加穩定，還能夠讓模型在訓練時更有效地利用每一層的輸入信息。