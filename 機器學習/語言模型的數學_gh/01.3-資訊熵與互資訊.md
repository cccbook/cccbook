### **1.3 資訊熵，互資訊與 Cross Entropy**

在信息論中，**資訊熵**、**互資訊**以及**交叉熵**（Cross Entropy）是三個關鍵概念，它們在語言模型中有著深刻的應用。理解這些概念對於設計和分析語言模型至關重要。

### **1.3.1 資訊熵（Entropy）**

**資訊熵**是衡量隨機變數不確定性或信息量的度量。對於一個隨機變數  $`X`$  其可能取值為  $`\{ x_1, x_2, \dots, x_n \}`$ ，且其機率質量函數為  $`P(X = x_i) = p_i`$ ，資訊熵  $`H(X)`$  定義為：


```math
H(X) = - \sum_{i=1}^{n} p_i \log p_i
```


這裡的  $`p_i`$  是  $`x_i`$  的概率。熵的單位通常是比特（bit），如果對數是以 2 為底計算的。熵越大，說明隨機變數的分佈越不確定，反之則越確定。

在語言模型中，熵通常用來衡量模型對下一個詞語的預測不確定性。較低的熵意味著模型對預測的詞語有較高的確定性，而較高的熵則表示預測的詞語多樣性較大，模型的確定性較低。

### **1.3.2 互資訊（Mutual Information）**

**互資訊**度量了兩個隨機變數之間共享的信息量。具體來說，它表示其中一個變數提供的資訊能減少對另一個變數不確定性的程度。對於兩個隨機變數  $`X`$  和  $`Y`$ ，其互資訊  $`I(X; Y)`$  定義為：


```math
I(X; Y) = H(X) + H(Y) - H(X, Y)
```


或者等價地：


```math
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}
```


其中， $`p(x, y)`$  是  $`X`$  和  $`Y`$  的聯合機率， $`p(x)`$  和  $`p(y)`$  是它們的邊際機率。

互資訊衡量的是兩個變數之間的依賴關係。在語言模型中，這通常用來衡量上下文（例如前文的單詞）與當前預測詞語之間的相關性。例如，給定句子的前一部分，模型會根據這些信息來預測下一個詞語，此時前後詞語之間的互資訊越高，預測越精確。

### **1.3.3 交叉熵（Cross Entropy）**

**交叉熵**是衡量兩個機率分佈之間差異的一種度量。假設我們有兩個機率分佈  $`P`$  和  $`Q`$ ，其中  $`P`$  是真實分佈， $`Q`$  是模型預測的分佈，則交叉熵  $`H(P, Q)`$  定義為：


```math
H(P, Q) = - \sum_{i=1}^{n} p_i \log q_i
```


其中  $`p_i`$  是真實分佈  $`P`$  中事件  $`i`$  的概率， $`q_i`$  是模型預測分佈  $`Q`$  中事件  $`i`$  的概率。交叉熵實際上是一種**加權的熵**，它考慮了真實分佈與預測分佈之間的差異。

交叉熵常用於機器學習中的損失函數，特別是在分類問題中。對於語言模型，交叉熵可以用來衡量模型的預測與真實語言分佈之間的差距。具體來說，在語言生成或預測中，若模型預測的機率分佈與真實的分佈（如某個詞的真實機率）接近，則交叉熵較小，表示模型的預測較準確；反之，交叉熵較大，則表示預測較差。

### **1.3.4 交叉熵與資訊熵的關聯**

交叉熵和資訊熵有密切的關聯。當我們計算交叉熵時，如果我們的預測分佈  $`Q`$  完全符合真實分佈  $`P`$ ，則交叉熵就等於真實分佈的資訊熵：


```math
H(P, P) = H(P)
```


這表示當模型的預測分佈與真實分佈一致時，交叉熵達到最小值，且該最小值正好是資訊熵，表示該模型達到了最佳的預測效果。

然而，如果預測分佈  $`Q`$  與真實分佈  $`P`$  不一致，交叉熵會大於資訊熵。交叉熵越大，說明模型的預測與真實分佈的差異越大。

### **1.3.5 交叉熵在語言模型中的應用**

在語言模型的訓練中，交叉熵是常用的損失函數。給定一個語言模型，它的目標是根據上下文  $`w_1, w_2, \dots, w_{n-1}`$  預測下一個單詞  $`w_n`$ 。我們希望模型學會使得其預測的機率分佈  $`P(w_n | w_1, w_2, \dots, w_{n-1})`$  與真實的機率分佈  $`P_{\text{true}}(w_n)`$  越接近越好。這時，交叉熵可以作為損失函數來指導模型的訓練，計算預測分佈和真實分佈之間的差距。

對於語言生成模型，交叉熵的最小化意味著模型能夠更準確地預測給定上下文的下一個單詞，從而生成更加合理和流暢的語言。

### **1.3.6 互資訊與交叉熵的區別**

互資訊和交叉熵都涉及到信息的度量，但其焦點有所不同：

- **資訊熵**是單一隨機變數的固有不確定性的度量。
- **交叉熵**是用來衡量兩個分佈之間的差異，通常用於衡量模型預測與真實分佈之間的差距。
- **互資訊**則是用來衡量兩個隨機變數之間共享的信息量，通常用來描述上下文與當前詞語之間的關聯。

總結來說，這些概念不僅是理論分析語言模型的重要工具，也在語言模型的訓練過程中發揮了核心作用，幫助提升模型的預測準確性和生成能力。