### **7.4 注意力分數計算**

在自注意力機制中，核心思想是根據輸入序列中的各個元素之間的關聯來計算每個元素的重要性（即注意力分數）。這些注意力分數被用來加權輸入序列的表示，從而生成最終的輸出表示。該過程的數學基礎在於計算 Query（查詢向量）和 Key（鍵向量）之間的相似度，並根據這些相似度生成權重。

#### **7.4.1 注意力分數的計算方式**

在 Transformer 中，每個輸入元素會被映射到三個向量：Query ( $`Q`$ )，Key ( $`K`$ ) 和 Value ( $`V`$ )。這些向量是通過學習的權重矩陣將原始輸入表示投影到新的空間得到的。

對於一個給定的查詢向量  $`q_i`$  和鍵向量  $`k_j`$ ，其注意力分數通常通過計算它們的相似度來確定，最常見的計算方法是使用內積：

1. **內積注意力（Dot-Product Attention）**：
   
```math
\text{score}(q_i, k_j) = \frac{q_i^T k_j}{\sqrt{d_k}}
```

   其中：
   -  $`q_i^T k_j`$  是查詢向量  $`q_i`$  和鍵向量  $`k_j`$  的內積。
   -  $`d_k`$  是鍵向量的維度，用來進行縮放，防止內積的數值過大，從而使梯度更新過快。

2. **注意力分數的縮放（Scaling）**：
   通常，為了避免內積的結果過大，會對其進行縮放處理。內積的結果會除以  $`\sqrt{d_k}`$ ，其中  $`d_k`$  是鍵向量的維度。這樣做的目的是保持分數的數值範圍合理，避免計算過程中的梯度消失或爆炸現象。

#### **7.4.2 注意力分數的正規化**

計算出的注意力分數需要經過正規化處理，通常使用 **softmax** 函數來將分數轉換為概率分佈。這樣可以保證每個元素的權重之和為 1，使得模型可以根據這些權重來加權輸入元素。


```math
\alpha_{ij} = \text{softmax}\left( \frac{q_i^T k_j}{\sqrt{d_k}} \right) = \frac{ \exp\left( \frac{q_i^T k_j}{\sqrt{d_k}} \right)}{\sum_{j=1}^n \exp\left( \frac{q_i^T k_j}{\sqrt{d_k}} \right)}
```

其中：
-  $`\alpha_{ij}`$  是位置  $`i`$  和位置  $`j`$  之間的注意力權重。
- softmax 函數將每個注意力分數映射為一個介於 0 和 1 之間的值，並且使所有權重的總和為 1。

#### **7.4.3 計算加權和（加權求和）**

一旦獲得了注意力權重  $`\alpha_{ij}`$ ，就可以利用這些權重來加權值向量  $`V_j`$ ，從而生成每個查詢向量  $`q_i`$  的加權和表示。這樣，模型會根據每個查詢向量與其他元素之間的相似度來加權其他元素的值，進而更新該查詢向量的表示。

加權和的計算公式為：

```math
\text{Attention Output}_i = \sum_{j=1}^{n} \alpha_{ij} V_j
```

其中：
-  $`\text{Attention Output}_i`$  是位置  $`i`$  的輸出表示，這是經過加權的值向量的總和。

#### **7.4.4 多頭注意力中的注意力分數**

在多頭注意力（Multi-Head Attention）中，注意力機制會將查詢、鍵和值分別映射到多個子空間，然後並行計算多組注意力分數。最終，這些不同的注意力結果會被拼接在一起，並通過線性變換獲得最終的輸出。

1. **多頭注意力的過程**：
   假設有  $`h`$  個頭，每個頭會計算獨立的注意力分數：
   
```math
\text{Attention Output}_i^{(head)} = \sum_{j=1}^{n} \alpha_{ij}^{(head)} V_j
```

   其中  $`\alpha_{ij}^{(head)}`$  是第  $`head`$  頭的注意力權重。

2. **拼接與線性變換**：
   最後，所有頭的注意力輸出會被拼接在一起，並通過一個線性層進行變換：
   
```math
\text{Final Output} = W_O \cdot \text{Concat}(\text{Attention Output}_1, \text{Attention Output}_2, \dots, \text{Attention Output}_h)
```

   其中  $`W_O`$  是一個可學習的權重矩陣， $`\text{Concat}(\cdot)`$  表示將各頭的輸出拼接起來。

#### **7.4.5 小結**

注意力分數的計算是自注意力機制中的關鍵步驟，它基於查詢和鍵之間的相似度來生成權重，並用這些權重加權值向量。通過這一過程，模型能夠根據輸入序列中的不同位置的重要性來更新每個元素的表示。在多頭注意力中，這一過程會並行進行多次計算，並最終通過拼接和線性變換來獲得最終的輸出。