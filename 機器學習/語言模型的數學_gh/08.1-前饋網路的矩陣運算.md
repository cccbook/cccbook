### **第 8 章：Transformer架構的數學**

#### **8.1 前饋網路的矩陣運算**

在 Transformer 架構中，前饋網路（Feed-Forward Network，FFN）是每一層的重要組成部分，它用於進行非線性轉換，並對每個位置的輸入進行逐位置的處理。這個前饋網路位於自注意力層之後，並且對每個位置的輸入進行相同的計算，這使得模型具有很強的並行性。前饋網路的結構通常由兩層全連接（fully connected）層組成，中間加入非線性激活函數，常見的激活函數是 ReLU。

#### **8.1.1 前饋網路的基本結構**

Transformer 的前饋網路通常包括以下幾個步驟：

1. **線性變換**：首先，對每個位置的輸入進行線性變換，即通過一個權重矩陣  $`W_1 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}`$  進行映射，其中  $`d_{\text{model}}`$  是輸入和輸出向量的維度， $`d_{\text{ff}}`$  是前饋層的隱藏層維度。對應的計算可以表示為：

   
```math
z_1 = X W_1 + b_1
```

   其中  $`X \in \mathbb{R}^{n \times d_{\text{model}}}`$  是來自上一層的輸入矩陣， $`b_1 \in \mathbb{R}^{d_{\text{ff}}}`$  是偏置項， $`z_1 \in \mathbb{R}^{n \times d_{\text{ff}}}`$  是線性變換的結果， $`n`$  是序列長度。

2. **激活函數**：接下來，對每個位置的向量應用非線性激活函數，通常使用 ReLU 函數。ReLU 函數的數學表達式為：

   
```math
\text{ReLU}(x) = \max(0, x)
```


   因此，經過激活函數後的結果為：

   
```math
a = \text{ReLU}(z_1) = \text{ReLU}(X W_1 + b_1)
```


3. **第二次線性變換**：在激活函數之後，對每個位置的向量再次進行線性變換，這一次使用一個新的權重矩陣  $`W_2 \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}`$ ，並添加偏置項  $`b_2 \in \mathbb{R}^{d_{\text{model}}}`$ 。此步驟的計算可以表示為：

   
```math
z_2 = a W_2 + b_2
```


   這裡  $`z_2 \in \mathbb{R}^{n \times d_{\text{model}}}`$  是最終的輸出，這個結果將進入後續的層。

4. **正規化與殘差連接**：為了提高訓練的穩定性，Transformer 中的前饋網路通常會與自注意力層的輸出進行殘差連接，並且進行層正規化。這意味著最終的輸出是以下兩個部分的和：

   
```math
\text{FFN Output} = \text{LayerNorm}(X + z_2)
```


   其中  $`X`$  是來自自注意力層的輸入， $`z_2`$  是經過兩層線性變換和激活函數處理後的結果。

#### **8.1.2 矩陣運算的詳細分析**

1. **線性變換**：

   在前饋網路的每一層，我們對每個位置的向量進行線性變換。首先，對於每個序列位置  $`x_i`$ ，這個向量會與一個權重矩陣  $`W_1`$  相乘。假設  $`x_i \in \mathbb{R}^{d_{\text{model}}}`$ ，則這個線性變換的結果  $`z_{i1}`$  是：

   
```math
z_{i1} = x_i W_1 + b_1
```


   這裡  $`x_i`$  是  $`i`$ -th 序列位置的向量， $`W_1`$  是變換矩陣， $`b_1`$  是偏置項。

2. **激活函數**：

   然後，我們對每個向量  $`z_{i1}`$  應用非線性激活函數  $`ReLU`$ ，得到激活後的向量  $`a_i`$ ：

   
```math
a_i = \text{ReLU}(z_{i1}) = \max(0, z_{i1})
```


   這個步驟可以對每個位置的向量進行非線性變換，捕捉更複雜的特徵。

3. **第二次線性變換**：

   接著，激活後的向量  $`a_i`$  會與第二個權重矩陣  $`W_2`$  相乘，並且再加上一個偏置項  $`b_2`$ ，得到最終的前饋網路輸出：

   
```math
z_{i2} = a_i W_2 + b_2
```


   這樣，每個位置的向量都經過了兩次線性變換和一次激活。

4. **批量正規化與殘差連接**：

   最後，前饋網路的輸出會與自注意力層的輸入進行殘差連接，並進行批量正規化。這樣，前饋網路的最終輸出可以表示為：

   
```math
\text{FFN Output} = \text{LayerNorm}(X + z_2)
```


#### **8.1.3 小結**

前饋網路在 Transformer 架構中扮演著至關重要的角色，它通過兩層線性變換和激活函數進行非線性轉換，並使得每個位置的表示得到強化。這一過程涉及矩陣運算、激活函數應用以及層正規化，並且通過與自注意力層的殘差連接來增強模型的表示能力。前饋網路的並行性使得 Transformer 架構在處理長序列時具有更高的效率和更強的表現能力。