### **5.2 詞嵌入的數學原理**

在自然語言處理（NLP）中，詞嵌入（Word Embedding）將語言中的單詞映射到一個低維的向量空間，使得語義上相似的單詞具有相近的向量表示。這一過程背後的數學原理依賴於機器學習中的優化方法與概率模型。在本節中，我們將深入探討兩種最著名的詞嵌入模型：Word2Vec 和 GloVe，並分析它們的數學推導與目標函數。

#### **5.2.1 Word2Vec數學推導**

Word2Vec 是由 Mikolov 等人於 2013 年提出的一種詞嵌入模型，通過訓練語料庫中的上下文來學習詞語的向量表示。Word2Vec 有兩種主要的架構：**CBOW（Continuous Bag of Words）** 和 **Skip-gram**。下面我們將主要集中於 Skip-gram 模型的數學推導，因為它在大多數情況下表現更好。

**Skip-gram 模型：**

Skip-gram 的目標是給定一個詞  $`w_t`$ ，預測它的上下文詞語  $`w_{t-k}, w_{t-k+1}, \dots, w_{t+k}`$ （其中  $`k`$  是上下文窗口的大小）。假設語料庫中有  $`N`$  個詞，每個詞  $`w`$  被映射到一個向量  $`\mathbf{v}_w \in \mathbb{R}^d`$ ，其中  $`d`$  是嵌入空間的維度。

Word2Vec 的目標是最大化上下文詞語給定目標詞語的條件概率。對於詞  $`w_t`$ ，其條件概率可以表示為：


```math
P(w_{t+j} | w_t) = \frac{\exp(\mathbf{v}_{w_{t+j}}^\top \mathbf{v}_w)}{\sum_{w'} \exp(\mathbf{v}_{w'}^\top \mathbf{v}_w)}
```


其中， $`\mathbf{v}_w`$  是目標詞語  $`w`$  的向量， $`\mathbf{v}_{w_{t+j}}`$  是上下文詞語  $`w_{t+j}`$  的向量，分母是所有詞的指數內積的總和（即對所有詞的概率進行歸一化）。

**模型目標：**  
對於每一對  $`w_t`$  和  $`w_{t+j}`$ ，Word2Vec 的目標是最大化其對數似然函數，這可以表示為：


```math
L = \sum_{t=1}^{T} \sum_{j=-k, j\neq 0}^{k} \log P(w_{t+j} | w_t)
```


對於每一對上下文詞語和目標詞語，Word2Vec 通過最大化這個對數似然函數來學習詞向量  $`\mathbf{v}_w`$ 。這樣，訓練過程實際上是在高維空間中尋找最佳的詞向量，使得在給定一個詞的情況下，它的上下文詞語概率最大。

**負采樣：**  
由於計算上述分母（即對所有詞的指數內積求和）是非常昂貴的，Word2Vec 引入了**負采樣**（Negative Sampling）技術，通過對少量“負樣本”（即不出現於上下文中的詞語）進行訓練來近似這一概率分布。這樣，訓練過程變得更加高效。

#### **5.2.2 GloVe目標函數分析**

GloVe（Global Vectors for Word Representation）是另一種詞嵌入模型，它由 Pennington 等人於 2014 年提出。與 Word2Vec 依賴於局部上下文窗口進行訓練不同，GloVe 則依賴於詞語在整體語料庫中出現的統計信息。GloVe 的主要思想是基於詞對共現頻率來學習詞向量。

**GloVe 目標函數：**

GloVe 通過對詞語對的共現概率進行建模，來學習每個詞語的向量表示。假設詞  $`w_i`$  和詞  $`w_j`$  在語料庫中的共現頻率為  $`X_{ij}`$ ，GloVe 假設詞  $`w_i`$  和  $`w_j`$  的共現頻率與它們的詞向量內積有關，即：


```math
X_{ij} \approx \frac{\exp(\mathbf{v}_i^\top \mathbf{v}_j)}{Z_{ij}}
```


其中， $`\mathbf{v}_i`$  和  $`\mathbf{v}_j`$  是詞  $`w_i`$  和  $`w_j`$  的詞向量， $`Z_{ij}`$  是一個歸一化常數，用於平衡數據中的稀疏性。

**GloVe 的目標函數：**

GloVe 旨在最小化以下損失函數：


```math
J = \sum_{i,j=1}^{V} f(X_{ij}) \left( \mathbf{v}_i^\top \mathbf{v}_j + b_i + b_j - \log X_{ij} \right)^2
```


其中， $`f(X_{ij})`$  是一個平滑函數，用於減少頻繁共現詞對對模型的過度影響。常見的選擇是  $`f(X_{ij}) = \left( \frac{X_{ij}}{X_{\text{max}}} \right)^\alpha`$ ，其中  $`X_{\text{max}}`$  是最大的共現頻率， $`\alpha`$  是一個調節超參數。

目標函數的第一項是詞向量的內積與共現頻率之間的誤差，第二項則是詞偏置項。GloVe 訓練的核心就是調整這些詞向量和偏置項，使得模型能夠有效地重建詞對的共現概率。

**數學解釋：**

GloVe 的設計使得詞向量在捕捉語言的全局統計信息方面表現出色。相較於 Word2Vec，GloVe 不僅關注局部上下文信息，還強調詞語對之間的全局共現結構。這使得 GloVe 在某些情況下可以學到更加豐富和高效的詞向量表示。

#### **5.2.3 小結**

在本節中，我們介紹了兩種詞嵌入模型的數學原理：Word2Vec 和 GloVe。Word2Vec 主要通過最大化條件概率來學習詞向量，而 GloVe 則是基於全局共現統計來進行訓練。這兩種方法在實際應用中各有優勢，並且已經成為現代自然語言處理中的重要技術。