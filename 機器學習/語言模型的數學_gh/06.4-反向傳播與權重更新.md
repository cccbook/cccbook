### **6.4 反向傳播與權重更新**

反向傳播（Backpropagation）是深度學習中訓練神經網絡的核心算法，通過這個算法，神經網絡能夠根據訓練數據不斷更新權重，最終使模型預測結果與真實標籤之間的誤差最小化。反向傳播利用梯度下降法進行優化，其基本思想是通過計算損失函數相對於各層權重的梯度，將誤差從輸出層反向傳遞至輸入層，進而更新每一層的權重和偏置。

#### **6.4.1 反向傳播的基本原理**

反向傳播的目的是計算損失函數對每一層權重的偏導數，然後根據這些偏導數來調整權重，以最小化損失函數。反向傳播包含以下步驟：

1. **前向傳播（Forward Propagation）**：
   - 將訓練數據輸入神經網絡，計算每層的輸出，直到最後一層的預測結果。
   - 根據輸出層的預測結果與真實標籤，計算損失函數。

2. **反向傳播（Backward Propagation）**：
   - 從輸出層開始，計算損失函數對每個權重的梯度。這需要應用鏈式法則來進行逐層的反向傳播。
   - 使用鏈式法則計算各層的梯度，並逐層將梯度反向傳遞到前一層。

3. **權重更新（Weight Update）**：
   - 利用計算出的梯度，通過梯度下降法或其變體（如隨機梯度下降、動量法、Adam 等）來更新每一層的權重和偏置，這樣模型就能夠朝著最小化損失函數的方向學習。

#### **6.4.2 反向傳播的數學推導**

假設神經網絡有  $`L`$  層，其中第  $`l`$  層的輸入為  $`\mathbf{x}^{(l)}`$ ，輸出為  $`\mathbf{y}^{(l)}`$ ，並且  $`\mathbf{W}^{(l)}`$  和  $`\mathbf{b}^{(l)}`$  分別是第  $`l`$  層的權重矩陣和偏置向量。

1. **前向傳播的數學表示**

   在每一層中，對於第  $`l`$  層，輸出是由下式給出的：

   
```math
\mathbf{y}^{(l)} = f^{(l)}(\mathbf{W}^{(l)} \mathbf{x}^{(l-1)} + \mathbf{b}^{(l)})
```


   其中  $`f^{(l)}`$  是激活函數， $`\mathbf{x}^{(l-1)}`$  是前一層的輸出。最終的輸出層輸出為  $`\mathbf{y}^{(L)}`$ ，對應的損失函數  $`L`$  通常選擇交叉熵（cross-entropy）或者均方誤差（mean squared error）等。

2. **反向傳播的數學推導**

   為了更新權重，我們需要計算每層權重的梯度。假設我們已經計算出了損失函數對最終層輸出  $`\mathbf{y}^{(L)}`$  的偏導數  $`\frac{\partial L}{\partial \mathbf{y}^{(L)}}`$ ，接下來通過鏈式法則反向傳遞梯度。

   - **輸出層梯度：** 對於輸出層，權重的梯度是損失函數對輸出層的梯度與輸入層的梯度之積：
     
     
```math
\frac{\partial L}{\partial \mathbf{W}^{(L)}} = \frac{\partial L}{\partial \mathbf{y}^{(L)}} \cdot \frac{\partial \mathbf{y}^{(L)}}{\partial \mathbf{W}^{(L)}}
```

     
     其中  $`\frac{\partial \mathbf{y}^{(L)}}{\partial \mathbf{W}^{(L)}} = \mathbf{x}^{(L-1)}`$ ，所以有：
     
     
```math
\frac{\partial L}{\partial \mathbf{W}^{(L)}} = \frac{\partial L}{\partial \mathbf{y}^{(L)}} \cdot \mathbf{x}^{(L-1)}
```


   - **隱藏層梯度：** 對於隱藏層，我們需要根據前一層的梯度計算該層的梯度。對於第  $`l`$  層，梯度計算如下：
     
     
```math
\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \left( \frac{\partial L}{\partial \mathbf{y}^{(l+1)}} \cdot \frac{\partial \mathbf{y}^{(l+1)}}{\partial \mathbf{x}^{(l)}} \right) \cdot \frac{\partial \mathbf{x}^{(l)}}{\partial \mathbf{W}^{(l)}}
```


   這一過程將重複直到輸入層，最終得到每一層權重的梯度。

3. **權重更新規則**

   計算出每一層的梯度之後，使用梯度下降法更新權重。對於每一層，權重的更新規則為：
   
   
```math
\mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \eta \cdot \frac{\partial L}{\partial \mathbf{W}^{(l)}}
```


   其中  $`\eta`$  是學習率，表示每次更新的步長。類似地，偏置項也可以根據以下規則更新：

   
```math
\mathbf{b}^{(l)} \leftarrow \mathbf{b}^{(l)} - \eta \cdot \frac{\partial L}{\partial \mathbf{b}^{(l)}}
```


#### **6.4.3 梯度下降法與變體**

梯度下降法（Gradient Descent）是反向傳播中的常見權重更新方法，但在實際應用中，為了加速訓練過程和避免陷入局部最小值，通常會使用一些梯度下降的變體，如：

1. **隨機梯度下降（SGD）**：每次更新只使用一個樣本，計算速度快，但有噪聲，容易跳出局部最小值。

2. **小批量梯度下降（Mini-batch SGD）**：每次使用一部分訓練數據（小批量）來計算梯度，折衷了計算速度和穩定性。

3. **動量法（Momentum）**：在梯度更新中引入前一次更新的影響，使得權重更新更平滑，並加速收斂。

4. **Adam（Adaptive Moment Estimation）**：結合了動量法和自適應學習率調整，使得每個參數有自適應的學習率，並且對梯度的估計更為準確。

#### **6.4.4 反向傳播的實際應用與挑戰**

反向傳播算法在實際應用中有很多挑戰，主要包括：

- **梯度消失與梯度爆炸**：在深層網絡中，梯度可能會隨著層數的增長而變得非常小或非常大，這會影響訓練的效率。為了解決這個問題，通常會使用更合適的激活函數（如 ReLU）以及進行適當的權重初始化。
  
- **學習率選擇**：學習率過大可能導致收斂過快或發散，過小則會使得訓練過程過慢。因此，選擇合適的學習率或使用動態學習率調整算法（如 Adam）是十分重要的。

#### **6.4.5 小結**

反向傳播是深度學習模型訓練的核心算法，它通過計算梯度並更新權重來最小化損失函數。反向傳播的效率和正確性取決於激活函數、損失函數以及梯度下降法的選擇。理解反向傳播的數學推導和權重更新規則，有助於對神經網絡訓練過程有更深入的了解。