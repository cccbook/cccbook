### **第 6 章：深度學習基礎數學**

#### **6.1 神經網絡的數學建模**

神經網絡（Neural Networks）是深度學習領域的核心結構之一，其數學建模基於神經元的簡化數學模型，這些模型模仿了人類神經系統的工作原理。神經網絡由大量的「神經元」組成，這些神經元以層級結構相連，通常分為三類層：輸入層、隱藏層和輸出層。每一層由若干個神經元組成，每個神經元與前一層的神經元相連，並通過加權和激活函數進行變換。

神經網絡的數學建模包括以下幾個主要部分：

#### **6.1.1 神經元的基本結構**

每個神經元接收來自前一層的輸入信號，並將其進行處理後，輸出信號到下一層。具體而言，神經元的計算過程如下：


```math
z = \sum_{i=1}^{n} w_i x_i + b
```


其中：
-  $`x_i`$  是第  $`i`$  個輸入值。
-  $`w_i`$  是與第  $`i`$  個輸入對應的權重。
-  $`b`$  是偏置項（bias），用來調整輸出的閾值。
-  $`z`$  是神經元的加權和（weighted sum）。

隨後，這個加權和  $`z`$  通過激活函數進行非線性變換，得到神經元的輸出：


```math
a = f(z)
```


其中  $`f(z)`$  是激活函數，常見的激活函數包括：
- **Sigmoid** 函數：
  
```math
f(z) = \frac{1}{1 + e^{-z}}
```

- **ReLU**（Rectified Linear Unit）函數：
  
```math
f(z) = \max(0, z)
```

- **Tanh**（雙曲正切）函數：
  
```math
f(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
```


這些激活函數的作用是引入非線性，使神經網絡能夠擬合複雜的數據分佈。

#### **6.1.2 神經網絡的層級結構**

神經網絡通常由多層神經元組成，包括輸入層、隱藏層和輸出層。每一層的神經元接收來自前一層的輸出，並將其進行加權和處理後，輸出到下一層。

1. **輸入層**：接收數據輸入，將其傳遞到第一個隱藏層。
2. **隱藏層**：一個或多個隱藏層，對輸入數據進行特徵提取和變換。隱藏層中的每個神經元都會進行加權和計算，然後使用激活函數處理，生成輸出。
3. **輸出層**：最終的輸出層將隱藏層的處理結果轉化為最終的預測結果。

假設一個簡單的三層神經網絡（輸入層、隱藏層、輸出層），其數學模型可以表示為：


```math
a^{(1)} = f(W^{(1)} x + b^{(1)})
```


```math
a^{(2)} = f(W^{(2)} a^{(1)} + b^{(2)})
```


```math
y = W^{(3)} a^{(2)} + b^{(3)}
```


其中：
-  $`x`$  是輸入數據， $`a^{(1)}`$  是隱藏層的激活輸出， $`a^{(2)}`$  是輸出層的激活輸出。
-  $`W^{(i)}`$  和  $`b^{(i)}`$  分別是層  $`i`$  的權重矩陣和偏置項。
-  $`f(\cdot)`$  是激活函數。

#### **6.1.3 反向傳播與梯度下降**

神經網絡的訓練是通過優化權重來減少預測誤差。這通常是通過反向傳播（backpropagation）算法和梯度下降法（gradient descent）來實現的。

1. **損失函數（Loss Function）**：
   損失函數用於衡量模型預測結果與實際結果之間的差距，常見的損失函數包括均方誤差（MSE）和交叉熵損失（Cross-Entropy Loss）。

   
```math
L = \frac{1}{2} \sum (y_{pred} - y_{true})^2
```

   或者
   
```math
L = - \sum y_{true} \log(y_{pred})
```


2. **梯度計算與反向傳播**：
   反向傳播算法通過鏈式法則（chain rule）計算損失函數對各個權重的梯度。具體而言，對於權重  $`W^{(i)}`$ ，梯度計算公式為：

   
```math
\frac{\partial L}{\partial W^{(i)}} = \frac{\partial L}{\partial a^{(i)}} \cdot \frac{\partial a^{(i)}}{\partial W^{(i)}}
```


3. **梯度下降法**：
   在計算出所有權重的梯度後，通過梯度下降法更新權重。梯度下降法的更新公式為：

   
```math
W^{(i)} = W^{(i)} - \eta \frac{\partial L}{\partial W^{(i)}}
```


   其中  $`\eta`$  是學習率（learning rate），用來控制每次更新的步長。

#### **6.1.4 深層神經網絡與多層感知機**

隨著隱藏層數量的增加，神經網絡可以學習更複雜的模式。深層神經網絡（Deep Neural Networks，DNN）就是這樣的多層結構，它能夠捕捉到數據中的高層次特徵。多層感知機（MLP，Multilayer Perceptron）是最常見的神經網絡結構，它由多層神經元組成，通常用於分類和回歸問題。

#### **6.1.5 小結**

神經網絡的數學建模是基於加權和、激活函數的運算，並通過反向傳播和梯度下降法進行優化。在深度學習中，神經網絡的層級結構使其能夠有效地從數據中學習特徵，並進行預測和決策。隨著層數的增加，神經網絡可以處理更複雜的問題，並廣泛應用於語音識別、圖像處理、自然語言處理等領域。