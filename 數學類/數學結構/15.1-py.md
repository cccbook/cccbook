### 第十五章：統計推斷的數學基礎

統計推斷的數學基礎主要研究如何從樣本數據中估計和推斷母體參數，並進行假設檢驗。這一章涵蓋了估計理論、極大似然估計、主成分分析等重要概念和方法，這些方法在統計學中有著廣泛的應用。

#### 15.1 估計理論的代數結構

估計理論關注如何估計模型的未知參數。常見的估計方法包括最小二乘估計、極大似然估計等。這些方法通常涉及代數結構和線性代數技巧。

##### 1. 最小二乘估計與線性代數

**定義**：
最小二乘估計是指在給定一組數據點的情況下，尋找使得觀測值和估計值之間誤差的平方和最小的參數。假設我們有一組觀測數據 \( \{(x_i, y_i)\} \)，並希望擬合一條直線 \( y = ax + b \)，則最小二乘估計的目標是最小化以下誤差平方和：
\[
S(a, b) = \sum_{i=1}^n (y_i - ax_i - b)^2
\]
最小二乘估計方法是線性回歸中的基礎方法。

**Python 實作**：
使用線性代數方法計算最小二乘解，並擬合一條直線。

```python
import numpy as np
import matplotlib.pyplot as plt

# 模擬數據
np.random.seed(0)
n = 50
x = np.linspace(0, 10, n)
y = 2 * x + 1 + np.random.randn(n)  # y = 2x + 1 + 噪聲

# 最小二乘估計的設置
X = np.vstack([x, np.ones(n)]).T  # 設計矩陣
theta = np.linalg.inv(X.T @ X) @ X.T @ y  # 最小二乘解

# 繪製結果
y_fit = X @ theta
plt.scatter(x, y, label="Data")
plt.plot(x, y_fit, color='red', label="Fitted Line")
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()

# 顯示估計參數
print(f"Estimated parameters: a = {theta[0]}, b = {theta[1]}")
```

在這段程式中，我們使用最小二乘法擬合一條直線，並利用線性代數方法計算最小二乘解。

---

##### 2. 極大似然估計的群論詮釋

**定義**：
極大似然估計（Maximum Likelihood Estimation, MLE）是一種估計方法，用於估計統計模型中的參數。假設我們有一組觀測數據 \( \{x_1, x_2, \dots, x_n\} \)，並且數據來自某個概率分佈 \( f(x; \theta) \)，其中 \( \theta \) 是待估計的參數。極大似然估計的目標是找到使得似然函數最大的參數 \( \hat{\theta} \)。

似然函數 \( L(\theta) \) 是所有樣本的概率密度函數的乘積：
\[
L(\theta) = \prod_{i=1}^n f(x_i; \theta)
\]
極大似然估計的目標是最大化似然函數，或者等價地，最大化其對數似然函數：
\[
\ell(\theta) = \sum_{i=1}^n \log f(x_i; \theta)
\]

在群論的詮釋下，極大似然估計可以被視為一種在對應群上進行優化的過程。

**Python 實作**：
假設我們的數據來自正態分佈 \( N(\mu, \sigma^2) \)，我們可以使用極大似然估計來估計均值和方差。

```python
from scipy.stats import norm

# 模擬數據
np.random.seed(0)
n = 100
mu_true = 5
sigma_true = 2
data = np.random.normal(mu_true, sigma_true, n)

# 定義對數似然函數
def log_likelihood(mu, sigma, data):
    return np.sum(np.log(norm.pdf(data, mu, sigma)))

# 優化對數似然函數
from scipy.optimize import minimize
result = minimize(lambda params: -log_likelihood(params[0], params[1], data),
                  x0=[np.mean(data), np.std(data)], bounds=[(None, None), (1e-5, None)])

mu_mle, sigma_mle = result.x
print(f"MLE for mu: {mu_mle}, MLE for sigma: {sigma_mle}")
```

這段程式中，我們模擬了來自正態分佈的數據，並使用極大似然估計來估計均值和標準差。

---

##### 3. 統計推斷中的代數模型

**定義**：
統計推斷中的代數模型包括描述樣本空間、模型參數以及觀測數據之間的關係的數學結構。這些模型通常可以使用線性代數來進行推導和求解。

例如，線性回歸模型就可以被視為一種代數模型，將樣本點的輸入變數與對應的輸出變數通過一個線性變換建立起關係。

**Python 實作**：
可以將這些模型視為線性系統來解決，這樣可以使用代數方法進行統計推斷。

```python
# 線性回歸的代數模型
X = np.vstack([x, np.ones(n)]).T  # 設計矩陣
y = 2 * x + 1 + np.random.randn(n)  # 真實關係 + 噪聲

# 解線性方程
theta = np.linalg.lstsq(X, y, rcond=None)[0]

# 顯示結果
print(f"Linear model parameters: a = {theta[0]}, b = {theta[1]}")
```

---

##### 4. 主成分分析與奇異值分解

**定義**：
主成分分析（PCA）是一種降維技術，用於從多維數據中提取最具解釋性的特徵。PCA 通常通過奇異值分解（SVD）來實現，SVD 將一個矩陣分解為三個矩陣的乘積：
\[
A = U \Sigma V^T
\]
其中 \( U \) 和 \( V \) 分別是左奇異向量和右奇異向量矩陣，\( \Sigma \) 是對角矩陣，包含奇異值。

**Python 實作**：
使用 SVD 來進行主成分分析，並提取數據的主要成分。

```python
from sklearn.decomposition import PCA

# 模擬數據
np.random.seed(0)
X = np.random.rand(100, 5)  # 100 個樣本，每個樣本有 5 維特徵

# 執行 PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 繪製降維後的數據
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.title("PCA Result (2 Components)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

# 顯示主成分
print("Principal components:")
print(pca.components_)
```

在這段程式中，我們通過奇異值分解進行主成分分析，並將高維數據降至 2 維進行可視化。

---

### 小結

本節介紹了統計推斷的數學基礎，涵蓋了最小二乘估計、極大似然估計的群論詮釋、統計推斷中的代數模型以及主成分分析與奇異值分解。我們通過 Python 實現了這些方法，並展示了它們在統計推斷中的應用。這些工具對於理解和解決現實問題中的估計與推斷問題非常重要。