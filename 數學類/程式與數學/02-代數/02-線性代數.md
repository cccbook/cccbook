### 2. **線性代數**

線性代數是數學中研究向量、向量空間（或稱線性空間）以及線性映射（或稱線性變換）等結構的分支。它在數學、物理學、工程學、計算機科學和數據科學等領域中具有重要的應用，尤其在機器學習、人工智慧、經濟學等領域中扮演著關鍵角色。

#### 2.1 **向量、矩陣與線性變換**

1. **向量（Vector）**：
   - 向量是有方向和大小的量，可以視為一個由多個數字組成的有序集合。向量通常用列或行表示。
   - 向量的基本運算包括加法、數乘、內積等。
   
   例如，二維向量可以表示為：
   \[
   \mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix}
   \]

2. **矩陣（Matrix）**：
   - 矩陣是由數字（或其他數學對象）排列成的矩形表格。矩陣通常表示線性變換，可以進行加法、數乘和矩陣乘法等運算。
   
   一個 \(m \times n\) 矩陣可以表示為：
   \[
   \mathbf{A} = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{bmatrix}
   \]

3. **線性變換（Linear Transformation）**：
   - 線性變換是將一個向量映射到另一個向量的函數，並且保持向量加法和數乘運算。
   - 如果 \( T \) 是線性變換，則對於向量 \( \mathbf{v}_1 \) 和 \( \mathbf{v}_2 \)，以及標量 \( c \)，有：
   \[
   T(\mathbf{v}_1 + \mathbf{v}_2) = T(\mathbf{v}_1) + T(\mathbf{v}_2)
   \]
   \[
   T(c\mathbf{v}) = cT(\mathbf{v})
   \]
   - 矩陣乘法實際上就是線性變換的一種實現。

#### 2.2 **矩陣運算與線性回歸**

- **矩陣運算**：
  - 矩陣運算包括矩陣加法、矩陣乘法、轉置、逆矩陣等。這些運算是線性代數中不可或缺的一部分，並且在數據處理、機器學習中具有重要應用。
  
- **線性回歸**：
  - 線性回歸是一種統計方法，通過建立自變量與因變量之間的線性關係來進行預測。它的基本模型是：
  \[
  y = X\beta + \epsilon
  \]
  其中 \(y\) 是因變量，\(X\) 是自變量矩陣，\(\beta\) 是回歸係數，\(\epsilon\) 是誤差項。解這個方程的目的是估計回歸係數 \(\beta\)。

---

### Python 實作

#### 1. **向量與矩陣的基本運算**

Python 中可以使用 NumPy 库來實現向量和矩陣的基本運算。

```python
import numpy as np

# 向量加法
v1 = np.array([1, 2])
v2 = np.array([3, 4])
v_sum = v1 + v2
print(f"向量加法 v1 + v2: {v_sum}")

# 向量內積
v_dot = np.dot(v1, v2)
print(f"向量內積 v1 · v2: {v_dot}")

# 矩陣加法
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
matrix_sum = A + B
print(f"矩陣加法 A + B: \n{matrix_sum}")

# 矩陣乘法
matrix_product = np.dot(A, B)
print(f"矩陣乘法 A * B: \n{matrix_product}")

# 矩陣轉置
A_transpose = A.T
print(f"矩陣轉置 A^T: \n{A_transpose}")

# 逆矩陣
A_inv = np.linalg.inv(A)
print(f"矩陣 A 的逆矩陣: \n{A_inv}")
```

#### 2. **線性回歸**

線性回歸的目標是通過最小二乘法來找到最佳擬合直線。可以使用 NumPy 的線性代數函數來解這個問題。

```python
import numpy as np

# 假設有一些自變量 X 和對應的因變量 y
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y = np.dot(X, np.array([1, 2])) + 3  # 實際係數為 [1, 2]，偏置為 3

# 計算回歸係數
X_b = np.c_[np.ones((X.shape[0], 1)), X]  # 添加偏置項
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

print(f"回歸係數: {theta_best}")

# 預測新數據
X_new = np.array([[3, 5]])
X_new_b = np.c_[np.ones((1, 1)), X_new]
y_predict = X_new_b.dot(theta_best)
print(f"對新數據的預測: {y_predict}")
```

#### 3. **線性回歸與 Scikit-learn**

在實際應用中，可以使用 `scikit-learn` 库來進行線性回歸建模，這樣可以更加簡單快捷地實現。

```python
from sklearn.linear_model import LinearRegression

# 假設有一些自變量 X 和對應的因變量 y
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y = np.dot(X, np.array([1, 2])) + 3  # 實際係數為 [1, 2]，偏置為 3

# 使用 scikit-learn 進行線性回歸
lin_reg = LinearRegression()
lin_reg.fit(X, y)

# 輸出回歸係數和截距
print(f"回歸係數: {lin_reg.coef_}")
print(f"截距: {lin_reg.intercept_}")

# 預測新數據
X_new = np.array([[3, 5]])
y_predict = lin_reg.predict(X_new)
print(f"對新數據的預測: {y_predict}")
```

---

### 結論

線性代數是數學中的基礎領域之一，對於理解和解決現代科學和工程中的各種問題至關重要。矩陣運算、向量空間和線性變換的概念廣泛應用於機器學習、數據分析、圖像處理等領域。Python 中的 `NumPy` 和 `scikit-learn` 库使得我們能夠高效地進行線性代數計算，並在實際應用中解決複雜的問題。

如果有任何問題或需要進一步的示範，隨時告訴我！