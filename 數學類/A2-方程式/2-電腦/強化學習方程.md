## ccc: 請用 latex 列出最重要的強化學習方程式
* Q-Learning: $Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma \max_{a'}Q(s',a') - Q(s,a))$
* SARSA: $Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma Q(s',a') - Q(s,a))$
* 估計动态规划: $V(s) \leftarrow V(s) + \alpha(r + \gamma V(s') - V(s))$
* Policy Gradient: $\nabla J(\theta) = \sum_{s} \sum_{a} \pi_\theta(a|s) \nabla \log \pi_\theta(a|s) Q^{\pi_\theta}(s,a)$
* Actor-Critic: $\nabla J(\theta) = \nabla_\theta \log \pi_\theta(a|s) (Q^{\pi_\theta}(s,a) - V(s))$
* A3C: $Loss = -log\pi(a_t|s_t) (Q(s_t,a_t) - V(s_t))$
* DDPG : $Loss = -Q(s_t,a_t) + r + \gamma Q(s_{t+1},\mu(s_{t+1}|\theta^-))$
* DQN: $Loss = \frac{1}{2}(Q(s,a|\theta) - (r + \gamma \max_{a'}Q(s',a'|\theta^-)))^2$
* REINFORCE: $\nabla J(\theta) = \sum_{t} \nabla_\theta \log \pi_\theta(a_t|s_t) R_t$
* PPO: $Loss = \text{clip}(r_t(\theta)\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)},1-\epsilon,1+\epsilon)A_t$


