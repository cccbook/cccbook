### 20. **馬可夫決策過程 (MDP)**

#### MDP的基本概念

馬可夫決策過程（Markov Decision Process，簡稱MDP）是一個數學模型，用來描述在隨機環境中進行決策的過程。它為我們提供了一個框架，來分析如何在不確定的情境下做出最佳決策，並且考慮每個決策對未來結果的影響。MDP被廣泛應用於許多領域，如強化學習、經濟學、機器人控制、運籌學等。

在馬可夫決策過程中，主要由以下幾個元素構成：

1. **狀態空間（State Space）**：
   狀態空間是一個集合，包含了所有可能的狀態。每一個狀態 \(s_t\) 描述了系統在某一時刻的具體情況。在MDP中，狀態空間通常用 \(S\) 來表示，包含了所有可能的狀態。

2. **行為空間（Action Space）**：
   行為空間是指所有可能的行為或決策的集合。每個行為 \(a_t\) 代表了在狀態 \(s_t\) 下可以選擇的動作。在MDP中，行為空間通常用 \(A\) 來表示，包含了所有可能的行為。

3. **轉移機率（Transition Probability）**：
   轉移機率 \(P(s'|s, a)\) 描述了當系統處於狀態 \(s\) 且選擇行為 \(a\) 後，轉移到狀態 \(s'\) 的機率。這是一個隨機過程，反映了決策的結果不僅依賴於當前狀態和行為，還與環境的隨機性有關。

4. **獎勳函數（Reward Function）**：
   獎勳函數 \(R(s, a)\) 定義了在狀態 \(s\) 下執行行為 \(a\) 時獲得的回報或成本。這是MDP中一個關鍵的部分，它反映了每個行為對系統的價值或代價。

5. **折扣因子（Discount Factor）**：
   折扣因子 \(\gamma \in [0, 1]\) 用於調整未來獎勳的權重。較小的 \(\gamma\) 值表明決策者更重視即時回報，而較大的 \(\gamma\) 值則表示決策者願意等待長期的回報。

6. **策略（Policy）**：
   策略 \(\pi(s)\) 是一個函數，它描述了在每個狀態 \(s\) 下應該採取的行為。策略可以是確定性的，也可以是隨機的。確定性策略在每個狀態下有一個確定的行為，而隨機策略則會根據某些機率選擇行為。

### MDP的數學表示

馬可夫決策過程的數學模型可以使用五元組表示：

\[
MDP = (S, A, P, R, \gamma)
\]

其中：
- \(S\) 是狀態空間。
- \(A\) 是行為空間。
- \(P\) 是轉移機率，即 \(P(s'|s, a)\)，表示從狀態 \(s\) 執行行為 \(a\) 轉移到狀態 \(s'\) 的機率。
- \(R\) 是獎勳函數，即 \(R(s, a)\)，表示在狀態 \(s\) 下執行行為 \(a\) 所獲得的回報。
- \(\gamma\) 是折扣因子。

### 馬可夫性（Markov Property）

馬可夫決策過程的核心特徵之一是 **馬可夫性**。這意味著，當在某一時刻選擇行為後，系統的未來狀態僅取決於當前狀態和行為，而與過去的狀態和行為無關。換句話說，給定當前狀態，未來的狀態轉移是條件獨立的。數學上，我們稱這一特性為：

\[
P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_t)
\]

這樣的假設簡化了決策問題，因為在每個時間步長中，只需考慮當前的狀態，而不需要關注歷史信息。

### MDP的目標

在馬可夫決策過程中，目標是找到一個最佳策略 \(\pi^*\)，該策略能夠最大化長期累積回報。累積回報可以定義為在一個狀態下執行策略所獲得的獎勳總和。數學上，我們關心的是 **回報的期望值**，即：

\[
V^\pi(s) = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \bigg| s_0 = s \right]
\]

這裡，\(V^\pi(s)\) 是在策略 \(\pi\) 下，從狀態 \(s\) 開始的期望回報。最佳策略 \(\pi^*\) 是使得 \(V^{\pi^*}(s)\) 最大化的策略。

### MDP與強化學習

馬可夫決策過程是強化學習（Reinforcement Learning, RL）中的基礎概念。在強化學習中，智能體根據環境的反饋來學習最佳策略，從而最大化其回報。MDP提供了描述這一過程的數學框架，並為強化學習算法提供了理論支持。

MDP的核心問題是如何根據當前狀態決定最佳的行為策略，這是強化學習中的 **政策學習**（Policy Learning）問題。許多強化學習算法（如Q學習、SARSA等）都是在MDP的框架下發展起來的，並通過互動過程來學習最佳策略。