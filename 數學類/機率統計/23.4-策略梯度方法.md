### 4. **策略梯度方法**

#### - **策略梯度方法的數學推導**

策略梯度方法是一種直接優化策略的強化學習技術，其目標是找到參數化策略 \( \pi_\theta(a | s) \) 的最優參數 \( \theta \)，以最大化累積回報的期望。策略梯度的核心是使用梯度上升來優化策略。

目標函數通常定義為策略的累積回報期望：

\[ J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} r_t \right] \]

其中 \( \tau \) 是一條軌跡，\( \pi_\theta \) 是參數化策略。根據策略梯度定理，目標函數的梯度可以表示為：

\[ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right] \]

其中 \( R(\tau) \) 是軌跡的累積回報。

這一公式表明，可以通過對策略的概率分佈 \( \pi_\theta \) 的對數梯度乘以累積回報來估計策略梯度。

#### - **REINFORCE演算法**

REINFORCE演算法是策略梯度方法的一種基本形式。其核心思想是使用蒙地卡羅方法來估計策略梯度：

1. 從當前策略 \( \pi_\theta \) 生成多條軌跡 \( \tau \)。
2. 計算每條軌跡的累積回報 \( R(\tau) \)。
3. 根據策略梯度公式更新參數：

\[ \theta \leftarrow \theta + \alpha \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \]

其中 \( \alpha \) 是學習率。

REINFORCE演算法簡單易實現，但由於使用蒙地卡羅方法估計回報，可能會導致高方差。

#### - **Actor-Critic方法**

Actor-Critic方法結合了策略梯度方法（Actor）和值函數逼近（Critic）的優勢。Actor-Critic方法的基本框架包括兩個主要組件：

1. **Actor**：使用策略梯度方法來更新策略 \( \pi_\theta(a | s) \)。
2. **Critic**：使用值函數 \( V^\pi(s) \) 來估計策略的期望回報，減少策略梯度估計的方差。

Critic的值函數估計可以幫助更準確地評估策略性能，從而使Actor能夠更有效地學習。

Actor-Critic方法的更新過程包括：

- 使用TD誤差來更新值函數 \( V^\pi(s) \)。
- 使用修正後的回報來更新策略參數 \( \theta \)：

\[ \theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t | s_t) \delta_t \]

其中 \( \delta_t = r_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t) \) 是TD誤差。

Actor-Critic方法通過結合策略更新和值函數逼近，實現了策略學習的穩定性和效率的提高。