### 20. **馬可夫決策過程 (MDP)**

#### 策略與價值函數

在馬可夫決策過程（MDP）中，**策略（Policy）**和**價值函數（Value Function）**是決定智能體行為和優化其決策的關鍵概念。這些函數用來衡量每個狀態的優劣，並指導智能體如何選擇動作以達到最終目標。策略和價值函數的理解對於解決強化學習中的最優決策問題至關重要。

#### 1. **策略（Policy）**

策略是智能體在每個狀態下應選擇的行為規則。它定義了在每一時刻（或每一狀態下），智能體將選擇哪一個動作。策略可以是**確定性**的，也可以是**隨機性**的。

- **確定性策略**：在每一個狀態下，策略只會選擇一個特定的動作。對於狀態 \( s_t \)，策略 \(\pi\) 定義為：
  \[
  \pi(s_t) = a_t
  \]
  這表示，在狀態 \(s_t\) 時，智能體將選擇動作 \(a_t\)。
  
- **隨機策略**：隨機策略在每個狀態下不只選擇一個動作，而是根據一定的機率分佈選擇動作。對於狀態 \( s_t \)，隨機策略 \(\pi\) 定義為：
  \[
  \pi(a_t | s_t) = \text{Pr}(a_t \mid s_t)
  \]
  這表示在狀態 \(s_t\) 下，選擇動作 \(a_t\) 的機率。

- **最優策略**：最優策略是指在每一個狀態下，選擇能最大化累積回報的策略。最優策略通常是通過價值函數來尋找的，通過最大化未來回報來選擇最佳動作。

#### 2. **價值函數（Value Function）**

價值函數是用來衡量在某個狀態下，智能體所能獲得的回報的期望值。具體而言，價值函數 \(V(s)\) 是從狀態 \(s\) 開始，根據某一策略進行行動後，智能體所能獲得的未來回報的期望。對於一個狀態 \(s\)，價值函數定義為：
\[
V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s \right]
\]
這表示從狀態 \(s\) 開始，根據策略 \(\pi\) 選擇動作所獲得的未來回報的期望，其中 \(\gamma\) 是折扣因子，用來衡量未來回報的重要性。

- **狀態價值函數的直觀理解**：
  - **即時回報和長期回報**：價值函數不僅考慮當前的回報，還包括智能體未來的回報。這讓價值函數能夠反映未來狀態的潛在價值，對智能體做出最優決策至關重要。
  - **折扣因子**：折扣因子 \(\gamma\) 用來調整未來回報的權重，當 \(\gamma\) 越接近 1，表示智能體會更加重視長期回報；當 \(\gamma\) 越接近 0，表示智能體會偏重即時回報。

- **行為價值函數（Q函數）**：與狀態價值函數不同，行為價值函數 \(Q^\pi(s, a)\) 衡量在狀態 \(s\) 下執行動作 \(a\) 後，根據策略 \(\pi\) 繼續行動所獲得的回報期望值：
  \[
  Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s, a_0 = a \right]
  \]
  行為價值函數用來評估智能體選擇某個動作後的長期回報。當選擇某個動作時，智能體會根據 \(Q\) 函數來衡量其價值，並進行選擇。

#### 3. **價值函數的更新**

在MDP中，價值函數的更新通常是通過迭代過程進行的，常見的算法有**值迭代**（Value Iteration）和**策略迭代**（Policy Iteration）。這些算法基於貝爾曼方程來計算每個狀態的價值。

- **貝爾曼方程**：
  - **狀態價值的貝爾曼方程**：
    \[
    V^\pi(s) = R(s, \pi(s)) + \gamma \sum_{s'} P(s'|s, \pi(s)) V^\pi(s')
    \]
    這表示狀態 \(s\) 的價值是當前回報 \(R(s, \pi(s))\) 加上未來狀態的期望價值，未來狀態是由轉移概率 \(P(s'|s, \pi(s))\) 描述的。
    
  - **行為價值的貝爾曼方程**：
    \[
    Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \sum_{a'} \pi(a'|s') Q^\pi(s', a')
    \]
    這表示選擇動作 \(a\) 後的行為價值是當前的回報加上未來狀態的期望行為價值。

#### 4. **策略的最優性與最優價值函數**

最優策略是指在每一個狀態下選擇能最大化回報的策略。通過計算每個狀態的最優行為價值函數 \(Q^*(s, a)\)，可以找到最優策略 \(\pi^*\)。最優價值函數 \(V^*(s)\) 是在最優策略下每個狀態的價值，並滿足以下貝爾曼最優性方程：
\[
V^*(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^*(s') \right]
\]
最優策略 \(\pi^*(s)\) 是在每個狀態下選擇使得 \(Q^*(s, a)\) 最大的動作：
\[
\pi^*(s) = \arg\max_a Q^*(s, a)
\]

#### 5. **小結**

策略和價值函數是MDP中描述智能體行為和評估行為效果的核心元素。策略指導智能體在每一狀態下選擇動作，價值函數則用來衡量每個狀態的長期回報。通過策略和價值函數的設計和優化，智能體能夠學習最優行為，達成其目標。