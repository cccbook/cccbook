### 2. **連續空間的策略表示**

#### - **連續空間中的策略表示方法**

在連續空間中，策略表示是強化學習中的一個核心問題。策略 \( \pi \) 定義了在每個狀態 \( s \) 下選擇行動 \( a \) 的方式。在連續狀態和動作空間中，策略通常使用參數化函數來表示，例如神經網絡或線性回歸模型，這些模型可以根據不同的狀態輸出對應的動作。

連續空間中的策略可以表示為：

\[ \pi: S \to A \]

其中 \( S \) 是連續狀態空間，\( A \) 是連續動作空間。這種策略需要考慮狀態和動作的無窮多可能性，因此策略的設計和學習需要更加複雜的數學工具和近似方法。

#### - **確定性與隨機策略的數學描述**

1. **確定性策略（Deterministic Policy）**：

確定性策略在每個狀態 \( s \) 下輸出一個確定的行動 \( a \)，數學上表示為：

\[ \pi(s) = a \]

確定性策略適用於一些具體問題，但在具有噪聲或不確定性的環境中，其表現可能不如隨機策略。

2. **隨機策略（Stochastic Policy）**：

隨機策略在每個狀態 \( s \) 下根據某個概率分佈來選擇行動 \( a \)。隨機策略的數學描述為：

\[ \pi(a | s) = P(A = a | S = s) \]

這表示給定狀態 \( s \)，選擇行動 \( a \) 的概率。在實際應用中，隨機策略通常使用高斯分佈或其他連續分佈來建模行動的選擇。

#### - **策略梯度方法**

策略梯度方法是一種通過直接優化策略參數來提升策略性能的技術。與價值函數方法不同，策略梯度方法直接針對策略進行優化，尤其適用於連續空間中的強化學習。

策略梯度方法基於最大化期望累積回報的目標函數 \( J(\theta) \)，其中 \( \theta \) 是策略的參數向量。策略梯度定理提供了這個目標函數的梯度計算公式：

\[ \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a | s) Q^{\pi_\theta}(s, a) \right] \]

這裡 \( Q^{\pi_\theta}(s, a) \) 是狀態-行動價值函數，反映了在狀態 \( s \) 採取行動 \( a \) 後的期望累積回報。通過計算和更新這個梯度，策略可以逐步優化。

策略梯度方法的主要優勢在於其可以處理高維連續動作空間，並且可以自然地應用於隨機策略的優化。然而，這種方法也存在收斂速度較慢和樣本效率低的挑戰，因此通常需要結合其他技術，如基於基線（baseline）的策略梯度或自然策略梯度（natural policy gradient），來提高性能。