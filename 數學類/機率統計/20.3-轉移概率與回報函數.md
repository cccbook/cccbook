### 20. **馬可夫決策過程 (MDP)**

#### 轉移概率與回報函數

在馬可夫決策過程（MDP）中，**轉移概率**和**回報函數**是描述智能體行為的兩個核心概念。它們分別用來描述智能體如何從一個狀態過渡到另一個狀態，以及每次行為所獲得的回報。這兩者在強化學習和決策理論中扮演了至關重要的角色，並直接影響智能體的策略學習和最終表現。

#### 1. **轉移概率（Transition Probability）**

轉移概率描述了在某一特定狀態下，智能體執行某一動作後，系統轉移到另一狀態的機率。具體來說，轉移概率 \(P(s_{t+1} | s_t, a_t)\) 表示在當前狀態 \(s_t\) 下執行動作 \(a_t\) 之後，系統轉移到下一個狀態 \(s_{t+1}\) 的條件概率。

- **定義**：
  \[
  P(s_{t+1} | s_t, a_t) = \text{Pr}(s_{t+1} \mid s_t, a_t)
  \]
  其中 \(s_t\) 是當前狀態，\(a_t\) 是執行的動作，\(s_{t+1}\) 是下一個狀態，\(P\) 是狀態轉移的機率。

- **轉移概率的性質**：
  - **馬可夫性質（Markov Property）**：轉移概率遵循馬可夫性質，即當前狀態的轉移只依賴於當前狀態和當前動作，而與過去的狀態或動作無關。這意味著在馬可夫決策過程中，給定當前狀態和動作，未來的狀態和過去的狀態無關。
  - **隨機性**：轉移概率本質上是隨機的，這意味著即使在相同的狀態和動作下，系統也可能以不同的機率轉移到不同的狀態。這使得MDP能夠建模真實世界中的不確定性和隨機性。

- **離散與連續轉移概率**：
  - **離散轉移概率**：在離散狀態空間中，轉移概率通常是離散的，即系統會在有限的狀態集合中進行轉移。
  - **連續轉移概率**：在連續狀態空間中，轉移概率可能是連續的，通常需要使用機率密度函數來描述。

#### 2. **回報函數（Reward Function）**

回報函數定義了智能體在每次執行某一動作後，所能獲得的獎勳或懲罰。回報是智能體與環境互動的結果，指示智能體當前行為的好壞。回報函數通常用來引導智能體的行為，使其學會選擇有利於達成目標的策略。

- **定義**：
  \[
  R(s_t, a_t) = \text{即智能體在狀態} s_t \text{執行動作} a_t \text{時所獲得的回報}
  \]
  回報函數 \(R\) 給出當前狀態 \(s_t\) 和動作 \(a_t\) 對應的即時回報。

- **即時回報 vs 長期回報**：
  - **即時回報**：回報是智能體在某一特定時間步的獲得，它通常直接反映當前的行為或結果。
  - **長期回報**：在強化學習中，智能體的目標是最大化其長期回報。這意味著，智能體需要考慮到每個動作對未來回報的影響。這通常是通過折扣因子 \(\gamma\) 來建模的，即將未來回報折扣至當前的價值。

- **回報的性質**：
  - **積極回報與負面回報**：回報可以是正數，也可以是負數。正數回報通常表示智能體所採取的行為是有利的，負數回報則表示行為帶來了不良後果。例如，在棋類遊戲中，勝利可能獲得正回報，而失敗可能帶來負回報。
  - **稀疏回報與密集回報**：在某些問題中，智能體可能需要進行長時間的探索才能獲得回報，這稱為稀疏回報；而在其他情況下，回報可能會頻繁地給出，這被稱為密集回報。

#### 3. **轉移概率與回報函數在MDP中的作用**

- **決策過程**：智能體的目標是根據當前的狀態和動作，最大化累積的回報。在每個時間步，智能體根據策略選擇動作，然後根據轉移概率進行狀態轉移，並獲得相應的回報。這個過程會不斷重複，直到達到某個終止條件（例如遊戲結束或達到預定目標）。

- **策略學習**：智能體的策略會根據回報函數來進行調整。回報函數幫助智能體評估行為的效果，並根據這些評價進行策略更新。在強化學習中，這通常通過值迭代、策略梯度等算法來實現。

- **期望回報**：智能體的最終目標是最大化期望回報。這可以通過選擇最佳策略來實現，使得對應於所有狀態和動作的回報加權總和最大。期望回報的計算涉及轉移概率和回報函數的組合，並通過折扣因子進行長期回報的折扣。

#### 4. **小結**

在馬可夫決策過程中，轉移概率和回報函數是描述智能體與環境互動的基本組成部分。轉移概率描述了智能體的行為如何影響環境，回報函數則反映了智能體的行為效果，指導其策略的學習和選擇。理解這兩個概念並合理設計它們對於成功應用MDP解決問題至關重要。