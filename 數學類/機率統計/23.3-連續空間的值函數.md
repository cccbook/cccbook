### 3. **連續空間的值函數**

#### - **值函數的連續表示**

在連續空間的強化學習中，值函數是描述狀態或狀態-行動對的長期回報的一種工具。對於連續狀態空間 \( S \) 和動作空間 \( A \)，值函數 \( V^\pi(s) \) 和狀態-行動值函數 \( Q^\pi(s, a) \) 的定義如下：

- **狀態值函數** \( V^\pi(s) \)：表示在狀態 \( s \) 下，根據策略 \( \pi \) 採取行動後的期望累積回報：

\[ V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid S_0 = s, \pi \right] \]

- **狀態-行動值函數** \( Q^\pi(s, a) \)：表示在狀態 \( s \) 下，採取行動 \( a \) 並接著根據策略 \( \pi \) 繼續行動後的期望累積回報：

\[ Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid S_0 = s, A_0 = a, \pi \right] \]

這些值函數是連續變數的函數，需要對狀態和行動空間中的無窮多點進行計算。

#### - **值函數與回報函數的數學關係**

值函數與回報函數之間有密切的數學關係。回報函數 \( r(s, a) \) 定義了在狀態 \( s \) 採取行動 \( a \) 時立即得到的回報，而值函數反映了未來累積回報的期望。值函數可以通過回報函數和轉移概率來遞歸地表達，這就是所謂的貝爾曼方程：

- **狀態值函數的貝爾曼方程**：

\[ V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot | s)} \left[ r(s, a) + \gamma \mathbb{E}_{s' \sim P(\cdot | s, a)} \left[ V^\pi(s') \right] \right] \]

- **狀態-行動值函數的貝爾曼方程**：

\[ Q^\pi(s, a) = r(s, a) + \gamma \mathbb{E}_{s' \sim P(\cdot | s, a)} \left[ \mathbb{E}_{a' \sim \pi(\cdot | s')} \left[ Q^\pi(s', a') \right] \right] \]

這些方程描述了值函數如何通過當前的即時回報和下一個狀態的值函數來遞歸計算。

#### - **值函數的逼近方法**

由於連續空間中無法直接存儲和計算值函數的所有可能值，因此通常使用逼近方法來近似值函數。常見的逼近方法包括：

1. **線性逼近**：使用線性模型來近似值函數，例如：

\[ V^\pi(s) \approx \phi(s)^T \theta \]

其中 \( \phi(s) \) 是特徵向量，\( \theta \) 是參數向量。

2. **神經網絡逼近**：使用神經網絡作為函數逼近器，輸入狀態 \( s \) 或狀態-行動對 \( (s, a) \)，輸出近似的值函數。

3. **高斯過程逼近**：使用高斯過程來建模值函數的分佈，特別適合於處理不確定性。

4. **核函數逼近**：通過核函數將輸入映射到高維特徵空間，進行值函數的非線性逼近。

這些方法允許在高維連續空間中有效地學習和估計值函數，提高強化學習算法的性能和適應性。