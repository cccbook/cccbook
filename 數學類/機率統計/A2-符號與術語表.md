### A2. **符號與術語表**

以下是一些在機率論、統計學和強化學習領域常用的符號和術語，旨在幫助理解相關的數學概念和算法。

#### 1. **常用數學符號**
   - \( \mathbb{R} \) : 實數集
   - \( \mathbb{Z} \) : 整數集
   - \( \mathbb{N} \) : 自然數集（不包括零）
   - \( \mathbb{N}_0 \) : 自然數集（包括零）
   - \( \mathbb{E}[X] \) : 隨機變數 \( X \) 的期望值
   - \( \text{Var}(X) \) : 隨機變數 \( X \) 的方差
   - \( \text{Cov}(X, Y) \) : 隨機變數 \( X \) 和 \( Y \) 的協方差
   - \( \mathbb{P}(A) \) : 事件 \( A \) 發生的機率
   - \( \mathbb{E}[X|Y] \) : 在給定 \( Y \) 的條件下 \( X \) 的條件期望值
   - \( \mathbb{P}(A|B) \) : 給定事件 \( B \) 發生的條件下，事件 \( A \) 發生的機率
   - \( \sigma^2 \) : 方差
   - \( \mu \) : 均值（期望值）
   - \( \Gamma(x) \) : 伽瑪函數
   - \( \delta(x) \) : 狄拉克 δ 函數（單位脈衝）
   - \( \partial \) : 偏導數

#### 2. **常見機率分佈與相關符號**
   - \( \mathcal{N}(\mu, \sigma^2) \) : 正態分佈，均值為 \( \mu \)，方差為 \( \sigma^2 \)
   - \( \text{Bin}(n, p) \) : 二項分佈，\( n \) 次試驗，每次成功的機率為 \( p \)
   - \( \text{Poisson}(\lambda) \) : 泊松分佈，平均發生率為 \( \lambda \)
   - \( \text{Exp}(\lambda) \) : 指數分佈，率參數為 \( \lambda \)
   - \( \text{Gamma}(\alpha, \beta) \) : 伽瑪分佈，形狀參數為 \( \alpha \)，尺度參數為 \( \beta \)
   - \( \text{Chi-Square}(k) \) : 卡方分佈，具有 \( k \) 自由度
   - \( t_{\nu} \) : t-分佈，自由度為 \( \nu \)
   - \( F_{\nu_1, \nu_2} \) : F-分佈，具有自由度 \( \nu_1 \) 和 \( \nu_2 \)

#### 3. **強化學習符號**
   - \( S \) : 狀態空間
   - \( A \) : 動作空間
   - \( P(s'|s, a) \) : 給定狀態 \( s \) 和動作 \( a \) 時，轉移到狀態 \( s' \) 的機率
   - \( R(s, a) \) : 在狀態 \( s \) 採取動作 \( a \) 後的即時回報
   - \( \gamma \) : 折扣因子，範圍為 \( 0 \leq \gamma < 1 \)
   - \( V(s) \) : 狀態 \( s \) 的價值函數（期望回報）
   - \( Q(s, a) \) : 給定狀態 \( s \) 和動作 \( a \) 時的 Q 函數（狀態-動作價值）
   - \( \pi(s) \) : 策略，表示在狀態 \( s \) 下選擇某一動作的概率
   - \( \pi^*(s) \) : 最優策略
   - \( V^*(s) \) : 最優狀態價值函數
   - \( Q^*(s, a) \) : 最優狀態-動作價值函數
   - \( R_t \) : 在時間 \( t \) 取得的回報
   - \( \mathbb{E}_t[ \cdot ] \) : 時間 \( t \) 下的期望操作
   - \( \rho(s) \) : 在狀態 \( s \) 的穩態分佈（在馬可夫決策過程中的長期分佈）

#### 4. **馬可夫決策過程（MDP）與動態規劃符號**
   - \( P(s'|s, a) \) : 轉移概率
   - \( \gamma \) : 折扣因子（discount factor）
   - \( R(s, a) \) : 即時回報
   - \( V(s) \) : 狀態價值函數
   - \( Q(s, a) \) : 動作價值函數
   - \( \pi(s) \) : 策略（在狀態 \( s \) 下的動作選擇概率）

#### 5. **數值優化與算法符號**
   - \( \nabla \) : 梯度（gradient）
   - \( \nabla^2 \) : 黑西（Hessian）矩陣（第二階導數）
   - \( \lambda \) : 正則化參數
   - \( L(x) \) : 損失函數（Loss Function）
   - \( \mathcal{L}(x) \) : 機率損失函數
   - \( \theta \) : 模型參數（通常表示神經網絡的權重）
   - \( \hat{\theta} \) : 參數的估計值
   - \( \partial_\theta \) : 參數 \( \theta \) 的偏導數

#### 6. **強化學習算法與術語**
   - **值迭代（Value Iteration）** : 求解馬可夫決策過程的動態規劃方法
   - **策略迭代（Policy Iteration）** : 基於策略的動態規劃方法
   - **Q-Learning** : 一種無模型強化學習算法，用於學習最優策略
   - **蒙特卡羅方法（Monte Carlo Methods）** : 基於模擬的隨機算法
   - **策略梯度（Policy Gradient）** : 直接在策略空間中進行優化的強化學習方法
   - **Actor-Critic 方法** : 同時使用值函數和策略來進行學習的算法

#### 7. **深度學習與神經網絡符號**
   - \( \mathcal{L} \) : 神經網絡的損失函數
   - \( W \) : 神經網絡中的權重矩陣
   - \( b \) : 神經網絡中的偏置向量
   - \( f(x) \) : 激活函數（如 ReLU, Sigmoid, Tanh）
   - \( \hat{y} \) : 預測結果
   - \( y \) : 真實標籤

這些符號和術語在討論概率論、強化學習、深度學習等領域的數學模型和算法中非常常見。