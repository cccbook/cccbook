### 20. **馬可夫決策過程 (MDP)**

#### 策略迭代與價值迭代算法

在馬可夫決策過程（MDP）中，策略迭代（Policy Iteration）和價值迭代（Value Iteration）是兩種常用的動態規劃方法，用於尋求最優策略。這兩種方法都依賴於貝爾曼方程，但它們在求解過程中的具體步驟和重點不同。以下將詳細介紹這兩種算法的原理、步驟及其優缺點。

### 1. **策略迭代（Policy Iteration）**

策略迭代是一種基於迭代更新策略和價值函數的求解方法。其基本思想是：從任意的初始策略出發，通過不斷地評估策略的價值函數並改進策略，最終找到最優策略。

#### 策略迭代的步驟：
1. **初始化策略**：首先隨機選擇一個初始策略  $`\pi_0(s)`$ （即對每個狀態  $`s`$  選擇一個動作）。
2. **策略評估**：對當前的策略  $`\pi`$ ，計算其對應的價值函數  $`V^\pi(s)`$ 。這可以通過解以下的貝爾曼方程來進行：
   
```math
V^\pi(s) = R(s, \pi(s)) + \gamma \sum_{s'} P(s'|s, \pi(s)) V^\pi(s')
```

   在每一次評估中，根據策略  $`\pi`$  更新每個狀態的價值，直到收斂為止（即價值函數不再改變）。
3. **策略改進**：根據當前的價值函數  $`V^\pi(s)`$ ，更新策略  $`\pi`$ ，即對每個狀態  $`s`$ ，選擇使價值函數最大的動作：
   
```math
\pi'(s) = \arg\max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \right]
```

4. **檢查收斂**：如果策略  $`\pi`$  在一步更新後不再改變，則該策略為最優策略，算法終止；否則，將新策略  $`\pi'`$  作為當前策略，返回步驟2繼續進行評估和改進。

#### 策略迭代的優缺點：
- **優點**：
  - 每次策略改進都是保證的進步，最終必定收斂到最優策略。
  - 如果計算和儲存足夠的資源，策略迭代通常能夠很快找到最優解。
- **缺點**：
  - 每次策略評估都需要解一組線性方程，這可能會非常耗時，特別是在狀態空間較大時。
  - 計算量隨著狀態數量和動作數量的增加而增大，可能導致高昂的計算成本。

### 2. **價值迭代（Value Iteration）**

價值迭代是基於貝爾曼最優方程的遞歸方法，通過不斷更新每個狀態的價值函數來找到最優策略。與策略迭代不同，價值迭代不直接顯示策略，而是將策略的尋找和價值的更新步驟合併在一起。

#### 價值迭代的步驟：
1. **初始化價值函數**：首先，初始化每個狀態的價值  $`V_0(s)`$ （通常設為零）。
2. **值更新**：根據貝爾曼最優方程更新每個狀態的價值函數：
   
```math
V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \right]
```

   重複進行這一過程，直到所有狀態的價值函數  $`V_k(s)`$  收斂，即達到預定的收斂誤差  $`\epsilon`$ ：
   
```math
\max_s |V_{k+1}(s) - V_k(s)| \leq \epsilon
```

3. **提取最優策略**：當價值函數收斂後，最優策略可以通過以下公式提取：
   
```math
\pi^*(s) = \arg\max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]
```

   
#### 價值迭代的優缺點：
- **優點**：
  - 值迭代不需要顯式地進行策略評估和改進，因此在一些情況下計算上比策略迭代更為高效。
  - 在每次更新中，價值函數的更新是根據所有可能的動作選擇的最大值，因此保證能夠收斂到最優解。
- **缺點**：
  - 每次更新需要計算所有動作的期望回報，這在狀態和動作空間很大時可能會變得非常慢。
  - 通常需要進行更多的迭代才能達到收斂，尤其是當問題較為複雜或狀態空間較大時。

### 3. **比較策略迭代與價值迭代**

- **收斂性**：兩者都能夠保證收斂到最優策略，但策略迭代每次策略改進步驟會更明確，並且通常在較少的迭代中收斂。而價值迭代可能需要更多的迭代來達到收斂，因為它在每次迭代中都會更新所有狀態的價值函數。
- **計算效率**：策略迭代每次都需要完整地評估一個策略，而價值迭代則在每次迭代中更新每個狀態的價值，並且每次只進行一次更新，因此在計算上可能更為高效，尤其是在狀態空間較大時。
- **適用情境**：對於較小的問題，策略迭代可能比較有效，因為它收斂較快且能夠直接得到最優策略。對於較大或較複雜的問題，價值迭代更為合適，因為它的每次計算步驟比較簡單。

### 小結

策略迭代和價值迭代是求解馬可夫決策過程（MDP）最常用的兩種動態規劃方法。策略迭代通過策略評估和策略改進交替進行，而價值迭代則通過直接更新每個狀態的價值函數來達到最優解。選擇哪一種方法取決於問題的特性、計算資源以及所需的收斂速度。